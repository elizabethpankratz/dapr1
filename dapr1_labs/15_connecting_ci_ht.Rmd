---
title: "Connecting confidence intervals and hypothesis testing"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
```


```{r include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', 
                      fig.height = 7, fig.width = 8.2, 
                      out.width = '70%')

set.seed(123)

library(tidyverse)
library(patchwork)
library(kableExtra)

theme_set(theme_light(base_size = 15))
```

:::lo

1. Interpret a confidence interval as the plausible values of a parameter that would not be rejected in a two-sided hypothesis test

2. Determine the decision for a two-sided hypothesis test from an appropriately constructed confidence interval

3. Be able to explain the potential problem with significant results when doing multiple tests

:::


# Research question and data


> Do mean hours of exercise per week differ between left-handed and right-handed students?


The data contain 50 observations on a random sample of students on the following 7 variables 

- `year`: Year in school
- `hand`: Left (l) or Right (r) handed?
- `exercise`: Hours of exercise per week
- `tv`: Hours of TV viewing per week
- `pulse`: Resting pulse rate (beats per minute)
- `pierces`: Number of body piercings


You can download the data here: https://uoepsy.github.io/data/ExerciseHours.csv


# Null and alternative hypothesis

The research question asks whether mean hours of exercise per week differ between left-handed and right-handed students, without specifying a direction. We are both interested in whether right-handed students exercise more than left-handed ones, or the other way around.


We start by defining the parameters needed to specify the null hypothesis:

- $\mu_R$ = mean exercise hours per week for all right-handed students
- $\mu_L$ = mean exercise hours per week for all left-handed students

These are estimated with the corresponding statistics in the sample:

- $\bar x_R$ = mean exercise hours per week for right-handed students in the sample
- $\bar x_L$ = mean exercise hours per week for left-handed students in the sample


Hypotheses:

$$
H_0: \mu_R = \mu_L \\
H_1: \mu_R \neq \mu_L
$$
Equivalently:
$$
H_0: \mu_R - \mu_L = 0 \\
H_1: \mu_R - \mu_L \neq 0
$$



# Data exploration

First, we will read the data into R:
```{r}
library(tidyverse)

exdata <- read_csv('https://uoepsy.github.io/data/ExerciseHours.csv')
head(exdata)
```

Next, we should make sure that the hand variable is correctly handled as a factor. At the same time, we will only select the columns of interest:
```{r}
exdata <- exdata  %>%
    select(hand, exercise) %>%
    mutate(hand = factor(hand))

head(exdata)
```

Visualise the distribution of exercise hours by dominant hand. First, let's add a column with the mean of each group so that we can show the mean as a vertical green line.
```{r fig.height = 6, fig.width = 7}
exdata_plot <- exdata %>%
    group_by(hand) %>% 
    mutate(avg_exercise = mean(exercise))

exdata_plot
```

Next, display the distribution of exercise hours per week as a dotplot (or a histogram if you prefer) and show the mean with a vertical line `geom_vline()`:
```{r}
ggplot(exdata_plot, aes(x = exercise)) +
    geom_dotplot(binwidth = 1, fill = 'lightblue', color = NA) +
    facet_grid(hand ~ .) +
    geom_vline(aes(xintercept = avg_exercise),
               color = 'darkolivegreen4', size = 1) +
    labs(x = "Exercise per week (Hours)")
```


Let's now create a table displaying, for the left and right handed students in the sample, the average hours of exercise per week and standard deviation:

```{r}
exdata_stats <- exdata %>% 
    group_by(hand) %>%
    summarise(count = n(),
              avg_exercise = mean(exercise),
              sd_exercise = sd(exercise))

exdata_stats
```

I will store the difference in sample means into a variable called `diff_obs` for ease of use later on:
```{r}
diff_obs <- exdata_stats$avg_exercise[2] - exdata_stats$avg_exercise[1]
diff_obs
```


Recall we are interested in testing a claim about the (unknown) population difference in means:
$$
\mu_R - \mu_L
$$


We estimate it with the sample difference in means
$$
\bar x_R - \bar x_L = `r round(diff_obs, 3)` \text{ hrs} \qquad = D_{obs} \text{ in short}
$$

Let's now compute the bootstrap distribution. Recall that this will be centred at the sample statistic, i.e. the difference in means in the original sample shown above!


# Bootstrap distribution

First, we should pause a second and reflect on how the data were collected in the first place. This is clearly not a randomized experiment as it is not the case that a sample of participants was collected from a population and then each participant was randomly allocated to either a treatment or another treatment.

We are, instead, in the presence of an _observational study_. An observational study is when researchers obtain a sample of units from a population and merely record the value of some variables on those units, without intervening in any way. In other words, they just observe the values that naturally exist in the units, without actively controlling the value of any variable.

In this case we have a sample of students from a more general population of left-handed students. Similarly, we have another sample of students from another more general population of right-handed students.
To create the bootstrap distribution, we need to resample in a way that reflects how the data were collected in the first place.

To obtain a bootstrap distribution, we do the following many times. Sample 9 students, with replacement, from the original sample of left-handed students. Sample 41 students, with replacement, from the original sample of right-handed students. Then, we compute the difference between the means of those 2 samples.

First, we need to import the `rep_sample_n()` function, which we will use to sample with replacement:
```{r}
source('https://uoepsy.github.io/files/rep_sample_n.R')
```

We will also specify the number of samples we want to obtain, 1000 say (you could do more if you wish to have more precision).
```{r}
num_samples <- 1000
```

Then we will start by focusing on the left-handed students:
```{r}
l_means <- exdata %>%
    filter(hand == 'l') %>%
    rep_sample_n(n = nrow(.), samples = num_samples, replace = TRUE) %>%
    group_by(hand, sample) %>%
    summarise(avg = mean(exercise))
l_means
```

Now, for the right-handed students:
```{r}
r_means <- exdata %>%
    filter(hand == 'r') %>%
    rep_sample_n(n = nrow(.), samples = num_samples, replace = TRUE) %>%
    group_by(hand, sample) %>%
    summarise(avg = mean(exercise))
r_means
```

Finally, we combine the means of the left-handed and right-handed students:
```{r}
all_means <- bind_rows(l_means, r_means) %>%
    pivot_wider(names_from = hand, values_from = avg, names_prefix = "avg_")
all_means
```

---

`bind_rows()` takes two tibbles and stacks them under each other (they must have the same column names!), while `pivot_wider()` changes the tibble by expanding one column into multiple columns.
For example, consider the following data, measuring an outcome "mean" on different subjects (1 and 2) at different recall periods (I = immediate, W = one week later).

The following data are in "long format":
```
ID Recall Mean
1  I      6
1  W      3
2  I      3
2  W      1
```

and the wide format is:
```
ID Mean_I Mean_W
1  6      3
2  3      1
```

To go from long to wide, use pivot_wider, provide the column which gives the names: names_from = Recall, where the values come from: values_from = Mean, and give a better name to the columns otherwise it would be just I and W, so we do names_prefix = "Mean_".

To go from wide to long, we use pivot_longer. You should provide which columns to collapse, how to call the column with the group labels: names_to = "Recall", and how to call the column with the values: values_to = "Mean"

---

Let's now compute the difference between the mean for right-handed students and left-handed students:
```{r}
boot_dist <- all_means %>%
    mutate(diff = avg_r - avg_l)
boot_dist
```


# Confidence interval

## Percentile method

We can obtain a 95% confidence interval for the difference in means using, for example, the 0.025 and 0.975 quantiles of the bootstrap distribution:
```{r}
boot_quant <- quantile(boot_dist$diff, probs = c(0.025, 0.975))
boot_quant
```

:::int
We are 95% confident that the mean exercise hours for right-handed students is between 3.2 hours less and and 6.5 hours more than for left-handed students.
:::

```{r echo=FALSE}
ggplot(boot_dist) +
    geom_histogram(aes(x = diff, y = ..density.., 
                       fill = stat(x) <= boot_quant[1] | stat(x) >= boot_quant[2]),
                   color = 'white', alpha = 0.5) +
    geom_vline(xintercept = diff_obs, color = 'darkolivegreen4', size = 1) +
    annotate(geom = 'text', x = diff_obs, y = -0.01, hjust = -0.1,
             label = round(diff_obs, 3), color = 'darkolivegreen4', size = 5) +
    annotate(geom = 'text', x = boot_quant[1], y = -0.01, hjust = 1.1,
             label = round(boot_quant[1], 3), color = 'red', size = 5) +
    annotate(geom = 'text', x = boot_quant[2], y = -0.01, hjust = -0.1,
             label = round(boot_quant[2], 3), color = 'red', size = 5) +
    geom_vline(xintercept = boot_quant[1], color = 'red', size = 1, linetype = 2) +
    geom_vline(xintercept = boot_quant[2], color = 'red', size = 1, linetype = 2) +
    scale_fill_manual(values = c('gray', 'red', 'red')) +
    theme(legend.position = 'none') +
    labs(x = "Difference in means", title = "") +
    annotate(geom = 'label', x = diff_obs, y = 0.10, label = '0.95',
             color = 'darkgray', fontface = 'bold', size = 5)
```


## SE method

If the bootstrap distribution is fairly symmetric, we can equivalently construct a 95% confidence interval by using the bootstrap standard error and report:

$$
\text{Statistic} \pm 1.96 * SE
$$

In our case

```{r}
se <- sd(boot_dist$diff)
se
```

And the interval goes from and to these two values:
```{r}
diff_obs - 1.96 * se
diff_obs + 1.96 * se
```

:::int
We are 95% confident that the mean exercise hours for right-handed students is between `r round(diff_obs - 1.96 * se, 1)` hours less and `r round(diff_obs + 1.96 * se, 1)` hours more than for left-handed students.
:::


In the this section, we have used the bootstrap distribution to provide a range of plausible values for the difference mean exercise hours per week.

We can see that the interval goes from `r round(diff_obs - 1.96 * se, 1)` to `r round(diff_obs + 1.96 * se, 1)` hrs. This means that a hypothetical value of 0 for the difference in means is contained in the interval. In turn, this means that 0 is a plausible value for the population difference in means and for this reason if we were to test such null hypothesis in a two-sided test, we would not reject $H_0 : \mu_R - \mu_L = 0$ at the 5% significance level.




# Null distribution

We now will perform a 5% test for our hypotheses but using the null distribution approach, rather than a confidence interval.

Recall that the null distribution must be consistent with the null hypothesis:
$$
H_0: \mu_R = \mu_L \\
H_1: \mu_R \neq \mu_L
$$

So we must make the mean of the right-handed students the same as that of left-handed students and sample from these new data.


**Left-handed students**

`r qbegin(1)`
Shift the exercise hours of left-handed students to have the same mean as the right-handed students.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
l_data <- exdata %>%
    filter(hand == 'l') %>%
    mutate(exercise = exercise + diff_obs)
l_data
```

`r solend()`



`r qbegin(2)`
Using `rep_sample_n()`, compute 1,000 resample means for left-handed students using the shifted exercise column.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
l_means <- l_data %>%
    rep_sample_n(n = nrow(.), samples = num_samples, replace = TRUE) %>%
    group_by(hand, sample) %>%
    summarise(avg = mean(exercise))
l_means
```
`r solend()`


**Right-handed students**

`r qbegin(3)`
Using `rep_sample_n()`, compute 1,000 resample means for right-handed students.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
r_data <- exdata %>%
    filter(hand == 'r')

r_means <- r_data %>%
    rep_sample_n(n = nrow(.), samples = num_samples, replace = TRUE) %>%
    group_by(hand, sample) %>%
    summarise(avg = mean(exercise))
r_means
```

`r solend()`


**Combine**

`r qbegin(4)`
Combine the left-handed and right-handed means into a single tibble.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
all_means <- bind_rows(l_means, r_means) %>%
    pivot_wider(names_from = hand, values_from = avg, names_prefix = "avg_")
all_means
```
`r solend()`



`r qbegin(5)`
Compute the difference between the mean for right-handed students and left-handed students.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
null_dist <- all_means %>%
    mutate(diff = avg_r - avg_l)
null_dist
```

`r solend()`




`r qbegin(6)`
Using the critical value method, test whether the sample data provide evidence (at the 5% significance level) that the mean exercise hours for right-handed students and left-handed students are not the same.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
null_crit <- quantile(null_dist$diff, probs = c(0.025, 0.975))
null_crit
```

:::int
The observed difference in means, $\bar x_R - \bar x_L = `r round(diff_obs, 3)`$ falls between the lower (`r round(null_crit[1], 3)`) and upper critical values (`r round(null_crit[2], 3)`) cutting a 5% area in the tails. For this reason, at the 5% significance level, we do not reject the null hypothesis that the mean exercise hours for right and left-handed students are the same.
:::
`r solend()`



`r qbegin(7)`
Perform the same test using the p-value approach.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
pvalue <- 2 * sum(null_dist$diff >= diff_obs ) / nrow(null_dist)
pvalue
```


:::int
We performed a two-sided test against the null hypothesis that the mean difference in exercise hours per week between right and left-handed students is 0. At the 5% significance level, the p-value of `r round(pvalue, 3)` indicates that the sample results do not provide sufficient evidence against the null and in favour of the alternative.

If the mean exercise hours for right and left handed students were the same, we expect to observe a sample difference as extreme as `r round(diff_obs, 3)` in about 46 out of 100 samples.
:::

`r solend()`



`r qbegin(8)`
Does a test of hypothesis provide you information that a confidence interval does not?

And does a confidence interval provide you information that a test of hypothesis does not?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
An hypothesis test provides us information about strength of evidence. 

In other words, imagine having a virtual population where the mean is actually 0, i.e. where the null $H_0: \mu = 0$ is true. Then you take many samples and compute the sample mean for each sample. The histogram of the sample means shows the null distribution, i.e. the values of the sample mean that you expect to observe when the null hypothesis is true.
If your original sample mean is not what we expect to observe when the null is true, i.e. when it is extreme in the null distribution, then we might doubt that the original sample came from a population where the mean is 0.

The probability of a sample mean as extreme or more extreme than the observed sample mean, when the null is true, is known as the p-value. This gives us information about the strength of evidence that the sample data provide in favour of the alternative hypothesis.


However, a test of hypothesis does not provide us with a range of plausible values for the parameter of interest. This would be especially useful in the case when the test of hypothesis has rejected the null, saying, for example, that we have evidence that the population parameter is not zero.

However, is it 1, or 5, or 10, or 100? How much different from 0? What's the magnitude of the difference?

This is where we gain more information by providing, after a significant hypothesis test, a confidence interval for the population parameter.
This way, we report evidence that the population parameter is not zero, and also the magnitude of the effect. If a 95% CI is 2.1 to 6, we might say that we are 95% confidence that the population parameter is between 2.1 and 6.
`r solend()`


:::yellow
**GOOD PRACTICE**

Whenever you reject a null hypothesis, i.e. you find significant results, it is good practice to follow up your analysis by reporting a confidence interval for your statistic.

The hypothesis test tells you the strength of evidence that your sample data provide against the null hypothesis, but it does not tell you much about the magnitude of that effect or difference in means. To get the magnitude, you need a confidence interval.
:::


<!-- Formatting -->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>

