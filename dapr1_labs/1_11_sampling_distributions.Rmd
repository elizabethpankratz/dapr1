---
title: "Sampling distributions"
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
```

```{r include=FALSE}
set.seed(3)

knitr::opts_chunk$set(out.width = '70%',
                      fig.align = 'center')
```


:::lo
**LEARNING OBJECTIVES**

1. Recognise the difference between parameters and statistics
1. Be able to use a sample statistic to estimate an unknown parameter
1. Understand what a sampling distribution is
1. Understand the concept of standard error
1. Recognise why sample size matters
:::



# Essential terminology


`r optbegin("Population vs sample", olabel = FALSE, show = params$SHOW_SOLS, toggle = params$TOGGLE)`

## Population vs sample

Typically:

- We do not have data for the entire population. There are different possible reasons:
    - It's too expensive
    - Because of deadlines, there is not sufficient time
- It's much easier to obtain data by taking a sample from that population of interest and measuring only the units chosen in the sample. 
    - Note that units do not necessarily have to be individuals, but they could be schools, companies, etc.
- We wish to use the sample data to:
    - Investigate a claim about the whole population.
    - Test an hypothesis about the entire population.
    - Answer a question about the whole population.

The process of using information from a sample (the part) in order to draw conclusions about the entire population (the whole) is known as statistical inference.
`r optend()`


`r optbegin("Parameters vs statistics", olabel = FALSE, show = params$SHOW_SOLS, toggle = params$TOGGLE)`
## Parameters vs statistics

As we do not typically have the data for the entire population, the population is considered as "unknown" with respect the data that we wish to investigate. We could shortly say that the population data are unknown.

For example, if we are interested in the average IQ in the population, we don't have the resources to go to every single individual and test their IQ score. So, in this respect, the population IQ scores are unknown.

As a consequence of this, any numerical summary of the population data is also unknown. In the above example, the population mean IQ score is unknown.

Sample data are more readily available or feasible to collect. Imagine collecting a sample of 50 individuals, chosen at random from the population, and testing each to obtain their IQ score. If you performed the random experiment, you would then obtain a sequence of 50 IQ measurements.

At the same time, it is also feasible to compute any numerical summary of the sample data. For example, you can compute the mean IQ score for those 50 individuals in the sample.

:::statbox
We typically use this terminology to distinguish a numerical summary when computed in the population (unknown) or in the sample (known).

- A **parameter** is a numerical summary of a population.

- A **statistic** is a numerical summary of the sample.

A statistic is often used as a "best guess" or "estimate" for the unknown parameter. That is, we use the (sample) statistic to estimate a (population) parameter.
:::

In the above example, the population mean IQ score is the parameter of interest, while the sample mean IQ score is the statistic.

It is typical to use special notation to distinguish between parameters and statistics in order to convey with a single letter: (1) which numerical summary is being computed, and (2) if it is computed on the population or on the sample data.

The following table summarizes standard notation for some population parameters, typically unknown, and the corresponding estimates computed on a sample.

|Numerical summary  | Population parameter   | Sample statistic         |
|:------------------|:----------------------:|:------------------------:|
|Mean               | $\mu$                  | $\bar{x}$ or $\hat{\mu}$ |
|Standard deviation | $\sigma$               | $s$ or $\hat{\sigma}$    |
|Proportion         | $p$                    | $\hat{p}$                |

Table: Notation for common parameters and statistics.


The Greek letter $\mu$ (mu) represents the population mean (parameter), while $\bar{x}$ (x-bar) or $\hat{\mu}$ (mu-hat) is the mean computed from the sample data (sample statistic).

The Greek letter $\sigma$ (sigma) represents the population standard deviation (parameter), while $s$ or $\hat{\sigma}$ (sigma-hat) is the standard deviation computed from the sample data (sample statistic).

The Getter $p$ represents the population proportion (parameter), while $\hat{p}$ (p-hat) is the proportion computed from the sample data (sample statistic).


#### Statistics are random variables

The process of sampling $n$ people at random from the population is a random experiment, as it leads to an uncertain outcome. Hence, a statistic is a numerical summary of a random experiment and for this reason it is a random variable (random number).

Before actually performing the random experiment and picking individuals for the sample, the sample mean is a random number, as its value is uncertain. 
Once we actually perform the random experiment and measure the individuals in the sample, we have an observed value for the sample mean, i.e. a known number.

Throughout the exercises we will use the following notation:

- Uppercase letters refer to random variables
    - A random variable represents a well defined number, whose value is uncertain.
    - For example, $\bar X$ = mean IQ score in a random sample of 50 individuals. It is clearly defined in operational terms by: (1) take a sample of 50 individuals at random, (2) measure their IQ score, and (3) compute the mean of their 50 IQ scores. However, the actual value that we can obtain is uncertain as it is the result of an experiment of chance involving random sampling from a population. There are many possible values we could obtain, so we are not sure which one we will see.
    - Second example. $X$ = number  of heads in 10 flips of a coin. This is also a clearly defined experiment: (1) flip a coin 10 times and (2) count the number of heads. However, the result is a random number, which is uncertain and will be unknown until we actually perform the experiment and flip a coin 10 times.

- Lowercase letters refer to observed (realised) values of the random variable.
    - An observed value of a random variable is just a number. For example, if we actually collected 50 individuals and measured their IQ scores, and observed a mean IQ score of 102.3, we would write $\bar x = 102.3$.
    
In short, we would use for a sample mean:

- An uppercase letter before the value is actually known: $\bar X$
- A lowercase letter once the value is actually known: $\bar x$

`r optend()`


`r optbegin("Avoiding bias due to sampling", olabel = FALSE, show = params$SHOW_SOLS, toggle = params$TOGGLE)`
## Avoiding bias due to sampling

Sampling bias occurs when the method used to select which units enter the sample causes the sample to not be a good representation of the population.

If sampling bias exists, we cannot generalise our sample conclusions to the population.

```{r echo=FALSE, out.width = '95%'}
knitr::include_graphics('images/prob/sampling_bias.png')
```

To be able to draw conclusions about the population, we need a representative sample. The key in choosing a representative sample is _random sampling_. 
Imagine an urn with tickets, where each ticket has the name of each population unit. Random sampling would involve mixing the urn and blindly drawing out some tickets from the urn.
Random sampling is a strategy to avoid sampling bias.


:::yellow
__Simple random sampling__

When we select the units entering the sample via simple random sampling, each unit in the population has an equal chance of being selected, meaning that we avoid sampling bias.

When instead some units have a higher chance of entering the same, we have misrepresentation of the population and sampling bias.
:::


In general, we have _bias_ when the method of collecting data causes the data to inaccurately reflect the population.
`r optend()`


`r optbegin("Sampling distribution", olabel = FALSE, show = params$SHOW_SOLS, toggle = params$TOGGLE)`
## Sampling distribution

The natural gestation period (in days) for human births is normally distributed in the population with mean 266 days and standard deviation 16 days. 
This is a special case which **rarely** happens in practice: we actually know what the distribution looks like in the population. 

We will use this unlikely example to study how well does the sample mean estimate the population mean and, to do so, we need to know what the population mean is so that we can compare the estimate and the true value. 
Remember, however, that in practice the population parameter would _not_ be known.

We will consider data about the gestation period of the 49,863 women who gave birth in Scotland in 2019. These can be found at the following address: https://uoepsy.github.io/data/pregnancies.csv

First, we read the population data:
```{r}
library(tidyverse)
gest <- read_csv('https://uoepsy.github.io/data/pregnancies.csv')
dim(gest)
```

The data set contains information about 49,863 cases. For each case an identifier and the length of pregnancy
Look at the top six rows of the data set (the "head"):
```{r}
head(gest)
```


We now want to investigate how much the sample means will vary from sample to sample. To do so, we will take many samples from the population of all gestation periods, and compute for each sample the mean.

To do so, we will load a function which we prepared for you called `rep_sample_n()`. This function is used to take a sample of $n$ units from the population, and it lets you repeat this process many times.

To get the function in your computer, run this code:
```{r}
source('https://uoepsy.github.io/files/rep_sample_n.R')
```

:::red
__NOTE__

You need to copy and paste the line 
```{r}
source('https://uoepsy.github.io/files/rep_sample_n.R')
```
at the top of each file in which you want to use the `rep_sample_n()` function.
:::

The function takes the following arguments:
```
rep_sample_n(data, n = <sample size>, samples = <how many samples>)
```

- `data` is the population

- `n` is the sample size

- `samples` is how many samples of size $n$ you want to take

Before doing anything involving random sampling, it is good practice to _set the random seed_. This is to ensure reproducibility of the results. 
Random number generation in R works by specifying a starting seed, and then numbers are generated starting from there.

Set the random seed to any number you wish. Depending on the number, you will get the same results as me or not:
```{r}
set.seed(1234)
```

Obtain 10 samples of $n = 5$ individuals each:
```{r}
samples <- rep_sample_n(gest, n = 5, samples = 10)
samples
```

The `samples` data frame contains 3 columns:

- `sample`, telling us which sample each row refers to

- `id`, telling us the units chosen to enter each sample

- `gest_period`, telling us the gestation period (in days) of each individual

Note that the tibble `samples` has `r nrow(samples)` rows, which is given by 5 individuals in each sample * 10 samples.

You can inspect the sample data in the following interactive table in which the data corresponding to each sample have been colour-coded so that you can distinguish the rows belonging to the 1st, 2nd, ..., and 10th sample:

```{r echo=FALSE}
library(DT)

datatable(samples,
          options = list(autoWidth = TRUE, pageLength = 10,
                         columnDefs = list(list(className = 'dt-center', 
                                                targets = 1:3)))) %>%
  formatSignif('gest_period', 5) %>%
  formatStyle('sample',
              backgroundColor = styleEqual(
                unique(samples$sample), 
                c("#8dd3c7", "#ffffb3", "#bebada", 
                  "#fb8072", "#80b1d3", "#fdb462", 
                  "#b3de69", "#fccde5", "#d9d9d9", 
                  "#bc80bd")
              ))
```

Now, imagine computing the mean of the five observation in each sample. This will lead to 10 means, one for each of the 10 samples (of 5 individuals each).

```{r}
sample_means <- samples %>%
  group_by(sample) %>%
  summarise(mean_gest = mean(gest_period))

sample_means
```

As you can see this leads to a tibble having 10 rows (one for each sample), where each row is a mean computed from the 5 individuals which were chosen to enter the sample.  

The gestation period (in days) for the first five women sampled were
<center>
`r filter(samples, sample == 1)$gest_period %>% round(2)`
</center>
This sample has a mean of $\bar x$ = `r filter(samples, sample == 1)$gest_period %>% mean() %>% round(2)` days.  

The second sample of 5 women had gestation periods
<center>
`r filter(samples, sample == 2)$gest_period %>% round(2)`
</center>
The second sample has a mean gestation period of $\bar x$ = `r filter(samples, sample == 2)$gest_period %>% mean() %>% round(2)` days.  

In Figure \@ref(fig:sampl-10-5-50) we display the individual gestation periods in each sample as dots, along with the means gestation period $\bar x$ of each sample. The position of the sample mean is given by a red vertical bar.

We then increased the sample size to 50 women and took 10 samples each of 50 individuals. This set of samples together with their means is also plotted in Figure \@ref(fig:sampl-10-5-50).


```{r sampl-10-5-50, echo = FALSE, fig.height = 5, fig.width=9, out.width = '100%', fig.cap = "Gestation period (in days) of samples of individuals."}
library(patchwork)

# data
da <- samples %>% 
  group_by(sample) %>%
  mutate(mean_gest = mean(gest_period))

db <- gest %>%
  rep_sample_n(n = 50, samples = 10) %>%
  group_by(sample) %>%
  mutate(mean_gest = mean(gest_period))

# theme
all_theme <- 
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.line.x = element_line(),
        axis.ticks.x = element_line(),
        plot.title = element_text(hjust = 0.5, face = 'bold'))

# plots
pa <- ggplot(da, aes(x = gest_period, y = as.factor(sample))) +
  # geom_vline(aes(xintercept = 266), color = 'gray') + 
  geom_point(color = 'black', size = 2, pch = 1) +
  geom_segment(aes(x = mean_gest, xend = mean_gest, 
                   y = sample - 0.25, yend = sample + 0.25),
               size = 1, color = 'tomato3') +
  all_theme +
  # scale_x_continuous(
  #   breaks = round(seq(min(da$y, 230),max(da$y), by = 10))
  # ) + 
  labs(x = 'Gestation period (days)',
       y = 'Sample number',
       title = '(a) 10 samples of size n = 5') +
    xlim(min(db$gest_period, da$gest_period), 
         max(db$gest_period, da$gest_period))


pb <- ggplot(db, aes(x = gest_period, y = as.factor(sample))) +
  # geom_vline(aes(xintercept = 266), color = 'gray') + 
  geom_point(color = 'black', size = 2, pch = 1) +
  geom_segment(aes(x = mean_gest, xend = mean_gest, 
                   y = sample - 0.25, yend = sample + 0.25),
               size = 1, color = 'tomato3') +
  all_theme +
  # scale_x_continuous(
  #   breaks = round(seq(min(df$y, 230),max(df$y), by = 10))
  # ) + 
  labs(x = 'Gestation period (days)',
       y = 'Sample number',
       title = '(b) 10 samples of size n = 50') +
    xlim(min(db$gest_period, da$gest_period), 
         max(db$gest_period, da$gest_period))

pa | pb
```


Two important points need to be made from Figure \@ref(fig:sampl-10-5-50). 
First, each sample (and therefore each sample mean) is different. This is due to the randomness of which individuals end up being in each sample.
The sample means vary in an unpredictable way, illustrating the fact that $\bar X$ is a summary of a random experiment (randomly choosing a sample) and hence is a random variable.
Secondly, as we increase the sample size from 5 to 50, there appears to be a decrease in the variability of sample means (compare the variability in the vertical bars in panel (a) and panel (b)). That is, with a larger sample size, the sample means fluctuate less and are more "consistent".

To further investigate the variability of sample means, we will now generate many more sample means computed on:

(a) 1,000 samples of $n = 5$ women
(b) 1,000 samples of $n = 50$ women
(c) 1,000 samples of $n = 500$ women

We will also add at the end of each tibble a column specifying the sample size. In the first tibble, `mutate(n = 5)` creates a column called n where all values will be 5, to remind ourselves that those means were computed with samples of size $n = 5$. Remember that `mutate()` takes a tibble and creates a new column or changes an existing one. 

```{r}
# (a) 1,000 means from 1,000 samples of 5 women each
sample_means_5 <- rep_sample_n(gest, n = 5, samples = 1000) %>%
  group_by(sample) %>%
  summarise(mean_gest = mean(gest_period)) %>%
  mutate(n = 5)
head(sample_means_5)

# (b) 1,000 means from 1,000 samples of 50 women each
sample_means_50 <- rep_sample_n(gest, n = 50, samples = 1000) %>%
  group_by(sample) %>%
  summarise(mean_gest = mean(gest_period)) %>%
  mutate(n = 50)
head(sample_means_50)

# (c) 1,000 means from 1,000 samples of 500 women each
sample_means_500 <- rep_sample_n(gest, n = 500, samples = 1000) %>%
  group_by(sample) %>%
  summarise(mean_gest = mean(gest_period)) %>%
  mutate(n = 500)
head(sample_means_500)
```

We now combine the above datasets of sample means for different sample sizes into a unique tibble. The function `bind_rows()` takes multiple tibbles and stacks them under each other.

```{r}
sample_means_n <- bind_rows(sample_means_5, 
                            sample_means_50, 
                            sample_means_500)
```

We now plot three different density histograms showing the distribution of 1,000 sample means computed from samples of size 5, 50, and 500.

This would correspond to creating a histogram of the "red vertical bars" from Figure \@ref(fig:sampl-10-5-50), the only difference is that we have many more samples (1,000).

```{r sampl-dist-mean, fig.cap = "Density histograms of the sample means from 1,000 samples of women ($n$ women per sample).", fig.height=6, fig.width=5}
ggplot(sample_means_n) +
  geom_histogram(aes(mean_gest, after_stat(density)), 
                 color = 'white', binwidth = 1) +
  facet_grid(n ~ ., labeller = label_both) +
  theme_bw() + 
  labs(x = 'Sample mean of gestation period (days)', y = 'Density')
```

Each of the density histograms above displays the distribution of the sample mean, computed on samples of the same size and from the same population.

Such a distribution is called the _**sampling distribution**_ of the sample mean.

:::yellow
__Sampling distribution__

The sampling distribution of a statistic is the distribution of a sample statistic computed on many different samples of the same size from the same population.

A sampling distribution shows how the statistic varies from sample to sample due to sampling variation.
:::

`r optend()`


`r optbegin("Centre and spread of a sampling distribution", olabel = FALSE, show = params$SHOW_SOLS, toggle = params$TOGGLE)`
## Centre and spread of the sampling distribution

What is the mean and standard deviation of each histogram?

```{r}
sample_means_n %>%
  group_by(n) %>%
  summarise(mean_xbar = mean(mean_gest),
            sd_xbar = sd(mean_gest))
```

Compare these quantities to the population mean and standard deviation: $\mu$ = `r mean(gest$gest_period) %>% signif(3)` and $\sigma$ = `r sd(gest$gest_period) %>% signif(3)`.

Regardless of the size of the samples we were drawing (5, 50, or 500), the average of the sample means was equal to the population mean.
However, the standard deviation of the sample means was smaller than the population mean. The variability in sample means also decreases as the sample size increases.

There is an interesting patter in the decrease, which we will now verify. 
It can be proved that the standard deviation of the sample mean $\sigma_{\bar X} = \frac{\sigma}{\sqrt{n}}$, i.e. the population standard deviation divided by $\sqrt{n}$ with $n$ being the sample size.

Obtain the population standard deviation. Remember the entire population data were called `gest` and  in this case we are very lucky to have the data for the entire population, typically we wouldn't have those and neither the population standard deviation.

```{r}
sigma <- sd(gest$gest_period)
```

Now compute add a column that compares the SD from sampling with the theory-based one:

```{r}
sample_means_n %>%
  group_by(n) %>%
  summarise(mean_xbar = mean(mean_gest),
            sd_xbar = sd(mean_gest)) %>%
  mutate(sd_theory = sigma / sqrt(n))
```

The last two columns will be closer and closer as we increase the number of different samples we take from the population (e.g. 5,000 or 10,000 or even more samples.)

The following result holds:
$$
\begin{aligned}
\mu_{\bar X} &= \mu = \text{Population mean} \\
\sigma_{\bar X} &= \frac{\sigma}{\sqrt{n}} = \frac{\text{Population standard deviation}}{\sqrt{\text{Sample size}}}
\end{aligned}
$$

Because on average the sample mean (i.e. the estimate) is equal to the population mean (i.e. the parameter), the sample mean $\bar X$ is an _unbiased_ estimator of the population mean. In other words, it does not consistently "miss" the target. (However, if your sampling method is biased, the sample mean will be biased too.)

The standard deviation of the sample means tells us that the variability in the sample means gets smaller smaller as the sample size increases. Because $\sqrt{4} = 2$ we halve $\sigma_{\bar X}$ by making the sample size 4 times as large. Similarly, as $\sqrt{9} = 3$, we reduce $\sigma_{\bar X}$ by one third by making the sample size 9 times as large.


The variability, or spread, of the sampling distribution shows how much the sample statistics tend to vary from sample to sample. This is key in understanding how accurate our estimate of the population parameter, based on just one sample, will be.

Recall that the standard deviation tells us the size of a typical deviation from the mean. Here, the mean is the population parameter $\mu$, and a deviation of $\bar x$ from $\mu$ is called an _estimation error_.
Hence, the standard deviation of the sample mean is called the _**standard error**_ of the mean. This tells us the typical estimation error that we commit when we estimate a population mean with a sample mean.

:::yellow
__Standard error__

The standard error of a statistic, denoted $SE$, is the standard deviation of its sampling distribution.
:::


<!-- In practice, we can only afford to take one sample from the population and we do not know the population standard deviation $\sigma$. -->
<!-- So, the standard error of the mean is computed using the standard deviation of the data in your sample: -->
<!-- $$ -->
<!-- SE = s_{\bar X} = \frac{s}{\sqrt{n}} -->
<!-- $$ -->

So, the standard error of the mean can be either computed as the standard deviation of the sampling distribution, or using the formula
$$
SE = \sigma_{\bar X} = \frac{\sigma}{\sqrt{n}}
$$

`r optend()`


`r optbegin("The sample mean is normally distributed", olabel = FALSE, show = params$SHOW_SOLS, toggle = params$TOGGLE)`
## The sample mean is normally distributed

We also notice that the density histograms in Figure \@ref(fig:sampl-dist-mean) are symmetric and bell-shaped. Hence, they follow the shape of the normal curve.

```{r echo=FALSE, fig.height=6, fig.width=5}
all_theme <- theme_bw() + 
  theme(
      plot.title = element_text(hjust = 0.5, face = 'bold')
  )

a <- ggplot(sample_means_5) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  geom_function(fun = dnorm, 
                args = list(mean = mean(sample_means_5$mean_gest), 
                            sd = sd(sample_means_5$mean_gest)), size = 1, color = 'red') + 
  all_theme +
  labs(x = 'Sample mean of gestation period (days)', y = 'Density',
       title = 'n: 5')

b <- ggplot(sample_means_50) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  geom_function(fun = dnorm, 
                args = list(mean = mean(sample_means_50$mean_gest), 
                            sd = sd(sample_means_50$mean_gest)), size = 1, color = 'red') + 
  all_theme + 
  labs(x = 'Sample mean of gestation period (days)', y = 'Density',
       title = 'n: 50')

c <- ggplot(sample_means_500) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  geom_function(fun = dnorm, 
                args = list(mean = mean(sample_means_500$mean_gest), 
                            sd = sd(sample_means_500$mean_gest)), size = 1, color = 'red') + 
  all_theme + 
  labs(x = 'Sample mean of gestation period (days)', y = 'Density',
       title = 'n: 500')

(a + xlim(240, 290)) /(b + xlim(240, 290)) / (c + xlim(240, 290))
```


The random variable $\bar X$ follows a normal distribution:
$$
\bar X \sim N(\mu,\ SE)
$$

We can also compute a z-score. We have that:
$$
Z = \frac{\bar X - \mu}{SE} \sim N(0, 1)
$$

We know that for a normally distributed random variable, approximately 95% of all values fall within two standard deviations of its mean. 
Thus, for approximately 95% of all samples, the sample means falls within $\pm 2 SE$ of the population mean $\mu$.
Similarly, since $P(-3 < Z < 3) = 0.997$, it is even more rare to get a sample mean which is more than three standard errors away from the population mean (only 0.3% of the times).

This suggests that:

- The standard error $SE$ is a measure of precision of $\bar x$ as an estimate of $\mu$.

- If is a pretty safe bet to say that the true value of $\mu$ lies somewhere between $\bar x - 2 SE$ and $\bar x + 2 SE$.

- We will doubt any hypothesis specifying that the population mean is $\mu$ when the value $\mu$ is more than $2 SE$ away from the sample mean we got from our data, $\bar x$. We shall be even more suspicious when the hypothesised value $\mu$ is more than $3 SE$ away from $\bar x$.




:::yellow
__Centre and shape of a sampling distribution__

- _Centre_: If samples are randomly selected, the sampling distribution will be centred around the population parameter. _(No bias)_

- _Shape_: For most of the statistics we consider, if the sample size is large enough, the sampling distribution will follow a normal distribution, i.e. it is symmetric and bell-shaped. _(Central Limit Theorem)_
:::


Clearly, we can compute sampling distributions for other statistics too: the proportion, the standard deviation, ...

This requires the following steps:

- Obtaining multiple samples, all of the same size, from the same population;

- For each sample, calculate the value of the statistic;

- Plot the distribution of the computed statistics.

`r optend()`



`r optbegin("Why sample size matters", olabel = FALSE, show = params$SHOW_SOLS, toggle = params$TOGGLE)`
## Why sample size matters

You might be wondering: why did we take multiple samples of size $n$ from the population when, in practice, we can only afford to take one?

This is a good question. We have taken multiple samples to show how the estimation error varies with the sample size.
We saw in Figure \@ref(fig:sampl-dist-mean), shown again below, that smaller sample sizes lead to more variable statistics, while larger sample sizes lead to more **precise** statistics, i.e. the estimates are more concentrated around the true parameter value.

```{r echo=FALSE, fig.cap = "Density histograms of the sample means from 5,000 samples of women ($n$ women per sample).", fig.height=6, fig.width=5}
ggplot(sample_means_n) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  facet_grid(n ~ ., labeller = label_both) +
  theme_bw() + 
  labs(x = 'Sample mean of gestation period (days)', y = 'Density')
```

__This teaches us that, when we have to design a study, it is better to obtain just one sample with size $n$ as large as we can afford.__


`r qbegin("Think about it", FALSE)`
What would the sampling distribution of the mean look like if we could afford to take samples as big as the entire population, i.e. of size $n = N$?
`r qend()`
`r solbegin(show=TRUE, toggle=params$TOGGLE)`
_If you can_, it is best to measure the entire population. 

If we could afford to measure the entire population, then we would find the exact value of the parameter all the time.
By taking multiple samples of size equal to the entire population, every time we would obtain the population parameter exactly, so the distribution would look like a histogram with a single bar on top of the true value: we would find the true parameter with a probability of one, and the estimation error would be 0.

```{r}
pop_means <- gest %>%
  rep_sample_n(n = nrow(gest), samples = 10) %>%
  group_by(sample) %>%
  summarise(mean_gest = mean(gest_period))
pop_means
```

The following is a dotplot of the means computed above:
```{r echo=FALSE, fig.height=3, fig.width=4}
ggplot(pop_means, aes(x = mean_gest)) +
  geom_dotplot(binwidth = 1, dotsize = 0.05) +
  theme_classic() +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) + 
  labs(x = 'Sample mean', y = '')
```
`r solend()`



To summarize:

- We have __high precision__ when the estimates are less variable, and this happens for a __large sample size__. 

- We have __no bias__ when we select samples that are representative of the population, and this happens when we do __random sampling__. No bias means that the estimates will be centred at the true population parameter to be estimated.


```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/prob/bias_prec.png')
```

`r optend()`


---


# Exercises

`r qbegin(1)`
Two students, Mary and Alex, wanted to investigate the average hours of study per week among students in their university. 
Each were given the task to sample $n = 20$ students many times, and compute the mean of each sample of size 20.
Mary sampled the students at random, while Alex asked students from the library.
The distribution of sample means computed by Mary and Alex are shown in the dotplot below in green and red, respectively.

```{r echo=FALSE, fig.height = 4}
library(tidyverse)

set.seed(1)

df <- tibble(
  x = rep(c("random", "library"), each = 100),
  y = c(rnorm(100, 15, 2), rnorm(100, 24, 2))
)

ggplot(df, aes(x = y, fill = x)) +
  geom_dotplot(color = NA, dotsize = 0.8, alpha = 0.5) +
  theme_classic() + 
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(x = "Average hours of study", y = "", fill = "Sampling method")
```

What do you notice in the distributions above? Why did Mary and Alex get so different results?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Mary selected samples using random sampling, so we expect the samples to be representative of the population of interest. Hence, we will believe that Mary's sampling distribution of the mean is centred at the true population average of 15 hours of study per week.

On the other hand, Alex selected the most readily available people and took convenience samples. This leads to samples which are not a good representation of the population as a part of the population is missing. 
We notice that Alex got consistently higher estimates of the population mean study time than Mary did. This tendency to overestimate the population parameter shows that the sampling method is _biased._ Students in the library perhaps tend to study more.
`r solend()`


`r qbegin(2)`
**Average price of goods sold by ACME Corporation**

Suppose you work for a company that is interested in buying ACME Corporation^[You might remember it from the cartoon Wile E. Coyote and the Road Runner.] and your boss wants to know within the next 30 minutes what is the average price of goods sold by that company and how the prices of the goods they sell differ from each other.

Since ACME Corporation has such a big mail order catalogue, see Figure \@ref(fig:acme), we will assume that the company sells many products. Furthermore, we only have the catalogue in paper-form and no online list of prices is available.

```{r acme, out.width='50%', fig.align='center', echo=FALSE, fig.cap="Product catalogue of ACME corporation."}
knitr::include_graphics('images/prob/acme.jpg')
```

1. Identify the population of interest and the population parameters.
2. Can we compute the parameters within the next 30 minutes?
3. How would you proceed in estimating the population parameters if you just had time to read through 100 item descriptions? Would you pick the first 100 items or would you pick 100 random page numbers?
4. State which statistics you would use to estimate the population parameters.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
1. The population of interest is all products sold by ACME Corporation. The population parameters are the mean price $\mu$ and the standard deviation $\sigma$.
2. Because the catalogue has so many pages, we can not compute the population parameters within the next 30 minutes.
3. We must estimate the population mean and standard deviation from a sample of size $n = 100$. We should choose the items entering the sample at random, to avoid sampling bias. If we were to choose 100 consecutive items, we might end up with a very good estimate of the average price for the category those consecutive items belong to (e.g. gardening). However, this would not be a good estimate of the overall price across the multiple categories of products sold.
4. Our best guess of the population mean would be the sample mean, $\bar{x}$, and our best guess of the population standard deviation would be the sample standard deviation, denoted $s$ or $\hat{\sigma}$.
`r solend()`


`r qbegin(3)`
What is a parameter? Give two examples of parameters.

What is a statistic? Give two example of statistics.

What is an estimate?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
A parameter is a numerical summary of the population. An example could be average blood pressure in Scotland, or the proportion of people with a car.

A statistic is a numerical summary of the sample data. Examples are the mean blood pressure in a sample of 20 people, or the proportion of people with a car in a sample of 20 people.

We call "estimate" the value of a statistic which is used to estimate an unknown population parameter.
`r solend()`


`r qbegin(4)`
What is the difference between a statistic before and after collecting the sample data?

Why is it made? What notational device is used to communicate the distinction?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Before we actually collect the sample, we are not sure which value the statistic will take, hence the statistic in this case is a random variable.

Once we actually collect the sample data and have computed the mean, we have an observed value for the statistic in the sample, called the **observed statistic**. This is just a number that we use as an estimate for the unknown population parameter.

We denote the estimate (observed statistic) with a lowercase letter and the statistic (random variable) with an uppercase letter.
`r solend()`


`r qbegin(8)`
**Sampling distributions**

What is a sampling distribution?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
The sampling distribution is the distribution of the values that a statistic takes on different samples of the same size and from the same population.
`r solend()`


`r optbegin("Data: HollywoodMovies.csv", FALSE, show = TRUE, toggle = params$TOGGLE)`
**Download link**

Download the data [here](https://uoepsy.github.io/data/HollywoodMovies.csv).

**Description**

The data set stores information about 970 movies produced in Hollywood between 2007 and 2013.
It can be considered as the entire population of movies produced in Hollywood in that time period.

Among the recorded variables, three will be of interest:

- `Movie`: title of movie
- `Genre`: one of 14 possible genres
- `Budget`: production budget (in $ millions)


**Preview**

```{r echo=FALSE}
library(kableExtra)
df <- read_csv('https://uoepsy.github.io/data/HollywoodMovies.csv')
gt::gt(head(df))
```


`r optend()`


`r qbegin(5)`
**Reading data into R**

Read the Hollywood movies data into R, and call it `hollywood`.

Check that the data were read into R correctly.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
hollywood <- read_csv('https://uoepsy.github.io/data/HollywoodMovies.csv')
glimpse(hollywood)
```

`r solend()`


`r qbegin(6)`
**Extracting relevant variables**

Extract from the `hollywood` tibble the three variables of interest (`Movie`, `Genre`, `Budget`) and keep the movies for which we have all information (no missing entries).

__Hint:__ Check the help page for the function `drop_na()` or `na.omit()`.

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We can extract variables using the function `select()`, while to keep the rows for which we have all measurements we use `na.omit()`:
```{r}
hollywood <- hollywood %>%
  select(Movie, Genre, Budget) %>%
  drop_na()

hollywood
```
`r solend()`


`r qbegin(7)`
**Proportion of comedy movies**

What is the population proportion of comedy movies?
What is an estimate of the proportion of comedy movies using a sample of size 20?
Using the appropriate notation, report your results in one or two sentences.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
prop_comedy <- hollywood %>%
  summarise(prop = sum(Genre == "Comedy") / n())
prop_comedy

sample_prop_comedy <- hollywood %>%
  rep_sample_n(n = 20, samples = 1) %>%
  group_by(sample) %>%
  summarise(prop = sum(Genre == "Comedy") / n())
sample_prop_comedy
```

The population proportion of comedy movies is $p =$ `r prop_comedy$prop %>% round(2)`, while the proportion of comedy movies in the sample is $\hat{p} =$ `r sample_prop_comedy$prop %>% round(2)`.

`r solend()`


`r qbegin(9)`
**Sampling distribution of the proportion**

Compute the sampling distribution of the proportion of comedy movies for samples of size $n = 20$, using 1000 different samples. 

Is it centred at the population value?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r w11-lab-sampling-dist, message=FALSE, fig.cap="Sampling distribution of the proportion for $n = 20$ with population parameter $p$ marked by a red vertical line."}
sample_props <- hollywood %>%
  rep_sample_n(n = 20, samples = 1000) %>% 
  group_by(sample) %>%
  summarise(prop = sum(Genre == 'Comedy') / n())

ggplot(sample_props, aes(x = prop)) +
  geom_histogram(color = 'white') +
  geom_vline(xintercept = prop_comedy$prop, color = 'red', size = 1) +
  labs(x = 'Sample proportion')
```

Yes, Figure \@ref(fig:w11-lab-sampling-dist) shows that the distribution is almost bell-shaped and centred at the population parameter.
`r solend()`

`r qbegin(10)`
**Standard error**

Using the replicated samples from the previous question, what is the standard error of the sample proportion of comedy movies?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
The standard error of the sample proportion is simply the standard deviation of the distribution of sample proportions for many samples. Since we have already computed the proportions for 1000 samples in the previous question, we just have to compute their variability using the standard deviation:
```{r}
se_prop <- sample_props %>%
  summarise(SE = sd(prop))
se_prop
```

The standard error of the sample proportion for sample size $n = 20$, based on 1000 samples, is $SE$ = `r se_prop$SE %>% round(2)`.
`r solend()`


`r qbegin(11)`
**The effect of sample size on the standard error of the sample proportion**

How does the sample size affect the standard error of the sample proportion?
Compute the sampling distribution for the proportion of comedy movies using 1,000 samples each of size $n = 20$, $n = 50$, and $n = 200$ respectively.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r w11-lab-vary-n, message=FALSE, fig.height=8, fig.width=7, fig.align='center', fig.cap="Three sampling distributions of the proportion, with population parameter $p$ marked by a red vertical line."}
sample_props_20 <- hollywood %>%
  rep_sample_n(n = 20, samples = 1000) %>% 
  group_by(sample) %>%
  summarise(prop = mean(Genre == 'Comedy'))

sample_props_50 <- hollywood %>%
  rep_sample_n(n = 50, samples = 1000) %>% 
  group_by(sample) %>%
  summarise(prop = mean(Genre == 'Comedy'))

sample_props_200 <- hollywood %>%
  rep_sample_n(n = 200, samples = 1000) %>% 
  group_by(sample) %>%
  summarise(prop = mean(Genre == 'Comedy'))

sample_props_vary_n <- bind_rows(
  sample_props_20 %>% mutate(n = 20),
  sample_props_50 %>% mutate(n = 50),
  sample_props_200 %>% mutate(n = 200)
)

ggplot(sample_props_vary_n, aes(x = prop)) +
  geom_histogram(color = 'white') +
  geom_vline(xintercept = prop_comedy$prop, color = 'red', size = 1) +
  facet_grid(n ~ ., labeller = label_both) +
  labs(x = 'Sample proportions', 
       title = "Sampling distribution of proportion for samples of size 20, 50, 200")
```

From Figure \@ref(fig:w11-lab-vary-n) we can see that, as the sample size increases, the standard error of the sample proportion decreases. Increasing the sample size, the spread of the statistic values is reduced.

`r solend()`



`r qbegin(12)`
**Comparing the budget for action and comedy movies**

What is the population average budget (in millions of dollars) allocated for making action vs comedy movies? And the standard deviation?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
budgets <- hollywood %>%
  filter(Genre == 'Action' | Genre == 'Comedy') %>%
  group_by(Genre) %>%
  summarise(avg_budget = mean(Budget),
            sd_budget = sd(Budget))
budgets
```

From the above tibble we see that action movies have been allocated a higher budget ($\mu_{Action} =$ `r budgets$avg_budget[1] %>% round(1)`) than comedy movies ($\mu_{Comedy} =$ `r budgets$avg_budget[2] %>% round(1)`). At the same time, action movies have a higher variability of budgets around the mean value ($\sigma_{Action} =$ `r budgets$sd_budget[1] %>% round(1)` vs $\sigma_{Comedy} =$ `r budgets$sd_budget[2] %>% round(1)`). Note that those numbers are population parameters as they summarise the entire population of Hollywood movies rather than just a sample.a
`r solend()`



---

# Glossary

- **Statistical inference.** The process of drawing conclusions about the population from the data collected in a sample.
- **Population.** The entire collection of units of interest.
- **Sample.** A subset of the entire population.
- **Random sample.** A subset of the entire population, picked at random, so that any conclusion made from the sample data can be generalised to the entire population.
- **Representation bias.** Happens when some units of the population are systematically underrepresented in samples.
- **Generalisability.** When information from the sample can be used to draw conclusions about the entire population. This is only possible if the sampling procedure leads to samples that are representative of the entire population (such as those drawn at random).
- **Parameter.** A fixed but typically unknown quantity describing the population.
- **Statistic.** A quantity computed on a sample.
- **Sampling distribution.** The distribution of the values that a statistic takes on different samples of the same size and from the same population.
- **Standard error.** The standard error of a statistic is the standard deviation of the sampling distribution of the statistic.




<!-- Formatting -->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>

