---
title: "What is probability?"
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
```

```{r include=FALSE}
set.seed(3)
```

::: {.lo}
**LEARNING OBJECTIVES**

1.  Understand the importance of uncertainty in data analysis.
2.  Understand principle of probability as long run frequency.
3.  Understand the concept of assigning probability to events.
:::

## Introduction {.unnumbered}

Think about flipping a coin once. Can you predict the outcome?

Think about flipping a coin many times, one million say. Are you able to
predict roughly how many heads or tails will show up?

It's hard to guess the outcome of just one coin flip because the outcome
could be one of two possible outcomes. Hence, we say that flipping a
coin is a *random experiment* or *random process.*

If you flip it over and over, however, you can predict the proportion of
heads you're likely to see *in the long run.* In the long run simply
means if you were to repeat the same experiment over and over many times
under the same conditions.

This discussion leads us to define the specific type of randomness that
we will be studying in this course.

::: {.frame}
**What is randomness?**

We will say that a repeatable process is random if its outcome is

-   unpredictable in the short run, and
-   predictable in the long run.
:::

It is this long-term predictability of randomness that we will use
throughout the rest of the course. To do that, we will need to talk
about the probabilities of different outcomes and learn some rules for
dealing with them.

## Video activity

Please watch the following video, explaining you to the concept of
"randomness".

<center>

<iframe width="560" height="315" src="https://www.youtube.com/embed/H2lJLXS3AYM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>

</iframe>

</center>

## Random experiments and probability

### Example 1: Flipping a coin

The process of flipping a coin is an example of a random experiment as
its outcome is uncertain. We do not know beforehand whether the coin
will land heads (H) or tails (T).

The collection of all possible outcomes is known as the *outcome space*
or *sample space.* We typically denote the sample space by $S$. In the
coin example, this is:
$$
S = \{ H, T \}
$$

One particular repetition (i.e. instance) of such experiment is known as
a *trial*.

### Example 2: Throwing a die

Another example of a random experiment is throwing a six-faced die as,
for each trial, we can not exactly predict which face will appear.

The list of possible outcomes for the die experiment is $1, 2, ..., 6$.
Hence, the sample space can be written:
$$
S = \{1, 2, 3, 4, 5, 6 \}
$$

### Events

Consider again the die experiment. Often, we are not interested in the
probability of observing a particular outcome, such as 3, but rather in
a collection of outcomes together. For example, we might be interested
in the probability of observing an even number.

Such collections of outcomes are called **events.** More formally, an
event is a set of outcomes.

Each individual outcome is also considered an event. To distinguish,
some people call *simple events* the individual outcomes, and *compound
events* a collection of two or more outcomes.

The event "an even number appears" is simply the collection of even
outcomes. We could call it "E" for "even" and write it
as: $$E = \{ 2, 4, 6\}$$

Two important events are:

-   the **empty set**: the set of no outcomes, denoted $\emptyset$;
-   the **sample space**: the set of all outcomes, denoted $S$.

In general, a finite sample space is written
$$
S = \{s_1, s_2, ..., s_n\}
$$

where each $s_i$ represents an outcome or simple event.

### Example 3: Flipping 2 coins

Consider flipping 2 coins simultaneously. The sample space is:
$$
S = \{ (H,H), (H,T), (T,H), (T,T)\}
$$

Typical events could be:

-   Observing tails at least once, $A = \{(H,T), (T,H), (T,T)\}$
-   Observing the same face twice, $B = \{(H,H), (T,T)\}$

## Defining probability

Consider repeating the random process of throwing a die many times, 500
say, and recording whether or not an even number appears. We will now
create a table listing the result of each trial.

First, we load the `tidyverse` package:

```{r}
library(tidyverse)
```

Next, we create the sample space:

```{r}
S <- 1:6
S
```

The following code defines the event "an even number appears":

```{r}
E <- c(2, 4, 6)
E
```

Say, now, that the outcome of a roll is 3. How can we check whether the
outcome belongs to the event of interest $E$?

We can use the `%in%` function to ask R: "Is 3 in E?" The answer can be
TRUE or FALSE.

```{r}
3 %in% E
```

Suppose instead, that the outcome of a roll is 2. Is 2 in E?

```{r}
4 %in% E
```

We now specify how many trials we will be performing:

```{r}
num_trials <- 500
```

Next, we repeat the experiment `num_trials` times and compute the
accumulated percentage of even outcomes:

```{r}
experiment <- tibble(
    trial = 1:num_trials,
    outcome = sample(S, num_trials, replace = TRUE),
    is_even = outcome %in% E,
    cumul_even = cumsum(is_even),
    cumul_perc_even = 100 * cumsum(is_even) / trial
)
```

The following code displays the top 10 rows of the experiment:

```{r}
head(experiment, n = 10)
```

------------------------------------------------------------------------

**Understanding the code**. Let's inspect each column in turn:

-   `trial` records the number of each trial: 1, 2, ..., 500;
-   `outcome` lists the result of each trial: 1, or 2, ..., or 6;
-   `is_even` checks whether the outcome of each trial belongs to the
    event $E$ (=TRUE) or not (=FALSE);
-   `cumul_even` computes the cumulative sum of `is_even`.

As we saw, the `is_even` column contains either TRUE or FALSE. This was
created with the function `%in%`, which is equivalent to asking a
question: Is the outcome in $E$? The result will be either TRUE or
FALSE. We note that, when summed, R considers a TRUE as 1, and a FALSE
as 0. For example, the cumulative sum of
`c(TRUE, FALSE, TRUE, TRUE, FALSE)` is `c(1, 1, 2, 3, 3)`. Finally, the
column `cumul_perc_even` computes the accumulate percentage of even
outcomes.

------------------------------------------------------------------------

The first trial's outcome was 5, which is not an even number. Hence the
cumulative percentage of even outcomes is 0 out of 100, or 0%. The next
four trials lead to 2, 4, 4, and 2 respectively, which all are even
numbers. The cumulative percentages will be 1 out of 2 (50%), 2 out of 3
(66.67%), 3 out of 4 (75%), and 4 out of 5 (80%). Next, we observe 3,
which is odd, hence the cumulative percentage of even outcomes is now 4
out of 66.67%, and so on.

Finally, we plot the accumulated percentage of even numbers against the
trial number:

```{r}
ggplot(experiment, aes(x = trial, 
                       y = cumul_perc_even)) +
    geom_point(color = 'darkolivegreen4') +
    geom_line(color = 'darkolivegreen4') +
    geom_hline(aes(yintercept = 50), color = 'red', linetype = 2) +
    labs(x = "Trial number", y = "Accumulated percent even")
```

As the number of trials increase, we see that the curve approaches 50%,
which is 0.5, and that the cumulative percentage of even outcomes keeps
fluctuating around 50%.

`r qbegin('Checkpoint', FALSE)`

What's the probability of obtaining an even number when throwing a fair
die?

`r qend()`

`r solbegin()`

If you answered 0.5 (or 50%), then you are on the right track!

`r solend()`

Based on the graph, it looks like the relative frequency of an even
number settles down to about 50%, so saying that the probability is
about 0.5 seems like a reasonable answer.

But do random experiments always behave well enough for this definition
of probability to always apply? Perhaps the relative frequency of an
event can bounce back and forth between two values forever, never
settling on just one number?

## The law of large numbers

Fortunately, Jacob Bernoulli proved the *Law of large numbers (LLN)* in
the 18th century, giving us the peace of mind that we need.

::: {.yellow}
**Law of large numbers**

The law of large numbers (LLN) states that as we repeat a random
experiment over and over, the proportion of times that an event occurs
does settle down to a single number. We call this number the
**probability** of that event.
:::

However, it is not that simple. The LLN requires two key assumptions:

1.  *Identical distribution*: The outcomes of the random experiment must
    have the same probabilities of occurring in each trial. That is, we
    can not have in trial 1 a 50% chance of observing an even number,
    and then a 80% chance in trial 2. The underlying chance needs to be
    the same. This is accomplished by not changing the random experiment
    we are studying, the repeated trials must happen in the *same
    conditions.*
2.  *Independence*: The outcome of one trial must not affect the
    outcomes of other trials.

For the die experiment, we can now write that the probability of
observing an even number is 0.5 as follows. First, we need to define the
event of interest, $E = \{2, 4, 6\}$, and then we can write:
$$
P(E) = 0.5
$$

If you do not give a name to the event, you must specify it inside of
the parentheses. Note the use of round parentheses for probability $P()$
and the curly brackets to list the outcomes of interest. 
$$
P(\{2, 4, 6\}) = 0.5
$$

We reached this definition of the probability of the event $E$. In the
long run,
$$
P(E) = \frac{\text{number of times outcome was in the event } E}{\text{total number of trials}}
$$

::: {.frame}
**NOTATION**

We typically use the first few capital letters of the alphabet to name
events. The letter $P$ will always be reserved for probability.

When we write $P(A) = 0.5$ we mean "the probability of the event $A$ is
0.5".

We use proportions (or decimal numbers) when reporting probability
values in a formal situation like writing a report or a paper. However,
when discussing probability informally, we often use percentages.
:::

## Law of averages = unicorn

You might have heard from friends or TV shows that sometimes the random
experiment "owes" you a particular outcome. Let's try to entangle this in
more detail and understand where the pitfall of this reasoning is.

The law of large numbers tells us that the probability of an event is
the proportion of times we would observe it **in the long run.** The
long run is really long - infinitely long. We, as humans and finite
entities, can not generate an infinitely long sequence of trials and
memorise it.

Many people believe that if you flip a fair coin, where fair means that
the chance of getting heads is the same as the chance of getting tails
(0.5) we expect the coin to "even out" the results in the coming trials
if heads has not appeared in the recent ones.

Say, for example, that in 10 trials you only observed 1 head. This is
quite a low proportion, 0.1 (1 out of 10) compared to the 0.5 (5 out of
10) that the player expected. Does this mean that the coin due to show
heads in the near future, as the coin "owes" us some heads to even out
the proportions?

The answer is **no.**

The long run means that the proportions will eventually even out in the
infinite sequence of trials, but you will not know when this happens and
there is absolutely no requirement for the coin to show heads again in
the upcoming trials in order to keep a probability of 0.5.

## Modelling probability

To assign a probability value to different events, we should make sure
that these coherence principles are satisfied:

**Rule 1: Probability assignment rule**

The probability of an impossible event (an event which never occurs) is
0 and the probability of a certain event (an event which always occurs)
is 1.

Hence, we have that *the probability is a number between 0 and 1*:
$$\text{for any event }A, \\ 0 \leq P(A) \leq 1$$

**Rule 2: Total probability rule**

If an experiment has a single possible outcome, it is not random as that
outcome will happen with certainty (i.e. probability 1).

When dealing with two or more possible outcomes, we need to be sure to
distribute the entire probability among all of the possible outcomes in
the sample space $S$.

The sample space must have probability 1: $$P(S) = 1$$

It must be that the will observe one of the outcomes listed within the
collection of all possible outcomes of the experiment.

**Rule 3: Complement rule**

If the probability of observing the face "2" in a die is 1/6 = 0.17,
what's the probability of not observing the face "2"? It must be 1 - 1/6
= 5/6 = 0.83.

If $A = \{2\}$, the event not A is written $\sim A$, which is a shortcut
for $S$ without $A$, that is $\{1, 3, 4, 5, 6\}$.

$$P(\sim A) = 1 - P(A)$$

**Rule 4: Addition rule for disjoint events**

Suppose the probability that a randomly picked person in a town is $A$ =
"a high school student" is $P(A) = 0.3$ and that the probability of
being $B$ = "a university student" is $P(B) = 0.5$.

What is the probability that a randomly picked person from the town is
*either* a high school student *or* a university student? We write the
event "either A or B" as $A \cup B$, pronounced "A union B".

If you said 0.8, because it is 0.3 + 0.5, then you just applied the
*addition rule*: 
$$
\text{If }A \text{ and } B \text{ are mutually exclusive events,}\\
P(A \cup B) = P(A) + P(B)
$$

**Rule 5: Multiplication rule for independent events**

We saw that probability of observing an even number ($E$) when throwing
a die is 0.5.

You also know that the probability of observing heads ($H$) when
throwing a fair coin is 0.5.

What's the probability of observing an even number and heads (that is,
$E$ *and* $H$, written $E \cap H$) when throwing both items together?

The rule simply says that in this case we multiply the two probabilities
together: 0.5 \* 0.5 = 0.25.

The *multiplication rule for independent events* says: 
$$
\text{If }A \text{ and } B \text{ are independent events,}\\
P(A \cap B) = P(A) \times P(B)
$$

### Probability in the case of equally likely outcomes

Consider a sample space of $n$ outcomes

$$
S = \{s_1, s_2, \dots, s_n \}
$$

and suppose these are all equally likely, with $p$ denoting the probability of each outcome:
$$
P(\{s_1\}) = P(\{s_2\}) = \cdots = P(\{s_n\}) = p
$$

As the outcomes in the sample space are mutually exclusive events,^[In the die example, 1 has nothing in common with 2, 2 has nothing in common with 3, and so on.] we can compute the probability of the sample space as

$$
1 = P(S) = P(\{s_1\}) + P(\{s_2\}) + \cdots + P(\{s_n\}) = p + p + \cdots + p = n p
$$

which leads to
$$
p = P(\{s_i\}) = \frac{1}{n}
$$

Next, consider an event $A$ comprising a few of the outcomes from $S$

$$
A = \{s_2, s_5, s_9\}
$$
which can be also written as the union of disjoint events

$$
A = \{s_2\} \cup \{s_5\} \cup \{s_9\}
$$

We can compute the probability of $A$ as follows

$$
\begin{aligned}
P(A) &= P(\{s_2\}) + P(\{s_5\}) + P(\{s_9\}) \\
&= p + p + p \\
&= 3 p \\
&= 3 \left(\frac{1}{n}\right) \\
&= \frac{3}{n} \\
&= \frac{n_A}{n} \\
&= \frac{\text{number of outcomes within }A}{\text{number of possible outcomes}}
\end{aligned}
$$
where $n_A$ is the number of outcomes within $A$ and $n$ is the total number of possible outcomes in $S$.




------------------------------------------------------------------------

## Glossary

-   **Random experiment.** A process or phenomenon which can have two
    ore more possible outcomes.
-   **Trial.** A single repetition of the experiment
-   **Outcome.** The value observed after running a trial
-   **Sample space.** The collection of all possible outcomes. The
    sample space is denoted $S$ and has probability 1.
-   **Event.** Either a single outcome (simple event) or a collection of
    outcomes (compound event).
-   **Probability.** A number reporting how likely is an even to occur
    when performing a trial (that is, obtaining an outcome within that
    event or satisfying the proposition of the event)
-   **Disjoint (or mutually exclusive) events.** Two events $A$ and $B$
    are disjoint if they share no outcomes in common. For disjoint
    events, knowing that one event occurs tells us that the other cannot
    occur.
-   **Independent events.** Two events are independent if learning that
    one event occurs does not change the probability that the other
    event occurs.
-   **Probability assignment rule.** Says that the probability of any
    event must be between 0 (the probability of an impossible event) and
    1 (the probability of a certain event).
-   **Total probability rule.** The probability that the experiment's
    outcome is in the sample space must be 1.
-   **Complement rule.** If $P(A)$ denotes the probability of the event
    $A$ occurring, the probability of "not A" is $P(\sim A) = 1 - P(A)$.
-   **Addition rule for disjoint events.** If $A$ and $B$ are disjoint
    events, then $P(A \cup B) = P(A) + P(B)$.
-   **Multiplication rule for independent events.** If A and B are
    independent events, then $P(A \cap B) = P(A) \times P(B)$.

------------------------------------------------------------------------



## Exercises

<!--# 1 -->

`r qbegin(1)`

**Sample spaces**. For each of the following experiments list the sample
space and tell whether you think the outcomes are equally likely or not.

a.  Rolling 2 six-sided dice.
b.  A person has recently travelled back to the UK from abroad and is about to take a COVID-19 test.

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

a.  The sample space is: $$
    S = \left\{
    \begin{matrix}
    (1,1) & (1,2) & (1,3) & (1,4) & (1,5) & (1,6) \\
    (2,1) & (2,2) & (2,3) & (2,4) & (2,5) & (2,6) \\
    (3,1) & (3,2) & (3,3) & (3,4) & (3,5) & (3,6) \\
    (4,1) & (4,2) & (4,3) & (4,4) & (4,5) & (4,6) \\
    (5,1) & (5,2) & (5,3) & (5,4) & (5,5) & (5,6) \\
    (6,1) & (6,2) & (6,3) & (6,4) & (6,5) & (6,6)
    \end{matrix}
    \right\}
    $$

    All of the outcomes are equally likely to occur. Each outcome has a chance of 1/36 of occurring.

b.  The sample space is $S = \{ \text{Positive}, \text{Negative} \}$. 

    The two outcomes are not equally likely. The chance of the test being positive is higher if they have been in contact with anyone positive.

`r solend()`



<!--# 2 -->

`r qbegin(2)`

**Rolling a die.** Consider rolling a six-sided die. Recall that each of
the die's six faces (1-6) are equally likely to occur, meaning that each
has a *long-run probability* of 1/6.

Relate the terms *trial*, *outcome*, *event*, and *sample space* to a
single roll of a die die. Provide an example of each.

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

-   A trial a single roll of the die.
-   An outcome is the face resulting after rolling the die. For example,
    when rolling the die, we might observe the face 4.
-   An event is a collection of one or more faces. For example, we might
    be interested in the event "even number", which can be written
    $E = \{2, 4, 6\}$.
-   The sample space is the collection of all possible faces of the die.
    In this case, the sample space is $S = \{1, 2, 3, 4, 5, 6\}$.

`r solend()`



<!--# 3 -->

`r qbegin(3)`

Consider again rolling a six-sided die. State whether each of the
following is a simple or a compound event. Next, calculate the
corresponding probability either by directly listing the involved
outcomes or by applying the probability rules (addition rule, complement
rule, multiplication rule).

a.  Rolling a 6
b.  Rolling a number less than or equal to 3
c.  Rolling a number other than 1
d.  Rolling a 1 or an even number

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

The sample space is $S = \{1, 2, 3, 4, 5, 6\}$. The number of its
elements is $n = 6$. Furthermore, as the outcomes are equally likely,
we have that 
$$
P(\{1\}) = P(\{2\}) = \cdots = P(\{6\}) = \frac{1}{6}
$$

a.  The simple event "Rolling a 6" can only happen when we observe "6". Let $A = \{ 6 \}$. Then,
    $$
    P(A) = \frac{n_A}{n} = \frac{1}{6} \approx 0.167
    $$
b.  The compound event "Rolling a number $\leq 3$" can happen when we
    observe 1, 2, or 3. Let $B = \{1, 2, 3\}$. Then,
    $$
    P(B) = \frac{n_B}{n} = \frac{3}{6} = \frac{1}{2}.
    $$ 
    
    Alternatively, as $B$ is the union of the disjoint events
    $\{1\}, \{2\}, \{3\}$, we can sum the probabilities of each simple
    event: 
    $$
    P(B) = P(\{1\} \cup \{2 \} \cup \{3\}) 
    = P(\{1\}) + P(\{2\}) + P(\{3\}) = 3 \times \frac{1}{6} 
    = \frac{1}{2}
    $$
    
c.  The compound event "Rolling a number other than 1" can happen when
    we observe 2, 3, 4, 5, 6. Let $C = \{2, 3, 4, 5, 6\}$. Then,
    $$
    P(C) = \frac{n_C}{n} = \frac{5}{6} \approx 0.833.
    $$ 
    
    Alternatively, as $C$ is equivalent to $\sim \{1\}$ (read "not
    {1}"), we can use the complement rule:
    $$
    P(\sim \{1\}) = 1 - P(\{1\}) = 1 - \frac{1}{6} 
    = \frac{5}{6} \approx 0.833.
    $$
    
d.  The compound event "Rolling a 1 or an even number" can happen when
    we observe 1, 2, 4, 6. Let $D = \{1, 2, 4, 6\}$. Then,
    $$
    P(D) = \frac{n_D}{n} = \frac{4}{6} \approx 0.667
    $$ 
    
    Alternatively, $D$ can be written as the union of four disjoint
    simple events $$
    \begin{aligned}
    P(D) &= P(\{1\} \cup \{2\} \cup \{4 \} \cup \{6\}) \\
    &= P(\{1\}) + P(\{2\}) + P(\{4 \}) + P(\{6\}) \\
    &= 4 \times \frac{1}{6} \\
    &= \frac{4}{6} \\
    &\approx 0.667
    \end{aligned}
    $$

`r solend()`



<!--# 4 -->

`r qbegin(4)`

**Playing cards.** Consider a standard deck of 52 playing cards. These
are shown in the diagram below:

```{r cards-deck, echo=FALSE, fig.cap='Example set of 52 playing cards; 13 of each suit clubs, diamonds, hearts, and spades. Source: Wikipedia (https://en.wikipedia.org/wiki/Standard_52-card_deck)', fig.align='center'}
knitr::include_graphics('images/prob/card-deck.png')
```

Relate the terms *trial*, *outcome*, *event*, and *sample space* to a
single draw from a standard deck of cards. Provide an example of each.

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)` 

- A trial represents a single draw of a card from the deck of 52 playing cards. 
- The outcome is the result of a draw. For example, drawing a card from
the deck might result in the 5 of Hearts. 
- Events are combinations of one or more cards. For example, we might be interested in whether the event "drawing a red card" occurs. This event can be written $R$ = \{AceDiamonds, 2Diamonds, ..., KingDiamonds, AceHearts, 2Hearts, ..., KingHearts\}. We say the event occurred if the result of a trial is a card within that collection.
- The sample space is the collection of all possible cards that can be drawn from the deck. In our case, the 52 cards shown in Figure \@ref(fig:cards-deck). 

`r solend()`

<br>

`r optbegin("Data: TipJoke.csv. Click to expand", FALSE, show = TRUE, toggle = params$TOGGLE)`

**Download link**

Download the data here: https://uoepsy.github.io/data/TipJoke.csv

**Description**

A [study](https://doi.org/10.1111/j.1559-1816.2002.tb00266.x) published
in the Journal of Applied Social Psychology[^probability_basics-2]
investigated if telling a joke affected whether or not a customer let a tip. 

[^probability_basics-2]: Gueaguen, N. (2002). The Effects of a Joke on
    Tipping When It Is Delivered at the Same Time as the Bill. *Journal
    of Applied Social Psychology, 32*(9), 1955-1963.


The waiter at a coffee bar of a famous seaside resort on the west Atlantic coast of France randomly assigned coffee-ordering customers to one of three
groups.

When receiving the bill, one group also received a card telling a joke,
another group received a card containing an advertisement for a local
restaurant, and a third group received no card at all.

The dataset contains the variables:

-   `Card`: None, Joke, Ad.
-   `Tip`: 1 = The customer left a tip, 0 = The customer did not leave
    tip.

**Preview**

The first six rows of the data are:

```{r echo=FALSE}
library(tidyverse)
library(kableExtra)
tipjoke <- read_csv('https://uoepsy.github.io/data/TipJoke.csv')
kable(head(tipjoke), align='c') %>% 
    kable_styling(full_width = FALSE)
```

`r optend()`

<!--# 5 -->

`r qbegin(5)` 

Load the TipJoke.csv data into R and call it `tipjoke`.

Recode the Tip variable so that

- 1 becomes "Tipped"
- 0 becomes "Not tipped"

Make sure that categorical variables are correctly encoded as factors.

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

Load the data into R:

```{r}
tipjoke <- read_csv('https://uoepsy.github.io/data/TipJoke.csv')
```

Check if the data were read into R correctly by examining the top 6
rows:

```{r}
head(tipjoke)
```

Recode the Tip variable:

```{r}
tipjoke <- tipjoke %>%
  mutate(Tip = ifelse(Tip == 1, 'Tipped', 'Not tipped'))

head(tipjoke)
```


:::frame
**What is ifelse?**

The function ifelse works as follows: `ifelse(test, value_when_true, value_when_false)`.

`test` is any condition on a vector. For example, if `x <- c(2, 4, 6, 9)`, test could be `x < 6`. This will return `c(TRUE, TRUE, FALSE, FALSE)`. 
The TRUEs will be replaced with the value specified in `value_when_true`, and the FALSEs will be replaced with the value specified in `value_when_false`.

Other possible tests are `x > 6`, `x == 6`, `x <= 6`, `x >= 6`, and so on...
:::


Both variables represent categorical variables and so should be encoded
as factors (`<fct>`).

There are many different equivalent ways to do so, see below.

**Option 1.**

```{r}
tipjoke <- tipjoke %>% 
  mutate(
    Card = as.factor(Card),
    Tip = as.factor(Tip)
  )

head(tipjoke)
```

**Option 2.**

```{r eval=FALSE}
tipjoke$Card <- as.factor(tipjoke$Card)
tipjoke$Tip  <- as.factor(tipjoke$Tip)
```

**Option 3. (Advanced)**

```{r eval=FALSE}
tipjoke <- tipjoke %>%
    mutate(across(c(Card, Tip), as.factor))
```

**Option 4. (Advanced)**

```{r eval=FALSE}
tipjoke <- tipjoke %>%
    mutate(across(everything(), as.factor))
```

`r solend()`


<!-- # 6 -->

`r qbegin(6)`
Define the sample space for the card tipping experiment.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
The set of possible outcomes is 
$$
S = \left\{
\begin{matrix}
(None,\ Tipped) & (None,\ Not\ Tipped) \\
(Joke,\ Tipped) & (Joke,\ Not\ Tipped) \\
(Ad,\ Tipped) & (Ad,\ Not\ Tipped) \\
\end{matrix}
\right\}
$$
`r solend()`



<!-- # 7 -->

`r qbegin(7)` 

Create a contingency table displaying how many customers
who were given no card, a a joke card, or an advertisement card left a
tip or not. 

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)` 

Create a table of counts:

```{r}
freq_tbl <- tipjoke %>%
    table()
freq_tbl
```

`r solend()`



<!-- # 8 -->

`r qbegin(8)`

Transform the table of counts to a relative frequency table.

Do the numbers in the table satisfy the requirements of probabilities?

Display the relative frequency table as a mosaic plot and comment on
what it highlights. 

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)` 

Convert it to a table of proportions:

```{r}
rel_freq_tbl <- freq_tbl %>%
    prop.table()
rel_freq_tbl
```


All values in the proportions table are greater than or equal 0:
```{r}
all(rel_freq_tbl >= 0)
```

All values in the proportions table are less than or equal 1:
```{r}
all(rel_freq_tbl <= 1)
```

The values in the relative frequency table sum to 1, which is the probability of the sample space:
```{r}
sum(rel_freq_tbl)
```


Visualise the relative frequency table as a mosaic plot:
```{r}
plot(rel_freq_tbl)
```

The mosaic plot shows that roughly the same number of customers were
given no card, an ad card or a joke card. Those who were given a joke
card had the highest number of tips. The number of tipping customers
doesn't seem to differ too much between those who were given no card or
an advertisement card.

`r solend()`



<!--# 9 -->

`r qbegin(9)`

The proportions computed above can be considered as the probabilities for a person randomly drawn from the participants in the experiment.

Using the proportions table compute above,

1.  What's the probability that a randomly chosen customer isn't given a
    card and doesn't tip?
2.  What's the probability that a randomly chosen customer is given an
    advertisement card and tips?
3.  What's the probability that a randomly chosen customer is given a
    joke card and tips?
4.  What's the probability that a randomly chosen customer is given some
    card and tips?
5.  What's the probability that a randomly chosen customer is given some
    card?

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)` 

1. P(no card and doesn't tip) = 0.232
2. P(ad card and tips) = 0.0664
3. P(joke cards and tips) = 0.142
4. P(ad card and tips) + P(joke card and tips) = 0.06635071 + 0.14218009 = 0.209
5. 1 - (P(none and doesn't tip) + P(none and tips)) = 1 - (0.23222749 + 0.07582938) = 0.692 

`r solend()`


<br>


Can telling a joke affect whether or not a waiter in a coffee bar
receives a tip from a customer?

To be continued next week...



<!--# Formatting -->

::: {.tocify-extend-page data-unique="tocify-extend-page" style="height: 0;"}
:::
