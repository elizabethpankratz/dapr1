---
title: "Visualising and describing relationships"
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
```


```{r include=FALSE}
library(tidyverse)
library(patchwork)
```

:::green
__Information about solutions__

Solutions for these exercises are available immediately below each question.  
We would like to emphasise that much evidence suggests that testing enhances learning, and we __strongly__ encourage you to make a concerted attempt at answering each question *before* looking at the solutions. Immediately looking at the solutions and then copying the code into your work will lead to poorer learning.  
We would also like to note that there are always many different ways to achieve the same thing in R, and the solutions provided are simply _one_ approach.  

Be sure to check the [**solutions to last week's exercises**](03_numerical.html).<br>You can still ask any questions about previous weeks' materials if things aren't clear!
:::
<!-- # Visualising and describing relationships {#chap-relationships} -->

:::lo
**LEARNING OBJECTIVES**

+ LO1: Understand the relation between X-Y (explanatory/outcome) specification and practical research questions.
+ L02: Understand how to summarise and visualize numeric-categorical relationships.
+ LO3: Understand how to summarise and visualize numeric-numeric relationships.

:::

# Outcome vs Explanatory 

In the previous couple of labs, we looked at how to handle different types of data, and how to describe and visualise categorical and numeric distributions.
More often than not, research involves investigating _relationships_ between variables, rather than studying variables in isolation.  

:::yellow 
If we are using one variable to help us understand or predict values of another variable, we call the former the __explanatory variable__ and the latter the __outcome variable__. 

<div style="margin-left: 2em;">
_**Other names**_

+ _outcome variable = dependent variable = response variable = Y_
+ _explanatory variable = independent variable = predictor variable = X_   
  
(referring to outcome/explanatory variables as _Y_ and _X_ respectively matches up with how we often want to plot them - the outcome variable on the y-axis, and the explanatory variable on the x-axis)
</div>  

:::

The distinction between explanatory and outcome variables is borne out in how we design __experimental studies__ - the researcher manipulates the explanatory variable for each unit _before_ the response variable is measured (for instance, we might randomly allocate participants to one of two conditions). 
This contrasts with __observational studies__ in which the researcher does not control the value of any variable, but simply observes the values as they naturally exist.  

We're going to pick up again with the data collected on the Stroop task from the previous lab.  
```{r}
library(tidyverse)
stroopdata <- read_csv("https://uoepsy.github.io/data/strooptask.csv")

# calculate the "stroop effect" - the difference in time taken to complete
# the matching vs mismatching sets 
stroopdata <- 
    stroopdata %>% 
        mutate(
            stroop_effect = mismatching - matching
        )
```

The data is experimental - researchers controlled the presentation of the stimuli (coloured words) and the assignment of whether or not participants received practice.  

The researchers are interested in two relationships: 

1. the relationship between receiving practice (*categorical*) and the stroop-effect (*numeric*)
2. the relationship between age (*numeric*) and the stroop-effect (*numeric*)

# Numeric and Categorical  

Recall that the "stroop-effect" is the difference (in seconds) between participants' times on the mismatching set of words vs the matching set. We know how to describe a numeric variable such as the stroop-effect, for instance by calculating the mean and standard deviation, or median and IQR. We saw how to produce visualisations of numeric variables in the form of density curves, histogram, and boxplots.  

```{r}
# take the "stroopdata" dataframe %>%
# summarise() it, such that there is a value called "mean_stroop", which
# is the mean() of the "stroop_effect" variable, and a value called "sd_stroop", which
# is the standard deviation of the "stroop_effect" variable.
stroopdata %>% 
  summarise(
    mean_stroop = mean(stroop_effect),
    sd_stroop = sd(stroop_effect)
  )
```

```{r}
ggplot(data = stroopdata, aes(x = stroop_effect)) + 
  geom_histogram()
```

To understand the relationship between *categorical* (practice) and the *numeric* (stroop effect), for now we will simply calculate these summary statistics for the numeric variable when it is split by the different levels in the categorical variable. 

In other words, we want to calculate the mean and standard deviation of the _stroop_effect_ variable separately for those observations where _practice_ is "no", and for those where _practice_ is "yes":
```{r echo=FALSE}
set.seed(421)
sample_n(stroopdata, n()) %>% select(practice,stroop_effect) %>% mutate(stroop_effect = round(stroop_effect,2)) %>% head(9L) %>% rbind("...") %>% knitr::kable()
```

<br> 
We can do this using the `group_by()` function. 

:::yellow
__group_by()__

The `group_by()` function creates a _grouping_ in the dataframe, so that subsequent functions will be computed _on each group._  

It is most useful in combination with `summarise()`, to reduce a variable into a summary value _for each group in a grouping variable_:  

```{r eval=FALSE}
# take the data %>%
# make it grouped by each unique value in the "grouping_variable" %>%
# summarise() it FOR EACH GROUP, creating a value called "summary_value" ()
data %>% 
  group_by(grouping_variable) %>%
  summarise(
    summary_value = ...
  )
```
  
:::

Let's do this for the Stroop Task data - we will `summarise()` the _stroop_effect_ variable, **after** grouping the data by the _practice_ variable:
```{r}
# take the "stroopdata" %>%
# and group it by each unique value in the "practice" variable (yes/no) %>%
# then summarise() it FOR EACH GROUP, creating summary values called 
# "mean_stroop" and "sd_stroop" which are the means and standard deviations of 
# the "stroop_effect" variable entries for each group of "practice".
stroopdata %>%
  group_by(practice) %>%
  summarise(
    mean_stroop = mean(stroop_effect),
    sd_stroop = sd(stroop_effect)
  )
```

## Visualising - Colours {-}  

`r qbegin("Question", qlabel=FALSE)`
Given the output above, which of the following visualisations is most representative of these statistics?
```{r echo=FALSE}
tibble(
  x = seq(-10, 25, 0.2),
  y = dnorm(x, 4.5, 4.2),
  y2 = dnorm(x, 0, 4.2)
) -> practdat

p1 <- ggplot(practdat,aes(x=x))+
  geom_line(aes(y=y, col="no"))+
  geom_line(aes(y=y2, col="yes"))+
  scale_color_manual("Practice",breaks = c("no","yes"),values=c("tomato1","blue"))+
  theme_light()+
  theme(axis.line.y = element_blank(), axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), axis.title.y = element_blank(),
        legend.position = "none")+labs(x="", title="B")
p2 <- ggplot(practdat,aes(x=x))+
  geom_line(aes(y=y2, col="no"))+
  geom_line(aes(y=y, col="yes"))+
  scale_color_manual("Practice",breaks = c("no","yes"),values=c("tomato1","blue"))+
  theme_light()+
  theme(axis.line.y = element_blank(), axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), axis.title.y = element_blank(),
        legend.position = "bottom")+
  labs(x="stroop effect", title="C")

tibble(
  x = seq(-10, 25, 0.2),
  y = dnorm(x, 10, 4.2),
  y2 = dnorm(x, 14.5, 4.2)
) %>% ggplot(.,aes(x=x))+
  geom_line(aes(y=y, col="no"))+
  geom_line(aes(y=y2, col="yes"))+
  scale_color_manual("Practice",breaks = c("no","yes"),values=c("tomato1","blue"))+
  theme_light()+
  theme(axis.line.y = element_blank(), axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), axis.title.y = element_blank(),
        legend.position = "none")+labs(x="", title="A")-> p3

tibble(
  x = seq(-10, 25, 0.2),
  y = dnorm(x, 4.5, 4.2),
  y2 = dnorm(x, 0, 2.2)
) %>% ggplot(.,aes(x=x))+
  geom_line(aes(y=y, col="no"))+
  geom_line(aes(y=y2, col="yes"))+
  scale_color_manual("Practice",breaks = c("no","yes"),values=c("tomato1","blue"))+
  theme_light()+
  theme(axis.line.y = element_blank(), axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), axis.title.y = element_blank(),
        legend.position = "none")+labs(x="stroop effect", title="D")-> p4

(p3 + p1) / (p2 + p4)

```
`r qend()`
`r solbegin("Answer", slabel=FALSE, show=TRUE)`
We know that the stroop effect for those _with_ practice (blue line) was on average less than those _without_ practice (red line). Both figures A and C don't fit with this.  

In _both_ of the figures B and D, the blue (with practice) distribution peaks at about 0, and the red (without practice) distribution peaks at about 5. However, in the figure D, the red distribution is much flatter and wider. It has a larger standard deviation than the blue distribution. In our calculations above, the distributions have very similar standard deviations.  
<br>
So the best visualisation of the two means and standard deviations we calculated is figure B. 
`r solend()`

We can visualise the data using the same code we had before, but with one small addition - we tell ggplot to _colour_ the data according to the different values in the _practice_ variable.  
<small>
__Note__ we add this inside the `aes()` mappings, because we are mapping something on the plot (the colour) to something in the data (the _practice_ variable). If we just wanted to make the line blue, we could put `col = "blue"` _outside_ the `aes()`.
</small>
```{r}
ggplot(data = stroopdata, aes(x = stroop_effect, col = practice)) +
  geom_density()
```

## Visualising - Facets {-}

Interpreting two density curves on top of one another works well, but overlaying two histograms on top of one another doesn't. Instead, we might want to create separate histograms for each set of values (the _stroop_effect_ variable values for each of practice/no practice groups).  

`facet_wrap()` is a handy part of `ggplot` which allows us to easily split one plot into many:  
```{r}
ggplot(data = stroopdata, aes(x = stroop_effect)) +
  geom_histogram() +
  facet_wrap(~practice)
```



# Numeric and Numeric

When we are interested in the relationship between two _numeric_ variables, such as the one we have between age and the stroop-effect, the most easily interpreted visualisation of this relationship is in the form of a scatterplot:

```{r}
# make a ggplot with the stroopdata
# put the possible values of the "age" variable on the x axis,
# and put the possible values of the "stroop_effect" variable on the y axis.
# for each entry in the data, add a "tomato1" coloured geom_point() to the plot, 
ggplot(data = stroopdata, aes(x = age, y = stroop_effect)) +
  geom_point(col="tomato1")
```

The visual pattern that these points make on the plot tells us something about the data - it looks like the older participants tended to have a greater stroop-effect.  
But we can also have relationships between two numeric variables that look the opposite, or have no obvious pattern, or have a more consistent patterning (see Figure \@ref(fig:numnumrels))
```{r numnumrels, echo=FALSE, fig.cap = "Relationships between two numeric variables can look very different"}
tibble(x=rnorm(100,100,15),
       y=-x*2+rnorm(100,0,10)) %>%
  ggplot(.,aes(x=x,y=y))+
  geom_point() + 
  theme_minimal()+
  theme(axis.text = element_blank(), axis.ticks = element_blank())+
  labs(title="observations with higher values of x\ntend to have lower values for y\n(obvious pattern)") -> p1


tibble(x=rnorm(100,100,15),
       y=rnorm(100)) %>%
  ggplot(.,aes(x=x,y=y))+
  geom_point() + 
  theme_minimal()+
  theme(axis.text = element_blank(), axis.ticks = element_blank())+
  labs(title="observations with higher values of x\ndo not tend to have systematically\nhigher/lower values for y\n(No pattern)") -> p2

p1 + p2
```


As a means of summarising these different types of relationships, we can calculate the __covariance__ to describe in what direction, and how strong (i.e., how clear and consistent) the pattern is.   

## Covariance {-}  

We know that __variance__ is the measure of how much a _single numeric variable_ varies around its mean.  
__Covariance__ is a measure of how _two numeric variables_ vary together, and can express the directional relationship between them.

:::yellow
__Covariance__ is the measure of how two variables vary together.  
For samples, covariance is calculated using the following formula:

$$\mathrm{cov}(x,y)=\frac{1}{n-1}\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})$$

where:

- $x$ and $y$ are two variables;
- $i$ denotes the observational unit, such that $x_i$ is value that the $x$ variable takes on the $i$th observational unit, and similarly for $y_i$;
- $n$ is the sample size.

:::  


```{r echo=FALSE, message=FALSE,warning=FALSE}
set.seed(7135)
tibble(x=runif(10,20,80),y=rnorm(10,160,10)) %>%
  mutate(y=y+x/2,
         coldir = ifelse( (x>mean(x) & y>mean(y)) | (x<mean(x) & y<mean(y)), "pos","neg")
  ) -> df

p1<-ggplot(df,aes(x=x,y=y))+
  geom_point()+
  theme_classic()
    
p2<-ggplot(df,aes(x=x,y=y))+
  geom_point()+
  theme_classic()+
  geom_vline(aes(xintercept=mean(x)), lty="dashed")+
  annotate("text",x=mean(df$x)+1, y=max(df$y)-5,label=expr(bar("x")))+
  geom_hline(aes(yintercept=mean(y)), lty="dashed")+
  annotate("text",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar("y")))
  
p3<-ggplot(df,aes(x=x,y=y))+
  geom_point()+
  theme_classic()+
  geom_vline(aes(xintercept=mean(x)), lty="dashed")+
  annotate("text",x=mean(df$x)-1, y=max(df$y)-5,label=expr(bar("x")))+
  geom_hline(aes(yintercept=mean(y)), lty="dashed")+
  annotate("text",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar("y")))+
  annotate("text",x=mean(df$x)+7, y=202,label=expression(x[i]-bar("x")))+
  annotate("text",x=71.5, y=mean(df$y)+7,label=expression(y[i]-bar("y")))+
  geom_segment(aes(x = mean(x), y = 200.9559, xend = 69.47989, yend = 200.9559), color="tomato1", data = df)+
  geom_segment(aes(x = 69.47989, y = mean(y), xend = 69.47989, yend = 200.9559), color="tomato1",data = df)


p4<-ggplot(df,aes(x=x,y=y))+
  geom_point(aes(col=coldir))+
  scale_color_manual("",values=c("black","red"))+
  theme_classic()+
  theme(legend.position = "none")+
  geom_vline(aes(xintercept=mean(x)), lty="dashed")+
  annotate("text",x=mean(df$x)-1, y=max(df$y)-5,label=expr(bar("x")))+
  geom_hline(aes(yintercept=mean(y)), lty="dashed")+
  annotate("text",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar("y")))+
  geom_segment(aes(x = mean(x), y = 200.9559, xend = 69.47989, yend = 200.9559), color="tomato1", data = df)+
  geom_segment(aes(x = 69.47989, y = mean(y), xend = 69.47989, yend = 200.9559), color="tomato1",data = df)+
  geom_segment(aes(x = mean(x), y = 211.5757, xend = 56.36053, yend = 211.5757), color="tomato1", data = df)+
  geom_segment(aes(x = 56.36053, y = mean(y), xend = 56.36053, yend = 211.5757), color="tomato1",data = df)+
  geom_segment(aes(x = mean(x), y = 180.5799, xend = 44.37031, yend = 180.5799), color="tomato1", data = df)+
  geom_segment(aes(x = 44.37031, y = mean(y), xend = 44.37031, yend = 180.5799), color="tomato1",data = df)

p5<-ggplot(df,aes(x=x,y=y))+
  geom_point(aes(col=coldir))+
  scale_color_manual("",values=c("blue","red"))+
  theme_classic()+
  theme(legend.position = "none")+
  geom_vline(aes(xintercept=mean(x)), lty="dashed")+
  annotate("text",x=mean(df$x)-1, y=max(df$y)-5,label=expr(bar("x")))+
  geom_hline(aes(yintercept=mean(y)), lty="dashed")+
  annotate("text",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar("y")))+
  geom_segment(aes(x = mean(x), y = 200.9559, xend = 69.47989, yend = 200.9559), color="tomato1", data = df)+
  geom_segment(aes(x = 69.47989, y = mean(y), xend = 69.47989, yend = 200.9559), color="tomato1",data = df)+
  geom_segment(aes(x = mean(x), y = 211.5757, xend = 56.36053, yend = 211.5757), color="tomato1", data = df)+
  geom_segment(aes(x = 56.36053, y = mean(y), xend = 56.36053, yend = 211.5757), color="tomato1",data = df)+
  geom_segment(aes(x = mean(x), y = 180.5799, xend = 44.37031, yend = 180.5799), color="tomato1", data = df)+
  geom_segment(aes(x = 44.37031, y = mean(y), xend = 44.37031, yend = 180.5799), color="tomato1",data = df)+
  geom_segment(aes(x = mean(x), y = 194.4188, xend = 45.32536, yend = 194.4188), color="skyblue3", data = df)+
  geom_segment(aes(x = 45.32536, y = mean(y), xend = 45.32536, yend = 194.4188), color="skyblue3",data = df)+
  geom_segment(aes(x = mean(x), y = 182.6440, xend = 66.73541, yend = 182.6440), color="skyblue3", data = df)+
  geom_segment(aes(x = 66.73541, y = mean(y), xend = 66.73541, yend = 182.6440), color="skyblue3",data = df)
```

It often helps to understand __covariance__ by working through a visual explanation.  
<br>
Consider the following scatterplot:  
```{r echo=FALSE, message=FALSE}
p1
```
<br>
Now let's superimpose a vertical dashed line at the mean of $x$ ($\bar{x}$) and a horizontal dashed line at the mean of $y$ ($\bar{y}$):
```{r echo=FALSE, message=FALSE, warning=FALSE}
p2
```
<br>
Now let's pick one of the points, call it $x_i$, and show $(x_{i}-\bar{x})$ and $(y_{i}-\bar{y})$.  
Notice that this makes a rectangle.  
As $(x_{i}-\bar{x})$ and $(y_{i}-\bar{y})$ are both positive values, their product -  $(x_{i}-\bar{x})(y_{i}-\bar{y})$ - is positive. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
p3
```
<br>
In fact, for all these points in red, the product $(x_{i}-\bar{x})(y_{i}-\bar{y})$ is positive (remember that a negative multiplied by a negative gives a positive): 
```{r echo=FALSE, message=FALSE, warning=FALSE}
p4
```
<br>
And for these points in blue, the product $(x_{i}-\bar{x})(y_{i}-\bar{y})$ is negative:  
```{r echo=FALSE, message=FALSE, warning=FALSE}
p5
```
<br>
Now take another look at the formula for covariance:  

$$\mathrm{cov}(x,y)=\frac{\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1}$$
  
It is the sum of all these products divided by $n-1$. It is the average of the products!  
<br>
We can easily calculate the covariance between variables in R using the `cov()` function. 
`cov()` takes two variables `cov(x = , y = )`.  
We can either use the `$` to pull out the variables from the datset:  
```{r}
cov(stroopdata$age, stroopdata$stroop_effect)
```
Or we can specify the dataframe, use the `%>%` symbol, and call `cov()` inside `summarise()`:  
```{r}
stroopdata %>%
  summarise(
    mean_age = mean(age),
    mean_stroop = mean(stroop_effect),
    cov_agestroop = cov(age, stroop_effect)
  )
```

# Categorical and Categorical  

What if we are interested in the relationship between two variables that are both categorical?  

As a quick example, let's read in a dataset containing information on passengers from the Titanic. 
We can see from the first few rows of the dataset that there are quite a few categorical variables here: 
```{r}
titanic <- read_csv("https://uoepsy.github.io/data/titanic.csv")
head(titanic)
```

Recall that we summarised one categorical variable using a **frequency table**:
```{r}
titanic %>%
  count(survived)
```

We can also achieve this using the `table()` function:
```{r eval=FALSE}
#thes two lines of code do exactly the same thing!
table(titanic$survived)
titanic %>% select(survived) %>% table()
```
```{r echo=FALSE}
table(titanic$survived)
```

## Contingency Tables  

Let's suppose we are interested in how the Class of passengers' tickets (1st Class, 2nd Class, 3rd Class) can be used to understand their survival.  
We can create **two-way** table, where we have each variable on either dimension of the table:  
```{r eval=FALSE}
#Either this:
table(titanic$class, titanic$survived)
# or this:
titanic %>% select(class, survived) %>% table()
```
```{r echo=FALSE}
table(titanic$class, titanic$survived)
```

And we can pass this to `prop.table()` to turn these into proportions.  
We can turn them into:

1. proportions of the total:  
    ```{r}
    titanic %>% 
      select(class, survived) %>% 
      table() %>%
      prop.table()
    ```
2. proportions of each row:  
    ```{r}
    titanic %>% 
      select(class, survived) %>% 
      table() %>%
      prop.table(margin = 1)
    ```
3. proportions of each column:  
    ```{r}
    titanic %>% 
      select(class, survived) %>% 
      table() %>%
      prop.table(margin = 2)
    ```

## Mosaic Plots  

The equivalent way to visualise a contingency table is in the form of a **mosaic plot**.  

```{r}
titanic %>% 
  select(class, survived) %>% 
  table() %>%
  plot()
```

You can think of the `prop.table(margin = )` as scaling the areas of one of the variables to be equal:
```{r}
titanic %>% 
  select(class, survived) %>% 
  table() %>%
  prop.table(margin = 1)
```
In the table above, each row (representing each level of the "Class" variable) sums to 1. 
The equivalent plot would make each of level of the "Class" variable as the same area:
```{r}
titanic %>% 
  select(class, survived) %>% 
  table() %>%
  prop.table(margin = 1) %>%
  plot()
```


---


# Glossary

+ __Explanatory variable:__ A variable used to understand or predict values of an outcome variable.
+ __Outcome variable:__ A variable which we are aiming to understand or predict via some explanatory variable(s).
+ __Scatterplot:__ A plot in which the values of two variables are plotted along the two axes, the pattern of the resulting points revealing any relationship which is present. 
+ __Covariance:__ A measure of the extent to which two variables vary together. 
<br><br>
+ `cov()` To calculate the covariance between two variables. 
+ `group_by()` To apply a grouping in a dataframe for each level of a given variable. Grouped dataframes will retain their grouping, so that if we use `summarise()` it will provide a summary calculation _for each group_. 
+ `geom_point()` To add points/dots to a ggplot.
+ `facet_wrap()` To split a ggplot into multiple plots (facets) for each level of a given variable. 
+ **Inline R code** _(introduced in the exercises below)_: R code which is written in-text, rather than inside a code-chunk. When a document with inline R code is rendered, the calculations are performed and resulting output is placed within the written text. 


---

# Exercises

`r qbegin(1)`  

1. Open a new Rmarkdown file, and give it an appropriate title for this set of exercises.  
1. In a new code-chunk, load the tidyverse package.  
1. Now create a new heading of "Stroop task data".  
    + **Hint:** remember that we specify headings and subheadings using #, ##, ### etc.
1. In a new code-chunk, read in the Stroop task data from https://uoepsy.github.io/data/strooptask.csv.  
1. Add a new variable to the data which is the "stroop-effect" for each observation (the difference in times taken to complete the matching and mismatchings sets of coloured words)  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
It should look something like the below:

![](images/relationships/initrmd.png) 
`r solend()`

---

`r qbegin(2)`  
Recall that above we visualised the relationship between practice (*categorical*) and Stroop-effect (*numeric*) by plotting two density curves overlayed on one another, or two histograms side-by-side.  
<br>
What other way might we visualise this relationship?  
**Hint:** 
```{r eval=FALSE}
ggplot(data = stroopdata, aes(x = practice, y = stroop_effect)) + 
geom_?????
```
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Take a moment to think - we have visualised numeric variables in a variety of ways: histograms, density curves, and boxplots.  
Both histograms and density curves already have values on the y-axis (histograms have "count" on the y, and density curves have "density").  
```{r}
ggplot(data = stroopdata, aes(x = practice, y = stroop_effect)) +
  geom_boxplot()
```
`r solend()`

---

`r qbegin(3)`  
Check what other variables there are in the data - use the function `names()`, and give it your dataframe:
```{r}
names(stroopdata)
```

We also have information recorded on the participants' heights (measured in cm).  
Produce a visualisation of the relationship between height and the Stroop-effect. Before you do so, think about what you expect to see?  
Calculate the covariance.  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We wouldn't really expect height to influence the Stroop-effect. We can look at the relationship in a scatterplot:
```{r}
ggplot(data = stroopdata, aes(x = height, y = stroop_effect)) +
  geom_point()
```

And the covariance is small (in comparison to the relationship we saw previous between the stroop effect and age):
```{r}
cov(stroopdata$height, stroopdata$stroop_effect)
```
`r solend()`

---

`r qbegin(4)`  
We're going to move to a different dataset now, using data from Edinburgh City Council, and from the Scottish Environment Protection Agency.  
<br>
Create a new heading in your Rmardkown for "Edinburgh Cyclists".  
<br>
The first dataset we are going to use is the from the Bicycle Counters on Middle Meadow Walk (see Figure \@ref(fig:mmwalk)).

```{r mmwalk, echo=FALSE, fig.cap="Photo from \\@SustransScot on Twitter, 2017"}
knitr::include_graphics("images/relationships/mmwalk.jpeg")
```

Read in the data https://uoepsy.github.io/data/cycling_mmwalk.csv and assign it a name of your choice.   

| Variable Name | Description            |
|---------------|--------------------|
| month   | Month (first three letters) |
| day    | Day of the month (1 - 31) |
| hour  | Hour of the day (0 - 24) |
| cyclists  | Number of cyclists recorded by the bike counters (in either direction) on Middle Meadow Walk, Edinburgh |
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We're going to assign it the name "mm_walk". 
```{r}
mm_walk <- read_csv("https://uoepsy.github.io/data/cycling_mmwalk.csv")
```
`r solend()`

---

`r qbegin(5)`  
The data contains information on the total number of cyclists travelling in either direction on Middle Meadow Walk for each hour of each day in 2012.  

Plot the relationship between hour of day and the number of cyclists.  
**Tip:** You will want to make the hour of day a __factor__ first, so that R treats it as different levels.   
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Currently, the _hour_ variable is numeric, and R won't know that we want a separate boxplot for each hour:
```{r}
ggplot(data = mm_walk, aes(x = hour, y = cyclists)) + 
  geom_boxplot()
```

What we need to do is make the _hour_ variable a factor:
```{r}
mm_walk$hour <- factor(mm_walk$hour)
```
OR
```{r}
mm_walk <- 
  mm_walk %>% 
  mutate(
    hour = factor(hour)
  )
```

and then plot it. Note that ggplot now knows to create a separate boxplot for each hour:
```{r}
ggplot(data = mm_walk, aes(x = hour, y = cyclists)) + 
  geom_boxplot()
```
`r solend()`

---

`r qbegin(6)`
Using `group_by()` and `summarise()`, you can _aggregate_ the data in to grouped averages (using `mean()`), or grouped totals (using `sum()`).    

1. Which month had the highest total number of cyclists?  
1. Which hour of the day had the highest average number of cyclists?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
To calculate the total number of cyclists in each month, we want to:
1. group by the month  
2. add together (*sum*) all the numbers of cyclists in that month 
```{r}
mm_walk %>%
  group_by(month) %>%
  summarise(
    monthly_cyclists = sum(cyclists)
  )
```
August had the most cyclists, with 55,165. August tends to be the busiest month in Edinburgh because of the festival.  

To calculate the average number of cyclists in each hour of the day, we want to:
1. group by the hour of the day
2. calculate the average of the numbers of cyclists for each hour  

**Tip:** we can use `arrange(somevariable)` order our data from smallest to largest according to "somevariable", and by adding in `desc()` too, we can go from largest to smallest!  
```{r}
mm_walk %>%
  group_by(hour) %>%
  summarise(
    avg_cyclists = mean(cyclists)
  ) %>%
  arrange(desc(avg_cyclists))
```
Matching the plot we made in the previous question, there are two peak hours - 8am and 5pm. 5pm has a slightly higher _mean_ number of cyclists, with 143. 
`r solend()`

---

`r qbegin(7)`  
Another dataset, available at https://uoepsy.github.io/data/cycling_invrow.csv, contains information on the total number of cyclists each month from 2014 to 2016 on Inverleith Row, Edinburgh. It also contains data on the monthly rainfall (in millimetres) measured at the nearby Royal Botanic Gardens.  

Read the data into R, and produce a visualisation of the relationship between monthly rainfall and monthly cyclists.   

| Variable Name | Description            |
|---------------|--------------------|
| year    | Year (2014 - 2016) |
| month   | Month (first three letters) |
| cyclists  | Number of cyclists recorded by the bike counters (in either direction) on Inverleith Row, Edinburgh |
| rainfall_mm  | Rainfall (in millimetres) recorded at the Royal Botanic Gardens, Edinburgh |  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
inv_row <- read_csv("https://uoepsy.github.io/data/cycling_invrow.csv")

ggplot(data = inv_row, aes(x = rainfall_mm, y = cyclists))+
  geom_point()
```
`r solend()`

---

`r qbegin("8 - Harder")`  
Make a new heading of "Pulse rate and exercise".   
Read in a new dataset from https://uoepsy.github.io/data/pulse.csv.  
  
|  Variable|  Description|
|--:|--:|
|  active | Pulse rate (beats per minute) after exercise |
|  rest |    Resting pulse rate (beats per minute) |
|  smoke   |  "Y" = Smoker or "N" = Nonsmoker  |
|  sex   |  "F" = Female or "M" = Male  |
| exercise  |  Typical hours of exercise (per week)  |
| hgt| Height (in inches) |
| wgt |	Weight (in pounds) |  
  

Explore _visually_ the relationship between active and resting pulse rate and the variables which influence resting pulse rate.   
For each plot you make:

+ Think about what variable we are assuming to influence another. Which should be on the x-axis, and which on the y?  
+ Think about what type of measurement each variable is - e.g., numeric, categorical. How is it best to visualise each relationship?  
+ Try to make your plots easily interpretable (for instance, add useful axes and titles using `labs(title = "---", x = "---", y = "---")`), and aesthetically pleasing (try out some of the different [themes](https://www.datanovia.com/en/blog/ggplot-themes-gallery/)). 
`r qend()`
 
 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)` 
Read in the data:
```{r}
pulse <- read_csv("https://uoepsy.github.io/data/pulse.csv")
```

Visualise the relationship between resting and active pulse rate. This should just be a scatterplot:  
```{r}
ggplot(data = pulse, aes(x = rest, y = active)) + 
  geom_point(col="red")+
  labs(title = "Relationship between resting and active pulse rate",x = "Resting pulse", y = "Pulse after exercise")+
  theme_minimal()
```

For a visual exploration, we now want to create similar plots between each variable and the _rest_ variable, to see which ones we see patterns in.  
```{r}
ggplot(data = pulse, aes(x = sex, y = rest)) + 
  geom_boxplot()

ggplot(data = pulse, aes(x = smoke, y = rest)) + 
  geom_boxplot()
```

When you look at the data, you'll notice that because the exercise per week is rounded to the nearest hour, there are actually only three different distinct values (1, 2, or 3). A scatterplot might have initially seemed the best choice (because we think of hours per week as _numeric_). However, a boxplot might be more convenient.   
Note that we have `x = factor(exercise)` in the code below. This temporarily makes the _exercise_ variable a factor (for the plot _only_, and making no permanent changes to the dataframe itself).
```{r}
ggplot(data = pulse, aes(x = factor(exercise), y = rest)) + 
  geom_boxplot()
```


```{r}
ggplot(data = pulse, aes(x = hgt, y = rest)) + 
  geom_point()

ggplot(data = pulse, aes(x = wgt, y = rest)) + 
  geom_point()
```
`r solend()`

---

`r qbegin(9)`  
Create a two-way contingency table to assess the relationship between Smoker/Non-Smoker and Male/Female.  
What percentage of Males are Smokers? 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
pulse %>%
  select(sex, smoke) %>%
  table()
```
```{r}
pulse %>%
  select(sex, smoke) %>%
  table() %>% 
  prop.table(margin=1)
```

14% of Males are smokers. 
`r solend()`

---

`r qbegin(10)`  
We're going to now learn about how to write a paragraph of text with embedded calculations. Each time we render the document, the calculations get performed and the ouput gets created. This is really useful as it means we can write a report which we can really easily update if we change our analysis, or include more data. 

Instead of writing in code-chunks, as we have been, we can also write __inline R code__, which we specify as shown in Figure \@ref(fig:inlinepng).  
```{r inlinepng, echo=FALSE, fig.cap="Writing Inline R code. The code on the left gets compiled to look like what is shown on the right (source: https://rmarkdown.rstudio.com/authoring_quick_tour.html)"}
knitr::include_graphics("images/relationships/inline.png")
```


Write the paragraph below, and fill in the blanks with inline R code.  
<br>
<center>
The average resting pulse was **????**, with a standard deviation of **????**. Heights of participants in the study ranged from **????** to **????**, with a median of **????** and an IQR of **????**.  
</center>
<br>
<small>
**Tip:** for writing inline code, often using code like `mean(data$variable)` is easier, because it is more compact than `data %>% summarise(mean = mean(variable)) %>% pull(mean)`.  
</small>
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Note that we have also used `round(2)` every now and then, to round the results to 2 decimal places. 
```{r echo=FALSE}
knitr::include_graphics("images/relationships/inline2.png")
```

`r solend()`

---

`r qbegin(11)`  
Final task for this set of exercises!  

Compile your Rmarkdown document into an html. Remember that you can do this using the "Knit" button at the top.  
<br>
You'll notice that as the **.html** file shows up in the files tab in the bottom-right of Rstudio.  
Make sure that your inline R code is displaying correctly.  
<br>
Upload your *.html* to the discussion board, so we can all admire each other's lovely ggplots!  
`r qend()`

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>