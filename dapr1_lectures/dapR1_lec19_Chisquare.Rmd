---
title: "<b>Chi-Square Tests</b>"
subtitle: "Data Analysis for Psychology in R 1"
author: "<b>Dr Emma Waterston</b>"
institute: "Department of Psychology<br/>The University of Edinburgh"
output:
  xaringan::moon_reader:
    self_contained: true
    css: 
      - un-xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(knitr)

options(htmltools.dir.version = FALSE)
#options(digits = 4, scipen = 2)
options(knitr.table.format = "html")

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE, message = FALSE,
  cache = FALSE,
  dev = "png",
  fig.align = 'center',
  fig.height = 5, fig.width = 6,
  out.width = "80%",
  dpi = 300
)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  outfile = "un-xaringan-themer.css"
)
```


```{r preamble, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(patchwork)
library(kableExtra)
library(moderndive)

theme_set(
    theme_classic(base_size = 15) +
    theme(plot.title = element_text(hjust = 0.5))
)
```

# Week's Learning Objectives

1. Understand the difference between $\chi^2$ goodness-of-fit and $\chi^2$ test of independence

2. Understand how to perform a $\chi^2$ goodness-of-fit and interpret results

3. Understand how to perform a $\chi^2$ test of independence and interpret results

---


class: inverse, center, middle

# Part 1
## Introduction to $\chi^2$

---

# Moving on from $t$-tests...

+ $t$-tests have allowed you to make comparisons using *continuous* data:

  + A continuous outcome variable from two separate groups (independent-samples $t$-test)
  + A continuous outcome variable from one group at two time points (paired-samples $t$-test)
  + One continuous variable against a single value (one-sample $t$-test)

--

+ You may instead want to test whether data are distributed across *categories* in the way that you would expect:

  + Is your sample distributed equally across levels of education?
  + Is smoking (Y/N) associated with cardiovascular disease (Y/N)? 

--

+ In this case, you will will need a test that checks whether data are grouped according to your expectations.

+ $\chi^2$-tests are used to compare **frequencies** across categories in your data


---
# $\chi^2$-tests vs $t$-tests

+ Similar to a $t$-test,
    1. Compute a test statistic
    2. Locate the test statistic on a distribution that reflects the probability of each test statistic value, given that $H_0$ is true.
    3. If the probability associated with your test statistic is small enough, your results are considered significant.

--

+ Like the $t$-distribution, the shape of the distribution depends on the degrees of freedom

+ Unlike the $t$-distribution, *df* in a $\chi^2$ test isn't computed using sample size, but the number of groups within your data

.pull-left[
.center[
** $t$ Distribution **
```{r, echo = F, fig.height = 2.5, fig.width = 4.5}
ggplot(data.frame(x = c(-3.5, 3.5)), aes(x = x)) +
  stat_function(fun = dt, args = list(df = 5), colour = '#002060', size = .8) +
  stat_function(fun = dt, args = list(df = 10), colour = '#BF1932', size = .8) +
  stat_function(fun = dt, args = list(df = 25), colour = '#88B04B', size = .8) +
  stat_function(fun = dt, args = list(df = 50), colour = '#FCBB06', size = .8) +
  labs(x='t', y = 'Probability') +
  annotate(geom = 'text', x = 2.5, y = 0.38, label = 'df = 50', colour = '#FCBB06', size = 4) +
  annotate(geom = 'text', x = 2.5, y = 0.34, label = 'df = 25', colour = '#88B04B', size = 4) +
  annotate(geom = 'text', x = 2.5, y = 0.30, label = 'df = 10', colour = '#BF1932', size = 4) +
  annotate(geom = 'text', x = 2.5, y = 0.26, label = 'df = 5', colour = '#002060', size = 4) +
  theme(axis.text = element_text(size = 12), axis.title = element_text(size = 14, face = 'bold'))
```
]
]

.pull-right[
.center[
** $\chi^2$ Distribution**
```{r, echo = F, fig.height = 2.5, fig.width = 4.5}
chiDist <- ggplot(data.frame(x = c(0, 15)), aes(x = x)) +
  stat_function(fun = dchisq, args = list(df = 2), colour = '#002060', size = 1) +
  stat_function(fun = dchisq, args = list(df = 4), colour = '#BF1932', size = 1) +
  stat_function(fun = dchisq, args = list(df = 6), colour = '#88B04B', size = 1) +
  stat_function(fun = dchisq, args = list(df = 8), colour = '#FCBB06', size = 1) +
  labs(x=expression(chi^{2}), y = 'Probability') +
  annotate(geom = 'text', x = 12, y = 0.45, label = 'df = 2', colour = '#002060', size = 4) +
  annotate(geom = 'text', x = 12, y = 0.4, label = 'df = 4', colour = '#BF1932', size = 4) +
  annotate(geom = 'text', x = 12, y = 0.35, label = 'df = 6', colour = '#88B04B', size = 4) +
  annotate(geom = 'text', x = 12, y = 0.3, label = 'df = 8', colour = '#FCBB06', size = 4) +
  theme(axis.text = element_text(size = 12), axis.title = element_text(size = 14, face = 'bold'))

chiDist
```

]
]

---
# $\chi^2$ distribution

.pull-left[
+ As the number of comparison groups increases, the distribution curve flattens

  + Larger $\chi^2$ values become more probable
  + A wider range of $\chi^2$ values become more likely

+ The $\chi^2$ distribution begins at 0
  
  + Categorical variables don't have direction
  + $p$-value is computed only in one direction (right-tail) as the Probability of observing a $\chi^2$ statistic as big or bigger than the one obtained
  + We can investigate this further by looking at the $\chi^2$ formula
]

.pull-right[
.center[
** $\chi^2$ Distribution**
```{r, echo = F, fig.height = 4, fig.width = 5}
chiDist
```

]
]

---

# Data Requirements & Assumptions of $\chi^2$ tests

- Data Requirements  
  + Variables should be measured at an ordinal or nominal level (i.e., categorical data).

    
- Assumptions
  + Expected counts $\geq$ 5  
  + Observations are independent   
    + Each observation appears only in a single cell  
---

# Types of $\chi^2$ tests

+ Goodness of Fit

+ Test of Independence

---

class: center, middle
# **Questions?**

---

class: inverse, center, middle

# Part 2 
## $\chi^2$ Goodness of Fit test

---

# $\chi^2$ Goodness of Fit test

.pull-left[
+ Tests whether the proportions / relative frequencies you actually have are consistent with the expected proportions / relative frequencies

+ Looks at the distribution of data across a single category

+ **Hypotheses:**

  + $H_0: p_1 = p_{1,0},\ p_2 = p_{2,0},\ ...,\ p_k = p_{k,0}$
  
  + $H_1:$ Some $p_i \neq p_{i,0}$
]

---
count: false

# $\chi^2$ Goodness of Fit test
.pull-left[
+ Tests whether the proportions / relative frequencies you actually have are consistent with the expected proportions / relative frequencies

+ Looks at the distribution of data across a single category

+ **Hypotheses:**

  + $H_0: p_1 = p_{1,0},\ p_2 = p_{2,0},\ ...,\ p_k = p_{k,0}$
  
  + $H_1:$ Some $p_i \neq p_{i,0}$

]

.pull-right[
.pull-left.center[**Expected Values ** 

```{r, echo = F, out.width='80%'}
knitr::include_graphics('figures/chiSqGoF_Exp.png')
```
]

.pull-right.center[ **Observed Values **

```{r, echo = F, out.width = '80%'}
knitr::include_graphics('figures/chiSqGoF_Obs.png')
```
]
]


---

# $\chi^2$ Goodness of Fit test

.center.f3[ 
$\chi^2 = \sum\limits_{i=1}^k \frac{(O_i - E_i)^2}{E_i}$
]

- where: 

  + $\Sigma$ = sum up
    + $\sum\limits_{i=1}^k$ : Sum all values from levels 1 through k
  + $E$ = Expected Cases
      + The values that you expect, given $H_0$ is true
  + $O$ = Observed Cases
     + The values you actually have
  + $i$ : Current level

---

# Example

+ A new flower shop is trying to decide which days of the week they will be open 

+ They want to know whether order number is consistent across days of the week

+ They count the total number of orders they take each day of the week over the course of a month


---

# Data

```{r, echo = FALSE}
flowerDat <- tibble(Day=as.factor(c('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday')),
                    Orders = c(54, 39, 44, 47, 68, 72, 53))

flowerDat
```

---

# Hypotheses

.pull-left[

+ I elect to use an alpha $(\alpha)$ of .05

+ My hypotheses are: 
  + $H_0$: Orders will be consistent throughout the week 
      + $p_{Monday}=p_{Tuesday}=\cdots =\ p_{Sunday}$
  
  + $H_1$: Orders will differ across the week
      + Some $p_{i}\not=p_{i0}$
  
]


.pull-right[
<br>
```{r, echo = F, results='asis'}
kable(flowerDat)
```
]

---

# Visualisation

```{r, echo = TRUE, eval=FALSE}
ggplot(data = flowerDat, aes(Day, Orders, fill = Day)) +
  geom_col()
```


```{r, fig.height=2.8}
flowerDat$Day <- factor(flowerDat$Day,
levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
ggplot(data = flowerDat, aes(Day, Orders, fill = Day)) +
  geom_col() + 
  theme(axis.text=element_text(size=7))
```

---

# Performing a $\chi^2$ Goodness of Fit test

**Compute the test statistic**

.center.f3[
$\chi^2 = \sum\limits_{i=1}^k \frac{(O_i-\color{#BF1932}{E_i})^2}{\color{#BF1932}{E_i}}$
]

+ $E_i=n\cdot\ p_i$

+ where:
    + $n$ = sample size
    + $p$ = the hypothesized population proportion for the category under the null hypothesis
     
+ In this example, we expect each level to be approximately equal, so the expected proportion will be the same across levels:

```{r, echo = TRUE}
n <- sum(flowerDat$Orders)
p <- (1/length(levels(flowerDat$Day))) # i.e., 1/7

E <- n * p
round(E, digits = 2)
```

  
```{r}
exVal <- sum(flowerDat$Orders)*(1/length(levels(flowerDat$Day)))
```

---

# Visualisation

```{r, fig.height=2.8}
flowerDat$Day <- factor(flowerDat$Day,
levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
ggplot(data = flowerDat, aes(Day, Orders, fill = Day)) +
  geom_col() + 
  geom_hline(yintercept = 53.86, colour = "#0F4C81", lwd = 1) + 
  theme(axis.text=element_text(size=7))
```

---

# Performing a $\chi^2$ Goodness of Fit test

**Compute the test statistic**

.center.f3[
$\chi^2 = \sum\limits_{i=1}^k \frac{(O_i-\color{#BF1932}{E_i})^2}{\color{#BF1932}{E_i}}$
]

```{r, echo = F}
flowerDat$Expected <- exVal

kable(flowerDat, digits = 2)
```


---

# Performing a $\chi^2$ Goodness of Fit test

**Compute the test statistic**

.center.f3[
$\chi^2 = \sum\limits_{i=1}^k \frac{(\color{#BF1932}{O_i - E_i})^2}{E_i}$
]

```{r, echo = F}
flowerDat$Difference <- flowerDat$Orders-flowerDat$Expected

kable(flowerDat, digits = 2)
```

---

# Performing a $\chi^2$ Goodness of Fit test

**Compute the test statistic**

.center.f3[
$\chi^2 = \sum\limits_{i=1}^k \frac{(O_i - E_i)\color{#BF1932}{^2}}{E_i}$
]

```{r, echo = F}
flowerDat$Squared <- flowerDat$Difference^2

kable(flowerDat, digits = 2)
```

---

# Performing a $\chi^2$ Goodness of Fit test

**Compute the test statistic**

.center.f3[
$\chi^2 = \sum\limits_{i=1}^k \color{#BF1932}{\frac{(O_i - E_i)^2}{E_i}}$
]

```{r, echo = F}
flowerDat$SqbyExp <- flowerDat$Squared/flowerDat$Expected
x2_stat_gof <- sum(flowerDat$SqbyExp)
kable(flowerDat, digits = 2)
```

---

# Performing a $\chi^2$ Goodness of Fit test

**Compute the test statistic**

.center.f3[
$\chi^2 = \color{#BF1932}{\sum\limits_{i=1}^k} \frac{(O_i - E_i)^2}{E_i}=$ `r round(sum(flowerDat$SqbyExp), 2)`
]

```{r, echo = F}
kable(flowerDat, digits = 2) %>%
  column_spec(6, color = '#BF1932')
```

---

# Performing a $\chi^2$ Goodness of Fit test

**Find the test statistic on the distribution**

.pull-left[
$\chi^2 = `r round(sum(flowerDat$SqbyExp), 2)`$  

$df=k-1$  

+ where $k$ = number of levels within categorical variable

+ so, in our example: 
  + $k$ = number of days in the week
  + $df ~=~ (7-1)~=~ 6$
]


.pull-right[

```{r, echo = F, fig.height=4.5}
GOFplot <- ggplot(data.frame(x = c(0, 20)), aes(x = x)) +
  stat_function(fun = dchisq, args = list(df = 6), colour = "#0F4C81", size = 1) +
  labs(x=expression(chi^{2}), y = 'Probability', title = 'df = 6') +
  theme(axis.text = element_text(size = 12), axis.title = element_text(size = 14, face = 'bold'))

GOFplot  + geom_vline(xintercept = sum(flowerDat$SqbyExp), linetype = 'dashed', colour = "#0F4C81")
```
]

---

# Performing a $\chi^2$ Goodness of Fit test

**Compute the probability of obtaining a $\chi^2$ statistic at least as extreme as the observed one, if $H_0$ is true**

.pull-left[
+ What proportion of the plot falls in the shaded area?
]

.pull-right[
```{r, echo = F, fig.height=4.5}
testDat <- seq(sum(flowerDat$SqbyExp), 20, by = .1)
dTest <- dchisq(testDat, df = 6)
testDat <- data.frame(vals=testDat, prob=dTest)

GOFplot + geom_area(data = testDat, aes(x=vals,y=prob), fill = "#0F4C81") +
  geom_vline(xintercept=sum(flowerDat$SqbyExp), linetype = 'dashed', color = "#0F4C81")

```
]

---
# Performing a $\chi^2$ Goodness of Fit test

**Compute the probability of obtaining a $\chi^2$ statistic at least as extreme as the observed one, if $H_0$ is true**

.pull-left[
+ What proportion of the plot falls in the shaded area?

```{r, echo = TRUE}
pchisq(x2_stat_gof, 
       df = 6, 
       lower.tail = FALSE)
```
+ The probability that we would have a $\chi^2$ value as extreme as `r round(sum(flowerDat$SqbyExp), 2)` if $H_0$ is true is only `r round(pchisq(sum(flowerDat$SqbyExp),df = 6,lower.tail = F), 3)`

]



.pull-right[
```{r, echo = F, fig.height=4.5}
GOFplot + geom_area(data = testDat, aes(x=vals,y=prob), fill = "#0F4C81") +
  geom_vline(xintercept=sum(flowerDat$SqbyExp), linetype = 'dashed', color = "#0F4C81")
```
]

---
# Performing a $\chi^2$ Goodness of Fit test in R

.pull-left[

```{r, echo = TRUE}
#Option 1
observed <- c(54, 39, 44, 47, 68, 72, 53)
expected <- c(1/7,1/7,1/7,1/7,1/7,1/7,1/7)
GOFtest <- chisq.test(x = observed, p = expected)
GOFtest
```

- where:
  + x: A numerical vector of observed frequencies
  + p: A numerical vector of expected proportions

] 

.pull-right[

```{r, echo = TRUE, eval = TRUE}
#Option 2
GOFtest <- chisq.test(flowerDat$Orders)
GOFtest
```

]

---

# Exploring our Results Further

+ If our results are significant, we are likely interested in knowing which levels within our category had the biggest differences.

+ We can get this information by looking at the Pearson residuals (AKA, standardized residuals)

+ $\frac{O_i-E_i}{\sqrt{E_i}}$  
  
--
  
**In R**

```{r echo = TRUE}
GOFtest$residuals
```

**By Hand**

+ In this case, you will calculate them separately for each level 
  
+ *Example of number of flowers sold on a Monday:* $\frac{54-53.86}{\sqrt{53.86}} = 0.019$

---

# Exploring our Results Further

.pull-left[
+ Positive residuals indicate that the observed frequency of the corresponding level is higher than the expected frequency  

+ Negative residuals indicate that the observed frequency of the corresponding level is lower than the expected frequency  

+ More extreme residuals indicate that the values are contributing more strongly to the results

  + Values $\leq$ -2 indicate the observed frequency of that level is **much lower** than expected
  
  + Values $\geq$ 2 indicate the observed frequency of that level is **much higher** than expected
]

.pull-right[
<br>
```{r, echo = F}
GOFtest <- chisq.test(flowerDat$Orders)
flowerDat$Residuals <- GOFtest$residuals
kable(flowerDat[,c('Day', 'Orders', 'Residuals')], digits = 2)
```
]
---

# Drawing Conclusions

.pull-left[
**If you owned the flower shop, which two days would you choose to close each week?**
]

.pull-right[
<br>
```{r, echo = F}
kable(flowerDat[,c('Day', 'Orders', 'Residuals')], digits = 2)
```
]

---

# Write Up

A $\chi^2$ Goodness of Fit test was conducted in order to determine whether the proportion of flower orders was equal across each day of the week. The goodness of fit test was significant $(\chi^2(6, n = 7) = 16.62, p = .011)$, and thus, with $\alpha = .05$, we would reject the null hypothesis as the proportion of flower orders differed across the days of the week. 

---

class: center, middle
# **Questions?**

---

class: inverse, center, middle

# Part 3 
## $\chi^2$ Test of Independence

---

# $\chi^2$ Test of Independence

.pull-left[

+ Checks whether two categorical variables from a single population are independent of each other

+ Specifically, tests whether membership in Variable 1 is dependent upon membership in Variable 2

+ **Hypotheses:**

  + $H_0:$ Variables A and B are independent 
  
  + $H_1:$ There is an association between Variable A and Variable B

]

  
---
count: false

# $\chi^2$ Test of Independence

.pull-left[

+ Checks whether two categorical variables from a single population are independent of each other

+ Specifically, tests whether membership in Variable 1 is dependent upon membership in Variable 2

+ **Hypotheses:**

  + $H_0:$ Variables A and B are independent 
  
  + $H_1:$ There is an association between Variable A and Variable B
  
]

.pull-right.center[

**Expected Values **

```{r, echo = F, out.width='90%'}
knitr::include_graphics('figures/chiSqToI_Exp.png')
```

**Observed Values **

```{r, echo = F, out.width = '90%'}
knitr::include_graphics('figures/chiSqToI_Obs.png')
```
]


---

# $\chi^2$ Test of Independence

.center.f3[ 
$\chi^2 = \sum\limits_{i=1}^r \sum\limits_{j=1}^c \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$
]

- where: 
  + $\Sigma$ = sum up
  + $E$ = Expected Cases
  + $O$ = Observed Cases
  + $i$ : current level within Variable A
  + $j$ : current level within Variable B
  + $r$ : total levels within Variable A
  + $c$ : total levels within Variable B

---

# Example


+ The flower shop is trying to decide on their flower stock

+ They want to know whether the flower type that sells the best depends on the season

+ **Hypotheses:**

  + $H_0$: Flower orders will be independent of season
  
  + $H_1$: Flower orders will be dependent on season


---

# Data

```{r, echo = F, results='asis'}

seasonDat <- tibble(Season=as.factor(rep(c('Spring', 'Summer', 'Autumn', 'Winter'), times=c(603, 592, 551, 602))), 
                    Flowers=as.factor(c(rep('Roses', 232), rep('Tulips', 185), rep('Lilies', 186), rep('Roses', 228),
                              rep('Tulips', 192), rep('Lilies', 172), rep('Roses', 219), rep('Tulips', 164),
                              rep('Lilies', 168), rep('Roses', 246), rep('Tulips', 173), rep('Lilies', 183))))

seasonDat$Season <- factor(seasonDat$Season, levels = c('Spring', 'Summer', 'Autumn', 'Winter'))
flowerCols <- c('Lilies', 'Roses', 'Tulips')
ObsVals <- table(seasonDat$Season, seasonDat$Flowers)
#kable(ObsVals)


```

.pull-left[

First 6 rows:

```{r, echo = FALSE}
head(seasonDat)
```

]

.pull-right[

Create a contingency table:
```{r, echo = TRUE, eval = FALSE}
#Option 1
xtabs(~ Season + Flowers, data = seasonDat)

#Option 2 
table(seasonDat$Season, seasonDat$Flowers)
```

```{r echo = FALSE, eval = TRUE}
ObsVals <- table(seasonDat$Season, seasonDat$Flowers)
ObsVals
```

]
---

# Visualisation

```{r, fig.height=2.8, echo = TRUE}
library(ggmosaic)
ggplot(data = seasonDat) +
  geom_mosaic(aes(x = product(Flowers, Season), fill = Season))
```

---

# Performing a $\chi^2$ Test of Independence

**Compute the test statistic**

.center.f3[ 
$\chi^2 = \sum\limits_{i=1}^r \sum\limits_{j=1}^c \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$
]

+ $E_{ij}=\frac{R_i\ \cdot\ C_j}{n}$


+ In this example, we expect the orders to be distributed evenly across season and flower type

---

# Performing a $\chi^2$ Test of Independence

.pull-left[

**Compute the test statistic**

<br>

+ $E_{ij}=\frac{R_i\ \cdot\ C_j}{n}$

<br>

```{r, echo = F}

kable(addmargins(ObsVals)) %>%
  column_spec(5, color = '#BF1932') %>%
  row_spec(5, color = '#BF1932')

```

]

.pull-right.center[ 


| Season |       Lilies     |      Roses       |      Tulips      |
|--------|------------------|------------------|------------------|
| Spring |$$\frac{(`r length(which(seasonDat$Season=='Spring'))` * `r length(which(seasonDat$Flowers=='Lilies'))`)}{`r nrow(seasonDat)`}$$ | $$\frac{(`r length(which(seasonDat$Season=='Spring'))` * `r length(which(seasonDat$Flowers=='Roses'))`)}{`r nrow(seasonDat)`}$$ | $$\frac{(`r length(which(seasonDat$Season=='Spring'))` * `r length(which(seasonDat$Flowers=='Tulips'))`)}{`r nrow(seasonDat)`}$$| 
| Summer |$$\frac{(`r length(which(seasonDat$Season=='Summer'))` * `r length(which(seasonDat$Flowers=='Lilies'))`)}{`r nrow(seasonDat)`}$$|$$\frac{(`r length(which(seasonDat$Season=='Summer'))` * `r length(which(seasonDat$Flowers=='Roses'))`)}{`r nrow(seasonDat)`}$$|$$\frac{(`r length(which(seasonDat$Season=='Summer'))` * `r length(which(seasonDat$Flowers=='Tulips'))`)}{`r nrow(seasonDat)`}$$| 
| Autumn |$$\frac{(`r length(which(seasonDat$Season=='Autumn'))` * `r length(which(seasonDat$Flowers=='Lilies'))`)}{`r nrow(seasonDat)`}$$|$$\frac{(`r length(which(seasonDat$Season=='Autumn'))` * `r length(which(seasonDat$Flowers=='Roses'))`) }{`r nrow(seasonDat)`}$$|$$\frac{(`r length(which(seasonDat$Season=='Autumn'))` * `r length(which(seasonDat$Flowers=='Tulips'))`)}{`r nrow(seasonDat)`}$$|
| Winter |$$\frac{(`r length(which(seasonDat$Season=='Winter'))` * `r length(which(seasonDat$Flowers=='Lilies'))`)}{`r nrow(seasonDat)`}$$|$$\frac{(`r length(which(seasonDat$Season=='Winter'))` * `r length(which(seasonDat$Flowers=='Roses'))`)}{`r nrow(seasonDat)`}$$|$$\frac{(`r length(which(seasonDat$Season=='Winter'))` * `r length(which(seasonDat$Flowers=='Tulips'))`)}{`r nrow(seasonDat)`}$$|


]


---

# Performing a $\chi^2$ Test of Independence

**Compute the test statistic**

.center.f3[
$\chi^2 = \sum\limits_{i=1}^r \sum\limits_{j=1}^c \frac{(O_{ij} - \color{#BF1932}{E_{ij}})^2}{\color{#BF1932}{E_{ij}}}$
]

.pull-left[
**Observed Counts**
```{r, echo = F}
kable(ObsVals[1:4,])
```
]

.pull-right[ 
**Expected Counts**
```{r, echo = F}
exVals <- tibble(Seasons=as.factor(c('Spring', 'Summer', 'Autumn', 'Winter')),
                 Lilies=c(length(which(seasonDat$Season=='Spring'))*length(which(seasonDat$Flowers=='Lilies'))/nrow(seasonDat),
                          length(which(seasonDat$Season=='Summer'))*length(which(seasonDat$Flowers=='Lilies'))/nrow(seasonDat),
                          length(which(seasonDat$Season=='Autumn'))*length(which(seasonDat$Flowers=='Lilies'))/nrow(seasonDat),
                          length(which(seasonDat$Season=='Winter'))*length(which(seasonDat$Flowers=='Lilies'))/nrow(seasonDat)),
                 Roses=c(length(which(seasonDat$Season=='Spring'))*length(which(seasonDat$Flowers=='Roses'))/nrow(seasonDat),
                          length(which(seasonDat$Season=='Summer'))*length(which(seasonDat$Flowers=='Roses'))/nrow(seasonDat),
                          length(which(seasonDat$Season=='Autumn'))*length(which(seasonDat$Flowers=='Roses'))/nrow(seasonDat),
                          length(which(seasonDat$Season=='Winter'))*length(which(seasonDat$Flowers=='Roses'))/nrow(seasonDat)),
                 Tulips=c(length(which(seasonDat$Season=='Spring'))*length(which(seasonDat$Flowers=='Tulips'))/nrow(seasonDat),
                          length(which(seasonDat$Season=='Summer'))*length(which(seasonDat$Flowers=='Tulips'))/nrow(seasonDat),
                          length(which(seasonDat$Season=='Autumn'))*length(which(seasonDat$Flowers=='Tulips'))/nrow(seasonDat),
                          length(which(seasonDat$Season=='Winter'))*length(which(seasonDat$Flowers=='Tulips'))/nrow(seasonDat)))

kable(exVals[1:4,], digits = 2)
```
]

---

# Performing a $\chi^2$ Test of Independence

**Compute the test statistic**

.center.f3[
$\chi^2 = \sum\limits_{i=1}^r \sum\limits_{j=1}^c \frac{\color{#BF1932}{(O_{ij} - E_{ij})}^2}{E_{ij}}$
]

.pull-left[
**Observed Counts**
```{r, echo = F}
kable(ObsVals[1:2,])
```
]

.pull-right[
**Expected Counts**
```{r, echo = F}
kable(exVals[1:2,], digits = 2)
```
]

.center[ 
**Difference**
```{r, echo = F}
DiffTab <- tibble(Seasons=as.factor(c('Spring', 'Summer', 'Autumn', 'Winter')),
                  Lilies=ObsVals[,'Lilies']-exVals$Lilies,
                  Roses=ObsVals[,'Roses']-exVals$Roses,
                  Tulips=ObsVals[,'Tulips']-exVals$Tulips)
kable(DiffTab[1:2,], digits = 2)
```
]
---

# Performing a $\chi^2$ Test of Independence

**Compute the test statistic**

.center.f3[
$\chi^2 = \sum\limits_{i=1}^r \sum\limits_{j=1}^c \frac{{(O_{ij} - E_{ij})\color{#BF1932}{^2}}}{E_{ij}}$
]

.pull-left.center[
**Difference**
```{r, echo = F}
kable(DiffTab, digits = 2)
```
]

.pull-right.center[ 
**Squared**
```{r, echo = F}
sqTab <- DiffTab
sqTab[,flowerCols] <- sqTab[,flowerCols]^2
kable(sqTab, digits = 2)
```
]

---

# Performing a $\chi^2$ Test of Independence

**Compute the test statistic**

.center.f3[
$\chi^2 = \sum\limits_{i=1}^r \sum\limits_{j=1}^c \color{#BF1932}{\frac{{(O_{ij} - E_{ij})^2}}{E_{ij}}}$
]

.pull-left.center[
**Squared**
```{r, echo = F}
kable(sqTab[1:2,], digits = 2)
```
]

.pull-right.center[ 
**Expected**
```{r, echo = F}
kable(exVals[1:2,], digits = 2)
```
]

.center[ 
**Squared over Expected**
```{r, echo = F}
divTab <- sqTab
divTab[,flowerCols] <- divTab[,flowerCols]/exVals[,flowerCols]
kable(divTab[1:2,], digits = 2)
```
]

---
# Performing a $\chi^2$ Test of Independence

**Compute the test statistic**

.center.f3[
$\chi^2 = \color{#BF1932}{\sum\limits_{i=1}^r \sum\limits_{j=1}^c}\frac{{(O_{ij} - E_{ij})^2}}{E_{ij}}$
]

.pull-left.center[
**Squared over Expected**
```{r}
kable(divTab, digits = 2)
```
]

.pull-right.center[ 
**Sum of Squared over Expected - $\chi^2$**

0.08 + 0.26 + 0.02 + ... + 0.08 + 0.55 = 
```{r}
sum(divTab[flowerCols])

x2_stat_toi <- sum(divTab[flowerCols])
```


]

---

# Performing a $\chi^2$ Test of Independence

**Find the test statistic on the distribution**

.pull-left[
$df=(r-1)(c-1)$

- where: 
  + $c$ = number of levels within Variable 1
  + $r$ = number of levels within Variable 2

- so, in our example: 
  + $c$ = number of levels within Season
  + $r$ = number of levels within Flowers
  + $df ~=~ (4-1)(3-1) ~=~ (3)(2) ~=~ 6$
]

.pull-right[

```{r, echo = F, fig.height=4.5}
TOIplot <- ggplot(data.frame(x = c(0, 30)), aes(x = x)) +
  stat_function(fun = dchisq, args = list(df = 6), colour = "#0F4C81", size = 1) +
  labs(x=expression(chi^{2}), y = 'Probability') +
  annotate(geom = 'text', x = 12, y = 0.1, label = 'df = 6', colour = "#0F4C81", size = 8) +
  theme(axis.text = element_text(size = 12), axis.title = element_text(size = 14, face = 'bold'))

TOIplot + geom_vline(xintercept = sum(sum(divTab[flowerCols])), linetype = 'dashed', colour = "#0F4C81")
```
]

---

# Performing a $\chi^2$ Test of Independence

**Compute the probability of obtaining a $\chi^2$ statistic at least as extreme as the observed one, if $H_0$ is true**

.pull-left[
+ What proportion of the plot falls in the shaded area?
]

.pull-right[
```{r, echo = F, fig.height=4.5}
testDat <- seq(sum(divTab[flowerCols]), 30, by = .1)
dTest <- dchisq(testDat, df = 6)
testDat <- data.frame(vals=testDat, prob=dTest)

TOIplot + geom_area(data = testDat, aes(x=vals,y=prob), fill = "#0F4C81") +
  geom_vline(xintercept=sum(sum(divTab[flowerCols])), linetype = 'dashed', color = "#0F4C81")

```
]

---
# Performing a $\chi^2$ Test of Independence

**Compute the probability of obtaining a $\chi^2$ statistic at least as extreme as the observed one, if $H_0$ is true**

.pull-left[
+ What proportion of the plot falls in the shaded area?

```{r, echo = TRUE}
pchisq(x2_stat_toi, 
       df = 6, 
       lower.tail = F)
```

+ The probability that we would have a $\chi^2$ value as extreme as `r round(sum(divTab[flowerCols]), 2)` if $H_0$ was true is `r round(pchisq(sum(divTab[flowerCols]),df = 6,lower.tail = F), 3)`.

]



.pull-right[
```{r, echo = F, fig.height=4.5}
TOIplot + geom_area(data = testDat, aes(x=vals,y=prob), fill = "#0F4C81") +
  geom_vline(xintercept=sum(divTab[flowerCols]), linetype = 'dashed', color = "#0F4C81")
```
]

---

# Performing a $\chi^2$ Test of Independence in R

```{r, echo = TRUE}
TOItest <- chisq.test(seasonDat$Season, seasonDat$Flowers)
TOItest
```

---

# Exploring our Results Further

+ We can also compute standardized residuals for the Test of Independence


+ $\frac{O_{ij}-E_{ij}}{\sqrt{E_{ij}}}$

--


.pull-left[

**In R**
```{r, echo = TRUE, out.width="80%"}
TOItest$residuals
```

]

.pull-right[

**By Hand**

+ In this case, you will calculate them separately by cell

+ *Example of number of Lilies sold in Spring*
    + $\frac{O_{ij}-E_{ij}}{\sqrt{E_{ij}}}$ = $\frac{186-182.08}{\sqrt{182.08}}$ = $0.291$

]



---

# Write Up

A $\chi^2$ test of independence was performed to examine whether the type of flower sold was independent of season. There was no significant association between these variables $(\chi^2(6, n = 2348) =  2.40, p = .880)$. Therefore, using an $\alpha = .05$, we failed to reject the null hypothesis.

---
class: center, middle
# **Questions?**

---

class: inverse, center, middle

# Part 4
## Effect Size

---


# Effect Sizes

+ There are 3 possibilities:

  + Phi coefficient $(\phi)$
  + Cramer's V $(V)$
  + Odds Ratios $(OR)$

+ You will learn more about odds ratios in DAPR2, so we will focus on Phi and Cramer's V

---

# Phi Coefficient

.center.f3[
$\phi=\sqrt{\frac{\chi^2}{n}}$
]

+ where
      - $n$ = total number of observations

+ Should only be used when you have a 2x2 contingency table (2 categorical variables with 2 levels each)

.pull-left[

+ Perhaps the most common 'cut-offs' for $\phi$-scores:

]

.pull-right[



| Verbal label         | Magnitude of $\phi$ |
|:--------------------:|:-------------------:|
| Small effect         | 0.1                 |
| Medium effect        | 0.3                 |
| Large effect         | 0.5                 |

]

---
# Phi Coefficient in R

```{r, echo = TRUE, eval = FALSE}
library(effectsize)
phi(TOItest)
```

---
# Cramer's V

.center.f3[
$V=\sqrt{\frac{\chi^2}{n\cdot\ df^*}}$
]

+ where
      - $n$ = total number of observations
      - $df^*$ = $min(r-1, c-1)$
  
+ Can be used when you aren't working with a 2x2 contingency table

.pull-left[

+ Cramer's V is interpreted based on $df^*$  
  
+ Cramer's V must lie between 0 and 1 (0 = complete independence, 1 = complete dependence)  

]

.pull-right[

| $df^*$ | small | medium | large |
|--------|-------|--------|-------|
|   1    |  .10  |  .30   |  .50  |
|   2    |  .07  |  .21   |  .35  |
|   3    |  .06  |  .17   |  .29  |
|   4    |  .05  |  .15   |  .25  |
|   5    |  .04  |  .13   |  .22  |

]

---
# Cramer's V

.pull-left[

**By hand**

$V=\sqrt{\frac{\chi^2}{n\cdot\ df^*}}$ = $\sqrt{\frac{2.40}{2348\cdot\ 6}}$ = $0.013$

]

.pull-right[

**In R**

```{r}
cont_table <- table(seasonDat$Season, seasonDat$Flowers)
```

```{r, echo = TRUE}
library(effectsize)
cramers_v(cont_table)
```

]
---

# Summary

+ Today we have covered:  
  + The $\chi^2$ distribution and how it compares to the $t$ distribution  
  + The assumptions of $\chi^2$ tests  
  + How the $\chi^2$ Goodness of Fit test and the $\chi^2$ Test of Independence are different  
  + How to calculate both types of $\chi^2$ values  
  + Standardized residuals and how they relate to your $\chi^2$ results  
  + Measures of effect size you may use with $\chi^2$ tests  
  
---
# Tasks 

+ Go to your lab and work on the assessed report  
+ Complete any assigned readings  
+ Go to office hours if you have questions  
  + Emma's Office Hours = Tuesdays 10:30-11:30 in G15, 7 George Square  
+ Complete the weekly quiz  
  + Opens Monday at 9am  
  + Closes Sunday at 5pm  
