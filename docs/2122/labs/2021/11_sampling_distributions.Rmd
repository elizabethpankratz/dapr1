---
title: "Sampling distributions"
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
```

```{r include=FALSE}
set.seed(3)

knitr::opts_chunk$set(out.width = '70%',
                      fig.align = 'center')
```


:::lo
**LEARNING OBJECTIVES**

1. Recognise the difference between parameters and statistics
1. Be able to use a sample statistic to estimate an unknown parameter
1. Understand what a sampling distribution is
1. Understand the concept of standard error
1. Recognise why sample size matters
:::



# Parameters and statistics

We can think of random or unpredictable data as arising in two ways. The first one involves _sampling from a finite population_ and measuring characteristics of the individuals chosen in the sample. An example of this are surveys and polls.
The second approach involves a _random process_ producing observations. An example of this is a production line for which we measure some characteristic of each produced item. Here, the underlying population is conceptual rather than real, and is the one that would be produced if the process was repeated a large number of times.

Both ways lead to a random observation possessing a distribution describing how the observation will vary.
Numerical summaries of that distribution are called _parameters_. Examples are the mean $\mu = E(X)$ of the distribution, the standard deviation $\sigma = SD(X)$, or a population proportion $p$.

If we are sampling the population of Scotland, we might be interested in $\mu$, the mean self-reported happiness level, or $p$, the proportion of vaccinated people.

:::yellow
__Parameters__

A _**parameter**_ is a numerical summary of a _**population**_ or _**distribution**_, for example the average income in the whole population.
:::

In practice, we know very little about the population we are sampling from (or the random process generating our data) and we collect data to find out more about these populations.
Therefore the parameters of interest are _unknown_ quantities that we want to estimate.

Consider again the population proportion of vaccinated people, $p$. If this is the quantity we are interested in, the obvious approach would be to take a sample from that population and use the proportion vaccinated in the sample, $\hat{p}$, as an _estimate_ of $p$.

To estimate the fault proportion $p$ in a light bulb production line, we can take some of the light bulb produced (i.e. a sample) and use the proportion of faulty light bulbs in the sample $\hat p$ as an estimate of the underlying proportion of faulty light bulbs $p$ for the production process.


:::yellow
__Statistic__

A _**statistic**_ is a numerical summary of the _**sample**_.

A (sample) statistic is often used to estimate a (population) parameter.
:::


From the above discussion, you can see that the population parameter and the sample statistic generally have the same name.
However, these are often written with different symbols to convey with just one letter:

1. what *summary* they represent;
2. if it is a *population* quantity or a quantity *computed on a sample*.

The following table summarizes standard notation for some population parameters, typically unknown, and the corresponding estimates computed on a sample.

|Numerical summary  | Population parameter   | Sample statistic         |
|:------------------|:----------------------:|:------------------------:|
|Mean               | $\mu$                  | $\bar{x}$ or $\hat{\mu}$ |
|Standard deviation | $\sigma$               | $s$ or $\hat{\sigma}$    |
|Proportion         | $p$                    | $\hat{p}$                |

Table: Notation for common parameters and statistics.


The Greek letter $\mu$ (mu) represents the population mean (parameter), while $\bar{x}$ (x-bar) or $\hat{\mu}$ (mu-hat) is the mean computed from the sample data (sample statistic).

The Greek letter $\sigma$ (sigma) represents the population standard deviation (parameter), while $s$ or $\hat{\sigma}$ (sigma-hat) is the standard deviation computed from the sample data (sample statistic).

The Getter $p$ represents the population proportion (parameter), while $\hat{p}$ (p-hat) is the proportion computed from the sample data (sample statistic).


<br>

The process of sampling $n$ people from the population is a _random process_. An _outcome_ of this random process is _a sample_ of size $n$. For each sample, we can calculate a statistic (e.g., the mean $\bar x$). 
Hence, a statistic is a numerical summary of a random experiment and for this reason it is a random variable, e.g. the mean denoted $\bar X$.

Throughout the exercises we will use the following notation:

- Uppercase letters refer to random variables

- Lowercase letters refer to observed values

<br>

```{r echo=FALSE, out.width = '95%'}
knitr::include_graphics('images/prob/sampling_cloud.png')
```

<br>

We use uppercase letters when we want to study the effects of sampling variation on a statistic, while we use lowercase letters for observed values.

:::frame
__TERMINOLOGY__

Statisticians often refer to the observed number in the sample as the _**estimate**_ ($\bar x$). 
This is just another way of saying a statistic which used to estimate a population parameter.

They also sometimes call _**estimator**_ the random variable ($\bar X$) which the observed number is a realisation of.
:::


# Avoiding sampling bias

Sampling bias occurs when the method used to select which units enter the sample causes the sample to not be a good representation of the population.

If sampling bias exists, we cannot generalise our sample conclusions to the population.

```{r echo=FALSE, out.width = '95%'}
knitr::include_graphics('images/prob/sampling_bias.png')
```

To be able to draw conclusions about the population, we need a representative sample. The key in choosing a representative sample is _random sampling_. 
Imagine an urn with tickets, where each ticket has the name of each population unit. Random sampling would involve mixing the urn and blindly drawing out some tickets from the urn.
Random sampling is a strategy to avoid sampling bias.


:::yellow
__Simple random sampling__

When we select the units entering the sample via simple random sampling, each unit in the population has an equal chance of being selected, meaning that we avoid sampling bias.

When instead some units have a higher chance of entering the same, we have misrepresentation of the population and sampling bias.
:::


In general, we have _bias_ when the method of collecting data causes the data to inaccurately reflect the population.



# Sampling distribution

The natural gestation period (in days) for human births is normally distributed in the population with mean 266 days and standard deviation 16 days. 
This is a special case which _**rarely**_ happens in practice: we actually know what the distribution looks like in the population. 

We will use this unlikely example to study how well does the sample mean estimate the population mean and, to do so, we need to know what the population mean is so that we can compare the estimate and the true value. 
Remember, however, that in practice the population parameter would _not_ be known.

We will consider data about the gestation period of the 49,863 women who gave birth in Scotland in 2019. These can be found at the following address: https://uoepsy.github.io/data/pregnancies.csv

First, we read the population data:
```{r}
library(tidyverse)
gest <- read_csv('https://uoepsy.github.io/data/pregnancies.csv')
dim(gest)
```

The data set contains information about 49,863 cases. For each case an identifier and the length of pregnancy
Look at the top six rows of the data set (the "head"):
```{r}
head(gest)
```

Let's load a function which we prepared for you called `rep_sample_n()`. This function is used to repeatedly sample $n$ units from the population.
To get the function in your computer, run this code:
```{r}
source('https://uoepsy.github.io/files/rep_sample_n.R')
```

:::red
__NOTE__

You need to copy and paste the line 
```{r}
source('https://uoepsy.github.io/files/rep_sample_n.R')
```
into each file where you want to use the `rep_sample_n()` function.
:::

The function takes the following arguments:
```
rep_sample_n(data, n = <sample size>, samples = <how many samples>)
```

- `data` is the population

- `n` is the sample size

- `samples` is how many samples of size $n$ you want to take

Before doing anything involving random sampling, it is good practice to _set the random seed_. This is to ensure reproducibility of the results. 
Random number generation in R works by specifying a starting seed, and then numbers are generated starting from there.

Set the random seed to any number you wish. Depending on the number, you will get the same results as me or not:
```{r}
set.seed(1234)
```

Obtain 12 samples of $n = 6$ individuals each:
```{r}
samples <- rep_sample_n(gest, n = 6, samples = 12)
samples
```

The `samples` data frame contains 3 columns:

- `sample`, telling us which sample each row refers to

- `id`, telling us the units chosen to enter each sample

- `gest_period`, telling us the gestation period (in days) of each individual

Note that the tibble `samples` has `r nrow(samples)` rows, which is given by 6 individuals in each sample * 12 samples.

You can inspect the sample data in the following interactive table in which the data corresponding to each sample have been colour-coded so that you can distinguish the rows belonging to the 1st, 2nd, ..., and 12th sample:
```{r echo=FALSE}
library(DT)

datatable(samples,
          options = list(autoWidth = TRUE, pageLength = 12,
                         columnDefs = list(list(className = 'dt-center', targets = 1:3)))) %>%
  formatSignif('gest_period', 5) %>%
  formatStyle('sample',
              backgroundColor = styleEqual(
                unique(samples$sample), 
                c("#8dd3c7", "#ffffb3", "#bebada", 
                  "#fb8072", "#80b1d3", "#fdb462", 
                  "#b3de69", "#fccde5", "#d9d9d9", 
                  "#bc80bd", "#ccebc5", "#ffed6f")
              ))
```

Now, imagine computing the mean of the six observation in each sample. This will lead to 12 means, one for each of the 12 samples (of 6 individuals each).

```{r}
sample_means <- samples %>%
  group_by(sample) %>%
  summarise(mean_gest = mean(gest_period))

sample_means
```

As you can see this leads to a tibble having 12 rows (one for each sample), where each row is a mean computed from the six individuals which were chosen to enter the sample.

<br>

The gestation period (in days) for the first six women sampled were
<center>
`r filter(samples, sample == 1)$gest_period %>% round(2)`
</center>
This sample has a mean of $\bar x$ = `r filter(samples, sample == 1)$gest_period %>% mean() %>% round(2)` days.

<br>

The second sample of 6 women had gestation periods
<center>
`r filter(samples, sample == 2)$gest_period %>% round(2)`
</center>
The second sample has a mean gestation period of $\bar x$ = `r filter(samples, sample == 2)$gest_period %>% mean() %>% round(2)` days.



<br>

In Figure \@ref(fig:sampl-12-6-24) we display the individual gestation periods in each sample as dots, along with the means gestation period $\bar x$ of each sample. The position of the sample mean is given by a red vertical bar.

We then increased the sample size to 24 women and took 12 samples each of 24 individuals. This set of samples together with their means is also plotted in Figure \@ref(fig:sampl-12-6-24).


```{r sampl-12-6-24, echo = FALSE, fig.height = 5, fig.width=9, out.width = '100%', fig.cap = "Gestation period (in days) of samples of individuals."}
library(patchwork)

all_theme <- 
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.line.x = element_line(),
        axis.ticks.x = element_line(),
        plot.title = element_text(hjust = 0.5, face = 'bold'))

da <- samples %>% 
  group_by(sample) %>%
  mutate(mean_gest = mean(gest_period))

pa <- ggplot(da, aes(x = gest_period, y = as.factor(sample))) +
  # geom_vline(aes(xintercept = 266), color = 'gray') + 
  geom_point(color = 'black', size = 2, pch = 1) +
  geom_segment(aes(x = mean_gest, xend = mean_gest, 
                   y = sample - 0.25, yend = sample + 0.25),
               size = 1, color = 'tomato3') +
  all_theme +
  # scale_x_continuous(
  #   breaks = round(seq(min(da$y, 230),max(da$y), by = 10))
  # ) + 
  labs(x = 'Gestation period (days)',
       y = 'Sample number',
       title = '(a) 12 samples of size n = 6')


db <- gest %>%
  rep_sample_n(n = 24, samples = 12) %>%
  group_by(sample) %>%
  mutate(mean_gest = mean(gest_period))

pb <- ggplot(db, aes(x = gest_period, y = as.factor(sample))) +
  # geom_vline(aes(xintercept = 266), color = 'gray') + 
  geom_point(color = 'black', size = 2, pch = 1) +
  geom_segment(aes(x = mean_gest, xend = mean_gest, 
                   y = sample - 0.25, yend = sample + 0.25),
               size = 1, color = 'tomato3') +
  all_theme +
  # scale_x_continuous(
  #   breaks = round(seq(min(df$y, 230),max(df$y), by = 10))
  # ) + 
  labs(x = 'Gestation period (days)',
       y = 'Sample number',
       title = '(b) 12 samples of size n = 24')

pa | pb
```


Two important points need to be made from Figure \@ref(fig:sampl-12-6-24). 
First, each sample (and therefore each sample mean) is different. This is due to the randomness of which individuals end up being in each sample.
The sample means, $\bar x$, vary in an unpredictable way, illustrating the fact that $\bar X$ is a summary of a random process (randomly choosing a sample) and hence is a random variable.
Secondly, as we increase the sample size from 6 to 24, there appears to be a decrease in the variability of sample means (compare the variability in the vertical bars in panel (a) and panel(b)).

To further investigate the variability of sample means, we will now generate many more sample means computed on:

(a) 5,000 samples of $n = 6$ women
(b) 5,000 samples of $n = 24$ women
(c) 5,000 samples of $n = 100$ women

```{r}
# (a) 5,000 means from 5,000 samples of 6 women
sample_means_6 <- rep_sample_n(gest, n = 6, samples = 5000) %>%
  group_by(sample) %>%
  summarise(mean_gest = mean(gest_period))

# (b) 5,000 means from 5,000 samples of 24 women
sample_means_24 <- rep_sample_n(gest, n = 24, samples = 5000) %>%
  group_by(sample) %>%
  summarise(mean_gest = mean(gest_period))

# (c) 5,000 means from 5,000 samples of 100 women
sample_means_100 <- rep_sample_n(gest, n = 100, samples = 5000) %>%
  group_by(sample) %>%
  summarise(mean_gest = mean(gest_period))
```

We now combine the above datasets of sample means for different sample sizes into a unique tibble. Before doing so, we add a column specifying the sample size. Remember that `mutate()` takes a tibble and adds or changes a column. The function `bind_rows()` takes multiple tibbles and stacks them under each other.
```{r}
sample_means_n <- bind_rows(
  sample_means_6 %>% mutate(n = 6),
  sample_means_24 %>% mutate(n = 24),
  sample_means_100 %>% mutate(n = 100)
)
```

We now plot three different density histograms showing the distribution of 5,000 sample means computed from samples of size 6, 24, and 100.

This would correspond to creating a histogram of the "red vertical bars" from Figure \@ref(fig:sampl-12-6-24), the only difference is that we have many more samples (5,000).

```{r sampl-dist-mean, fig.cap = "Density histograms of the sample means from 5,000 samples of women ($n$ women per sample).", fig.height=6, fig.width=5}
ggplot(sample_means_n) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  facet_grid(n ~ ., labeller = label_both) +
  theme_bw() + 
  labs(x = 'Sample mean of gestation period (days)', y = 'Density')
```

Each of the density histograms above displays the distribution of the sample mean, computed on samples of the same size and from the same population.

Such a distribution is called the _**sampling distribution**_ of the sample mean.

:::yellow
__Sampling distribution__

The sampling distribution of a statistic is the distribution of a sample statistic computed on many different samples of the same size from the same population.

A sampling distribution shows how the statistic varies from sample to sample due to sampling variation.
:::


# Centre and spread of the sampling distribution

What is the mean and standard deviation of each histogram?

```{r}
sample_means_n %>%
  group_by(n) %>%
  summarise(mean_xbar = mean(mean_gest),
            sd_xbar = sd(mean_gest))
```

Compare these quantities to the population mean and standard deviation: $\mu$ = `r mean(gest$gest_period) %>% signif(3)` and $\sigma$ = `r sd(gest$gest_period) %>% signif(3)`.

Regardless of the size of the samples we were drawing (6, 24, or 100), the average of the sample means was equal to the population mean.
However, the standard deviation of the sample means was smaller than the population mean. The variability in sample means also decreases as the sample size increases.

There is an interesting patter in the decrease, which we will now verify. 
It can be proved that the standard deviation of the sample mean $\sigma_{\bar X} = \frac{\sigma}{\sqrt{n}}$, i.e. the population standard deviation divided by $\sqrt{n}$ with $n$ being the sample size.

```{r}
sigma <- sd(gest$gest_period)

sample_means_n %>%
  group_by(n) %>%
  summarise(mean_xbar = mean(mean_gest),
            sd_xbar = sd(mean_gest)) %>%
  mutate(sd_theory = sigma / sqrt(n))
```

The last two columns will be closer and closer as we increase the number of different samples we take from the population (e.g. 5,000 or 10,000 or even more samples.)

The following result holds:
$$
\begin{aligned}
\mu_{\bar X} &= \mu = \text{Population mean} \\
\sigma_{\bar X} &= \frac{\sigma}{\sqrt{n}} = \frac{\text{Population standard deviation}}{\sqrt{\text{Sample size}}}
\end{aligned}
$$

Because on average the sample mean (i.e. the estimate) is equal to the population mean (i.e. the parameter), the sample mean $\bar X$ is an _unbiased_ estimator of the population mean. In other words, it does not consistently "miss" the target. (However, if your sampling method is biased, the sample mean will be biased too.)

The standard deviation of the sample means tells us that the variability in the sample means gets smaller smaller as the sample size increases. Because $\sqrt{4} = 2$ we halve $\sigma_{\bar X}$ by making the sample size 4 times as large. Similarly, as $\sqrt{9} = 3$, we reduce $\sigma_{\bar X}$ by one third by making the sample size 9 times as large.


The variability, or spread, of the sampling distribution shows how much the sample statistics tend to vary from sample to sample. This is key in understanding how accurate our estimate of the population parameter, based on just one sample, will be.

Recall that the standard deviation tells us the size of a typical deviation from the mean. Here, the mean is the population parameter $\mu$, and a deviation of $\bar x$ from $\mu$ is called an _estimation error_.
Hence, the standard deviation of the sample mean is called the _**standard error**_ of the mean. This tells us the typical estimation error that we commit when we estimate a population mean with a sample mean.

:::yellow
__Standard error__

The standard error of a statistic, denoted $SE$, is the standard deviation of its sampling distribution.
:::


<!-- In practice, we can only afford to take one sample from the population and we do not know the population standard deviation $\sigma$. -->
<!-- So, the standard error of the mean is computed using the standard deviation of the data in your sample: -->
<!-- $$ -->
<!-- SE = s_{\bar X} = \frac{s}{\sqrt{n}} -->
<!-- $$ -->

So, the standard error of the mean can be either computed as the standard deviation of the sampling distribution, or using the formula
$$
SE = \sigma_{\bar X} = \frac{\sigma}{\sqrt{n}}
$$



# The sample mean is normally distributed

We also notice that the density histograms in Figure \@ref(fig:sampl-dist-mean) are symmetric and bell-shaped. Hence, they follow the shape of the normal curve.

```{r echo=FALSE, fig.height=6, fig.width=5}
all_theme <- theme_bw() + 
  theme(
      plot.title = element_text(hjust = 0.5, face = 'bold')
  )

a <- ggplot(sample_means_6) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  geom_function(fun = dnorm, 
                args = list(mean = mean(sample_means_6$mean_gest), 
                            sd = sd(sample_means_6$mean_gest)), size = 1, color = 'red') + 
  all_theme +
  labs(x = 'Sample mean of gestation period (days)', y = 'Density',
       title = 'n: 6')

b <- ggplot(sample_means_24) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  geom_function(fun = dnorm, 
                args = list(mean = mean(sample_means_24$mean_gest), 
                            sd = sd(sample_means_24$mean_gest)), size = 1, color = 'red') + 
  all_theme + 
  labs(x = 'Sample mean of gestation period (days)', y = 'Density',
       title = 'n: 24')

c <- ggplot(sample_means_100) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  geom_function(fun = dnorm, 
                args = list(mean = mean(sample_means_100$mean_gest), 
                            sd = sd(sample_means_100$mean_gest)), size = 1, color = 'red') + 
  all_theme + 
  labs(x = 'Sample mean of gestation period (days)', y = 'Density',
       title = 'n: 100')

(a + xlim(240, 290)) /(b + xlim(240, 290)) / (c + xlim(240, 290))
```


The random variable $\bar X$ follows a normal distribution:
$$
\bar X \sim N(\mu,\ SE)
$$

We can also compute a z-score. We have that:
$$
\frac{\bar X - \mu}{SE} \sim N(0, 1)
$$

We know that for a normally distributed random variable, approximately 95% of all values fall within two standard deviations of its mean. 
Thus, for approximately 95% of all samples, the sample means falls within $\pm 2 SE$ of the population mean $\mu$.
Similarly, since $P(-3 < Z < 3) = 0.997$, it is even more rare to get a sample mean which is more than three standard errors away from the population mean (only 0.3% of the times).

This suggests that:

- The standard error $SE$ is a measure of precision of $\bar x$ as an estimate of $\mu$.

- If is a pretty safe bet to say that the true value of $\mu$ lies somewhere between $\bar x - 2 SE$ and $\bar x + 2 SE$.

- We will doubt any hypothesis specifying that the population mean is $\mu$ when the value $\mu$ is more than $2 SE$ away from the sample mean we got from our data, $\bar x$. We shall be even more suspicious when the hypothesised value $\mu$ is more than $3 SE$ away from $\bar x$.




:::yellow
__Centre and shape of a sampling distribution__

- _Centre_: If samples are randomly selected, the sampling distribution will be centred around the population parameter. _(No bias)_

- _Shape_: For most of the statistics we consider, if the sample size is large enough, the sampling distribution will follow a normal distribution, i.e. it is symmetric and bell-shaped. _(Central Limit Theorem)_
:::


Clearly, we can compute sampling distributions for other statistics too: the proportion, the standard deviation, ...

This requires the following steps:

- Obtaining multiple samples, all of the same size, from the same population;

- For each sample, calculate the value of the statistic;

- Plot the distribution of the computed statistics.



# Why sample size matters

You might be wondering: why did we take multiple samples of size $n$ from the population when, in practice, we can only afford to take one?

This is a good question. We have taken multiple samples to show how the estimation error varies with the sample size.
We saw in Figure \@ref(fig:sampl-dist-mean), shown again below, that smaller sample sizes lead to more variable statistics, while larger sample sizes lead to more **precise** statistics, i.e. the estimates are more concentrated around the true parameter value.

```{r echo=FALSE, fig.cap = "Density histograms of the sample means from 5,000 samples of women ($n$ women per sample).", fig.height=6, fig.width=5}
ggplot(sample_means_n) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  facet_grid(n ~ ., labeller = label_both) +
  theme_bw() + 
  labs(x = 'Sample mean of gestation period (days)', y = 'Density')
```

__This teaches us that, when we have to design a study, it is better to obtain just one sample with size $n$ as large as we can afford.__


`r qbegin("Think about it", FALSE)`
What would the sampling distribution of the mean look like if we could afford to take samples as big as the entire population, i.e. of size $n = N$?
`r qend()`
`r solbegin(show=TRUE, toggle=params$TOGGLE)`
_If you can_, it is best to measure the entire population. 

If we could afford to measure the entire population, then we would find the exact value of the parameter all the time.
By taking multiple samples of size equal to the entire population, every time we would obtain the population parameter exactly, so the distribution would look like a histogram with a single bar on top of the true value: we would find the true parameter with a probability of one, and the estimation error would be 0.

```{r}
pop_means <- gest %>%
  rep_sample_n(n = nrow(gest), samples = 10) %>%
  group_by(sample) %>%
  summarise(mean_gest = mean(gest_period))
pop_means
```

The following is a dotplot of the means computed above:
```{r echo=FALSE, fig.height=3, fig.width=4}
ggplot(pop_means, aes(x = mean_gest)) +
  geom_dotplot(binwidth = 1, dotsize = 0.05) +
  theme_classic() +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) + 
  labs(x = 'Sample mean', y = '')
```
`r solend()`



To summarize:

- We have __high precision__ when the estimates are less variable, and this happens for a __large sample size__. 

- We have __no bias__ when we select samples that are representative of the population, and this happens when we do __random sampling__. No bias means that the estimates will be centred at the true population parameter to be estimated.


```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/prob/bias_prec.png')
```




---

# Glossary

- **Statistical inference.** The process of drawing conclusions about the population from the data collected in a sample.
- **Population.** The entire collection of units of interest.
- **Sample.** A subset of the entire population.
- **Random sample.** A subset of the entire population, picked at random, so that any conclusion made from the sample data can be generalised to the entire population.
- **Representation bias.** Happens when some units of the population are systematically underrepresented in samples.
- **Generalisability.** When information from the sample can be used to draw conclusions about the entire population. This is only possible if the sampling procedure leads to samples that are representative of the entire population (such as those drawn at random).
- **Parameter.** A fixed but typically unknown quantity describing the population.
- **Statistic.** A quantity computed on a sample.
- **Sampling distribution.** The distribution of the values that a statistic takes on different samples of the same size and from the same population.
- **Standard error.** The standard error of a statistic is the standard deviation of the sampling distribution of the statistic.


---


# Exercises

`r qbegin(1)`
Two students, Mary and Alex, wanted to investigate the average hours of study per week among students in their university. 
Each were given the task to sample $n = 20$ students many times, and compute the mean of each sample of size 20.
Mary sampled the students at random, while Alex asked students from the library.
The distribution of sample means computed by Mary and Alex are shown in the dotplot below in green and red, respectively.

```{r echo=FALSE, fig.height = 4}
library(tidyverse)

set.seed(1)

df <- tibble(
  x = rep(c("random", "library"), each = 100),
  y = c(rnorm(100, 15, 2), rnorm(100, 24, 2))
)

ggplot(df, aes(x = y, fill = x)) +
  geom_dotplot(color = NA, dotsize = 0.8, alpha = 0.5) +
  theme_classic() + 
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(x = "Average hours of study", y = "", fill = "Sampling method")
```

What do you notice in the distributions above? Why did Mary and Alex get so different results?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Mary selected samples using random sampling, so we expect the samples to be representative of the population of interest. Hence, we will believe that Mary's sampling distribution of the mean is centred at the true population average of 15 hours of study per week.

On the other hand, Alex selected the most readily available people and took convenience samples. This leads to samples which are not a good representation of the population as a part of the population is missing. 
We notice that Alex got consistently higher estimates of the population mean study time than Mary did. This tendency to overestimate the population parameter shows that the sampling method is _biased._ Students in the library perhaps tend to study more.
`r solend()`


`r qbegin(2)`
**Average price of goods sold by ACME Corporation**

Suppose you work for a company that is interested in buying ACME Corporation^[You might remember it from the cartoon Wile E. Coyote and the Road Runner.] and your boss wants to know within the next 30 minutes what is the average price of goods sold by that company and how the prices of the goods they sell differ from each other.

Since ACME Corporation has such a big mail order catalogue, see Figure \@ref(fig:acme), we will assume that the company sells many products. Furthermore, we only have the catalogue in paper-form and no online list of prices is available.

```{r acme, out.width='50%', fig.align='center', echo=FALSE, fig.cap="Product catalogue of ACME corporation."}
knitr::include_graphics('images/prob/acme.jpg')
```

1. Identify the population of interest and the population parameters.
2. Can we compute the parameters within the next 30 minutes?
3. How would you proceed in estimating the population parameters if you just had time to read through 100 item descriptions? Would you pick the first 100 items or would you pick 100 random page numbers?
4. State which statistics you would use to estimate the population parameters.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
1. The population of interest is all products sold by ACME Corporation. The population parameters are the mean price $\mu$ and the standard deviation $\sigma$.
2. Because the catalogue has so many pages, we can not compute the population parameters within the next 30 minutes.
3. We must estimate the population mean and standard deviation from a sample of size $n = 100$. We should choose the items entering the sample at random, to avoid sampling bias. If we were to choose 100 consecutive items, we might end up with a very good estimate of the average price for the category those consecutive items belong to (e.g. gardening). However, this would not be a good estimate of the overall price across the multiple categories of products sold.
4. Our best guess of the population mean would be the sample mean, $\bar{x}$, and our best guess of the population standard deviation would be the sample standard deviation, denoted $s$ or $\hat{\sigma}$.
`r solend()`


`r qbegin(3)`
What is a parameter? Give two examples of parameters.

What is a statistic? Give two example of statistics.

What is an estimate?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
A parameter is a numerical summary of the population. An example could be average blood pressure in Scotland, or the proportion of people with a car.

A statistic is a numerical summary of the sample data. Examples are the mean blood pressure in a sample of 20 people, or the proportion of people with a car in a sample of 20 people.

We call "estimate" the value of a statistic which is used to estimate an unknown population parameter.
`r solend()`


`r qbegin(4)`
What is the distinction between an estimate and an estimator? 

Why is it made? What notational device is used to communicate the distinction?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
An estimate is the observed value in the same, while an estimator refers to all possible value of the statistic across all possible samples (hence, it's a random variable).

We denote the estimate (observed value) with a lowercase letter and the estimator (random variable) with an uppercase letter.

We make the distinction because we refer to the random variable (estimator) when we want to study the variability of the statistic from sample to sample, for example to investigate how precise it is.
`r solend()`



`r optbegin("Data: HollywoodMovies.csv", FALSE, show = TRUE, toggle = params$TOGGLE)`
**Download link**

Download the data [here](https://uoepsy.github.io/data/HollywoodMovies.csv).

**Description**

The data set stores information about 970 movies produced in Hollywood between 2007 and 2013.
It can be considered as the entire population of movies produced in Hollywood in that time period.

Among the recorded variables, three will be of interest:

- `Movie`: title of movie
- `Genre`: one of 14 possible genres
- `Budget`: production budget (in $ millions)


**Preview**

```{r echo=FALSE}
library(kableExtra)
df <- read_csv('https://uoepsy.github.io/data/HollywoodMovies.csv')
gt::gt(head(df))
```


`r optend()`


`r qbegin(5)`
**Reading data into R**

Read the Hollywood movies data into R, and call it `hollywood`.

Check that the data were read into R correctly.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
hollywood <- read_csv('https://uoepsy.github.io/data/HollywoodMovies.csv')
glimpse(hollywood)
```

`r solend()`


`r qbegin(6)`
**Extracting relevant variables**

Extract from the `hollywood` tibble the three variables of interest (`Movie`, `Genre`, `Budget`) and keep the movies for which we have all information (no missing entries).

__Hint:__ Check the help page for the function `drop_na()` or `na.omit()`.

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We can extract variables using the function `select()`, while to keep the rows for which we have all measurements we use `na.omit()`:
```{r}
hollywood <- hollywood %>%
  select(Movie, Genre, Budget) %>%
  drop_na()

hollywood
```
`r solend()`


`r qbegin(7)`
**Proportion of comedy movies**

What is the population proportion of comedy movies?
What is an estimate of the proportion of comedy movies using a sample of size 20?
Using the appropriate notation, report your results in one or two sentences.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
prop_comedy <- hollywood %>%
  summarise(prop = sum(Genre == "Comedy") / n())
prop_comedy

sample_prop_comedy <- hollywood %>%
  rep_sample_n(n = 20, samples = 1) %>%
  group_by(sample) %>%
  summarise(prop = sum(Genre == "Comedy") / n())
sample_prop_comedy
```

The population proportion of comedy movies is $p =$ `r prop_comedy$prop %>% round(2)`, while the proportion of comedy movies in the sample is $\hat{p} =$ `r sample_prop_comedy$prop %>% round(2)`.

`r solend()`


`r qbegin(8)`
**Sampling distributions**

What is a sampling distribution?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
The sampling distribution is the distribution of the values that a statistic takes on different samples of the same size and from the same population.
`r solend()`


`r qbegin(9)`
**Sampling distribution of the proportion**

Compute the sampling distribution of the proportion of comedy movies for samples of size $n = 20$, using 1000 different samples. 

Is it centred at the population value?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r w11-lab-sampling-dist, message=FALSE, fig.cap="Sampling distribution of the proportion for $n = 20$ with population parameter $p$ marked by a red vertical line."}
sample_props <- hollywood %>%
  rep_sample_n(n = 20, samples = 1000) %>% 
  group_by(sample) %>%
  summarise(prop = sum(Genre == 'Comedy') / n())

ggplot(sample_props, aes(x = prop)) +
  geom_histogram(color = 'white') +
  geom_vline(xintercept = prop_comedy$prop, color = 'red', size = 1) +
  labs(x = 'Sample proportion')
```

Yes, Figure \@ref(fig:w11-lab-sampling-dist) shows that the distribution is almost bell-shaped and centred at the population parameter.
`r solend()`

`r qbegin(10)`
**Standard error**

Using the replicated samples from the previous question, what is the standard error of the sample proportion of comedy movies?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
The standard error of the sample proportion is simply the standard deviation of the distribution of sample proportions for many samples. Since we have already computed the proportions for 1000 samples in the previous question, we just have to compute their variability using the standard deviation:
```{r}
se_prop <- sample_props %>%
  summarise(SE = sd(prop))
se_prop
```

The standard error of the sample proportion for sample size $n = 20$, based on 1000 samples, is $SE$ = `r se_prop$SE %>% round(2)`.
`r solend()`


`r qbegin(11)`
**The effect of sample size on the standard error of the sample proportion**

How does the sample size affect the standard error of the sample proportion?
Compute the sampling distribution for the proportion of comedy movies using 1,000 samples each of size $n = 20$, $n = 50$, and $n = 200$ respectively.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r w11-lab-vary-n, message=FALSE, fig.height=8, fig.width=7, fig.align='center', fig.cap="Three sampling distributions of the proportion, with population parameter $p$ marked by a red vertical line."}
sample_props_20 <- hollywood %>%
  rep_sample_n(n = 20, samples = 1000) %>% 
  group_by(sample) %>%
  summarise(prop = mean(Genre == 'Comedy'))

sample_props_50 <- hollywood %>%
  rep_sample_n(n = 50, samples = 1000) %>% 
  group_by(sample) %>%
  summarise(prop = mean(Genre == 'Comedy'))

sample_props_200 <- hollywood %>%
  rep_sample_n(n = 200, samples = 1000) %>% 
  group_by(sample) %>%
  summarise(prop = mean(Genre == 'Comedy'))

sample_props_vary_n <- bind_rows(
  sample_props_20 %>% mutate(n = 20),
  sample_props_50 %>% mutate(n = 50),
  sample_props_200 %>% mutate(n = 200)
)

ggplot(sample_props_vary_n, aes(x = prop)) +
  geom_histogram(color = 'white') +
  geom_vline(xintercept = prop_comedy$prop, color = 'red', size = 1) +
  facet_grid(n ~ ., labeller = label_both) +
  labs(x = 'Sample proportions', 
       title = "Sampling distribution of proportion for samples of size 20, 50, 200")
```

From Figure \@ref(fig:w11-lab-vary-n) we can see that, as the sample size increases, the standard error of the sample proportion decreases. Increasing the sample size, the spread of the statistic values is reduced.

`r solend()`



`r qbegin(12)`
**Comparing the budget for action and comedy movies**

What is the population average budget (in millions of dollars) allocated for making action vs comedy movies? And the standard deviation?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
budgets <- hollywood %>%
  filter(Genre == 'Action' | Genre == 'Comedy') %>%
  group_by(Genre) %>%
  summarise(avg_budget = mean(Budget),
            sd_budget = sd(Budget))
budgets
```

From the above tibble we see that action movies have been allocated a higher budget ($\mu_{Action} =$ `r budgets$avg_budget[1] %>% round(1)`) than comedy movies ($\mu_{Comedy} =$ `r budgets$avg_budget[2] %>% round(1)`). At the same time, action movies have a higher variability of budgets around the mean value ($\sigma_{Action} =$ `r budgets$sd_budget[1] %>% round(1)` vs $\sigma_{Comedy} =$ `r budgets$sd_budget[2] %>% round(1)`).
`r solend()`




<!-- Formatting -->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>

