---
title: "<b>Week 11: Samples, Statistics & Sampling Distributions </b>"
subtitle: "Data Analysis for Psychology in R 1<br><br> "
author: "DapR1 Team"
institute: "Department of Psychology<br>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
    base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```

```{r premable, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(infer)
library(kableExtra)
knitr::opts_chunk$set(dev = 'svg')

baseColor <- "#0F4C81"
```

# Week's Learning Objectives
1. Understand the difference between a population parameter and a sample statistic.
2. Understand the concept and construction of sampling distributions. 
3. Understand the effect of sample size on the sampling distribution.
4. Understand how to quantify the variability of a sample statistic and sampling distribution (standard error). 

---
# Topics for today
- Understand the principles of sampling from populations.
- Be familiar with the specific statistical terminology for sampling.
- Understand the concept of a sampling distribution.

---
## Concepts to carry forward
- Data can be of different types.

--

- Dependent on type (continuous vs. categorical), we can visualise and describe the distribution of data differently.

--

- When thinking about events ("things happening") we can assign probabilities to the event.

--

- We can define a probability distribution that describes the probability of all possible events.

---
## Why are These Concepts Relevant to Psych Stats?
- In psychology, we design a study, measure variables, and use these measurements to calculate a value that carries some meaning.
    - E.g., reaction time of one group vs another.

--


- Given it has meaning based on the study design, we want to know something about the value:
    - Is it unusual or not?
    - This is what we'll be focusing on throughout the remainder of the course. 

--

- **Today:**
    - We will talk about populations, samples, and sampling.
    - Basic concepts of sampling may seem simple and intuitive.
    - These concepts will be very useful when we start talking about _statistical inference_, or how we make decisions about data.

---
## Populations vs Samples
- In statistics, we often refer to populations and samples
    - **Population:** The group of people about whom you'd like to make inferences
    - **Sample:** The subset of the population from whom you will collect data to make these inferences

--

- To get the most accurate measure of our variable of interest, it would be ideal to collect data from the entire population; however, this is not feasible unless:
    - The population is small and easily defined
    - All members are easily accessible
    - All members are willing to provide data

--

- In the majority of cases, researchers take data from samples and use these results to make inferences about the population as a whole. 

- Let's visualise this...

---
## Populations vs Samples

- Suppose I wanted to know the proportion of UG students at the University of Edinburgh born in Scotland?
    - In stats talk, _all_ UG at the UoE are our **population**.

.center[
```{r, echo = F, out.width='65%', out.height='65%'}
knitr::include_graphics('dapR1_lec10_SamplingDist_files/Population.png')
```
]

---
## Populations vs Samples

- Suppose I wanted to know the proportion of UG students at the University of Edinburgh born in Scotland?
    - In stats talk, _all_ UG at the UoE are our **population**.
    - The proportion of students born in Scotland is a **population parameter** (a measure that describes the entire population).

.center[
```{r, echo = F, out.width='65%', out.height='65%'}
knitr::include_graphics('dapR1_lec10_SamplingDist_files/PopulationMetric.png')
```
]

---
## Populations vs Samples
- So how can we collect this information? 

--

- We could send out an email requesting all students to provide their place of birth...but it's not likely that all students will respond. 

--

- We could ask instructors to collect this data from students in their classes, but not every student will attend each class, and not every instructor will comply.

--

- Even with this relatively small, accessible population, it's not likely we could collect information from every single member.

---
## Populations vs Samples

- Instead, we have to use the data from students who _do_ respond to make inferences about the overall student population 
    - In this case, these students are our **sample**
    - The proportion of _these_ students born in Scotland is a **sample parameter**, or a **point-estimate**

.center[
```{r, echo = F, out.width='75%', out.height='75%'}
knitr::include_graphics('dapR1_lec10_SamplingDist_files/FullSample.png')
```
]

---
## Parameters and point-estimates
- It is the population parameter (proportion of Scottish born students at UoE) we are interested in. This is a *true* value of the world.

--

- We can draw a sample, and calculate this proportion in the sample.
- In a single sample, this **point-estimate** is our best guess at the population parameter.

--

.center[
```{r, echo = F, out.width='60%', out.height='60%'}
knitr::include_graphics('dapR1_lec10_SamplingDist_files/Samples.png')
```
]

---
## 2021/22 actual proportion
- If we draw multiple samples, we can produce a **sampling distribution**, which is a probability distribution of some statistic obtained from repeatedly sampling the population. 

--

- Let's use real data from this year to demonstrate this concept further (this is actually data regarding country of residence rather than birthplace, but we'll go with it for the purpose of this example).

```{r, echo=FALSE, warning=FALSE, message = FALSE}
#sim data - 8665/9
Edinburgh <- tibble(Scottish = c(rep('Yes', 8665), rep('No', 28755-8665)))

UoE_prop <- Edinburgh %>%
    group_by(Scottish) %>%
    summarise(n=n()) %>%
    mutate(Freq = round(n / sum(n),2))

kable(UoE_prop) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

--

- Using these data, let's simulate drawing a bunch of samples of students from the University and see what proportion of each sample is born in Scotland.

--

To do this, we'll...

  - 1) Randomly select 10 students from the population.
  - 2) Create a histogram showing how frequently our sample demonstrated specific proportions of Scotland-born students.

---
## A brief note on notation...

- It's important to know that although you may have seen these different types of notation used interchangeably in the past, they are actually slightly different when one is referring to a _population_ versus a _sample_:



|Population | Parameter         | Sample   |
|-----------|-------------------|----------|
| $\mu$     | Mean              | $\bar{x}$|
| $\sigma$  | Standard Deviation| $s$      |
| $N$       | Size              | $n$      |

---
## Visualizing sampling distributions

- Imagine we took only a single sample of 10 students. 
- Our histogram looks pretty empty at the moment, but we can use this to get a sense of what a full histogram will show.
- This also shows you how a single small sample may (or may not!) capture the truth of the overall population.

.center[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4}
set.seed(44)
sampDat <- sample(Edinburgh$Scottish, 10)

propDat <- tibble(Prop=as.numeric(table(sampDat) %>% prop.table)[2])

propDat %>%
  ggplot(., aes(x=Prop)) +
  geom_histogram(colour='gray', fill=baseColor) +
  scale_x_continuous(breaks = seq(0,1, 0.1), limits = c(0,1)) + 
  labs(x='Proportion', y='Frequency') +
  geom_vline(xintercept = 0.30, col = "red")

```
]


---
## Visualizing sampling distributions

- Now, let's look at a histogram that shows the distribution of our results if we took 10 samples of 10 students each.

.center[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4}
N <- 10
B <- 10

set.seed(820)

sampProps <- tibble(
    Sample1 = replicate(B, prop.table(table(sample(Edinburgh$Scottish, size = N, replace = TRUE)))[2]),
    Sample2 = replicate(B, prop.table(table(sample(Edinburgh$Scottish, size = N, replace = TRUE)))[2]),
    Sample3 = replicate(B, prop.table(table(sample(Edinburgh$Scottish, size = N, replace = TRUE)))[2])
    )


s10 <- sampProps %>%
  ggplot(., aes(Sample1)) +
  geom_histogram(breaks = seq(0, 1, 0.1), colour='gray', fill=baseColor) +
  labs(x='Proportion', y='Frequency') + xlim(0, .75) +
  geom_vline(xintercept = 0.30, color = "red", size = 1) +
  geom_vline(xintercept = mean(sampProps$Sample1), col = '#FCBB06', size = 1)

s10
```
]

---
## Visualizing sampling distributions

- If we were to repeat this process 2 more times, we can create three sampling distributions, each of which look different.

- Each sampling distribution is characterising the _sampling variability_ in our estimate of the parameter of interest (proportion of Scottish students at UoE).

- **Do samples with values close to the population value tend to be more or less likely?**

.center[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3.75, fig.width=7}

sampPropsLong <- reshape2::melt(sampProps)

meanDat <- data.frame(variable = c('Sample1', 'Sample2', 'Sample3'), Z = c(mean(sampProps$Sample1, na.rm = T), mean(sampProps$Sample2, na.rm = T), mean(sampProps$Sample3, na.rm = T)))

x <- sampPropsLong %>%
  ggplot(., aes(value)) +
  geom_histogram(breaks = seq(0, 1, 0.1), colour='gray', fill=baseColor) +
  labs(x='Proportion', y='Frequency') + xlim(0, .75) +
  geom_vline(xintercept = 0.30, color = "red", size = 1) +
  facet_wrap(~variable) +
  geom_vline(data = meanDat, aes(xintercept = Z), color = '#FCBB06', size = 1)

x

```
]


---
## More samples
- So far we have taken 10 samples...what if we took more? 
- Let's imagine we sampled 10 students 100 times. 

--

.center[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3.75, fig.width=7}
B <- 100

sampProps <- tibble(
    Sample1 = replicate(B, prop.table(table(sample(Edinburgh$Scottish, size = N, replace = TRUE)))[2]),
    Sample2 = replicate(B, prop.table(table(sample(Edinburgh$Scottish, size = N, replace = TRUE)))[2]),
    Sample3 = replicate(B, prop.table(table(sample(Edinburgh$Scottish, size = N, replace = TRUE)))[2])
    )

sampPropsLong <- reshape2::melt(sampProps)

meanDat <- data.frame(variable = c('Sample1', 'Sample2', 'Sample3'), Z = c(mean(sampProps$Sample1, na.rm = T), mean(sampProps$Sample2, na.rm = T), mean(sampProps$Sample3, na.rm = T)))

y <- sampPropsLong %>%
  ggplot(., aes(value)) +
  geom_histogram(breaks = seq(0, 1, 0.1), colour='gray', fill=baseColor) +
  labs(x='Proportion', y='Frequency') + xlim(0, .75) +
  geom_vline(xintercept = 0.30, color = "red", size = 1) +
  facet_wrap(~variable) +
  geom_vline(data = meanDat, aes(xintercept = Z), color = '#FCBB06', size = 1)

y

s100 <- sampProps %>%
  ggplot(., aes(Sample1)) + 
  geom_histogram(breaks = seq(0, 1, 0.1), colour='gray', fill=baseColor) +
  labs(x='Proportion', y='Frequency') + xlim(0, .75) +
  geom_vline(xintercept = 0.30, color = "red", size = 1) +
  geom_vline(xintercept = mean(sampProps$Sample1), color = '#FCBB06', size = 1)
  
```
]

--

- **What do you notice about these three plots compared to the previous three plots?**
    
---
## Bigger samples

- We've been taking samples of 10 students. Let's see what happens when we increase our sample size to $n = 50$, and then $n = 100$. 

.center[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3.75, fig.width=7}

N <- 50
sampProps$Sample2 <- replicate(B, prop.table(table(sample(Edinburgh$Scottish, size = N, replace = TRUE)))[2])

N <- 100
sampProps$Sample3 <- replicate(B, prop.table(table(sample(Edinburgh$Scottish, size = N, replace = TRUE)))[2])

sampPropsLong <- reshape2::melt(sampProps)

meanDat <- data.frame(variable = c('Sample1', 'Sample2', 'Sample3'), Z = c(mean(sampProps$Sample1, na.rm = T), mean(sampProps$Sample2, na.rm = T), mean(sampProps$Sample3, na.rm = T)))

z <- sampPropsLong %>%
  ggplot(., aes(value)) +
  geom_histogram(breaks = seq(0, 1, 0.1), colour='gray', fill=baseColor) +
  labs(x='Proportion', y='Frequency') + xlim(0, .75) +
  geom_vline(xintercept = 0.30, color = "red", size = 1) +
  facet_wrap(~variable) +
  geom_vline(data = meanDat, aes(xintercept = Z), color = '#FCBB06', size = 1)

z
```
]

---
## Properties of sampling distributions
- Remember: frequency distributions are characterising the variability in sample estimates.
    - Variability can be thought of as the spread in data/plots.

--

- So as we increase $n$, we are getting less variable samples (harder to get an unrepresentative sample as your $n$ increases).

--

- Let's put this phenomenon in the language of probability: 
    - As $n$ increases, the probability of observing an estimate in a sample that is a long way from the population parameter (here 0.30) decreases (becomes less probable).

--

- So when we have large samples, our estimates from those samples are likely to be closer to the population value.
    - That's good!

---
## Standard error
- We can formally calculate the "narrowness" of a sampling distribution, or the **standard error**

.center[

### $SE=\frac{\sigma}{\sqrt{N}}$

]

--

- This is essentially calculating the standard deviation (as we have done before) of the sampling distribution, with a key difference:
      - The standard deviation describes the variability _within_ one sample
      - The standard error describes variability _across_ multiple samples.
      
--

- The standard error gives you a sense of how different $\bar{x}$ is likely to be from $\mu$

--

- In this example where we're working with binomial data, the standard error indicates how greatly a particular sample proportion is likely to differ from the proportion in the population. 

---
## Properties of sampling distributions

.pull-left[
- Mean of the sampling distribution is close to $\mu$, even with a small number of samples.

- As the number of samples increases:
  - $\bar{x}$ of the sampling distribution approaches $\mu$.
  - The sampling distribution approaches a normal distribution.
    - $\bar{x}s$ pile up around the population value
    
]

.pull-right[

```{r, echo = F, fig.width=2.5, fig.height=2.5, message=F, warning=F}
s10
s100
```

]

---
count: false

## Properties of sampling distributions

.pull-left[
- Mean of the sampling distribution is close to $\mu$, even with a small number of samples.

- As the number of samples increases:
  - $\bar{x}$ of the sampling distribution approaches $\mu$.
  - The sampling distribution approaches a normal distribution.
    - $\bar{x}s$ pile up around the population value
    
- As $n$ per sample increases, the SE of the sampling distribution decreases (becomes narrower).
    - With large n, all our point-estimates are closer to the population parameter.
]

.pull-right[

```{r, echo = F, fig.width=2.5, fig.height=2.5, message=F, warning=F}
s10
s100
```

```{r, echo = F, fig.width=5, fig.height=2.5, message=F, warning=F}
z
```

]

---
## Two Related Concepts

- What we've seen throughout the lecture demonstrates **The Law of Large Numbers**
    - This states that as $n$ increases, $\bar{x}$ approaches $\mu$

--

- Another important and related concept is the **Central Limit Theorem**.  
    - The central limit theorem (roughly) states that when estimates of $\bar{x}$ are based on increasingly large samples ( $n$ ), the sampling distribution of $\bar{x}$ becomes more normal (symmetric), and narrower (quantified by the standard error).
    - To demonstrate this, let's explore some different distributions.
    
---
## Uniform distribution

.pull-left[
- Continuous probability distribution 

- There is an equal probability for all values within a given range.
]

.pull-right[
```{r, echo=FALSE, fig.width=5, fig.height=5, warning=F, message=F}
unif <- tibble(
  x = runif(10000, 0, 5))

uni_pop <- ggplot(unif, aes(x=x)) +
  geom_histogram(color = "white", fill = baseColor) +
    theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  ggtitle("Population")

uni_pop
```
]


---
## Uniform distribution

```{r, echo=FALSE, warning=FALSE, message=FALSE}
uni1 <- unif %>%
  rep_sample_n(size = 10, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarise(x = mean(x)) %>%
  ggplot(., aes(x = x)) + 
  xlim(0,5) +
  geom_histogram(color = "white", fill = baseColor) + 
  labs(x = "\n Mean of X") +
  ggtitle("N = 10") +
      theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

uni2 <- unif %>%
  rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarise(x = mean(x)) %>%
  ggplot(., aes(x = x)) + 
  xlim(0,5) +
  geom_histogram(color = "white", fill = baseColor) + 
  labs(x = "\n Mean of X") +
  ggtitle("N = 50") +
      theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

uni3 <- unif %>%
  rep_sample_n(size = 100, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarise(x = mean(x)) %>%
  ggplot(., aes(x = x)) + 
  xlim(0,5) +
  geom_histogram(color = "white", fill = baseColor) + 
  labs(x = "\n Mean of X") +
  ggtitle("N = 100") +
      theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```
.pull-left[
.center[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=5, fig.height=2.5}
uni_pop
uni2
```
]]

.pull-right[
.center[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=5, fig.height=2.5}
uni1
uni3
```
]]

---
## Chi-square distribution

.pull-left[
- Continuous probability distribution
- Non-symmetric
]

.pull-right[
```{r, echo=FALSE, fig.height=5}
ggplot(data.frame(x = c(0,20)), aes(x = x)) +
  stat_function(fun=dchisq,
                geom = "line",
                args = list(df=3)) +
    stat_function(fun=dchisq,
                geom = "line",
                col = "red",
                args = list(df=5)) +
      stat_function(fun=dchisq,
                geom = "line",
                col = "blue",
                args = list(df=10)) +
  xlab("\n X") +
  ylab("Probability \n") +
  ggtitle("Chi-square distributions with 3 (black), 5 (red) & 10 (blue) df")
```
]


---
## Chi-square distribution
```{r, echo=FALSE, warning=FALSE, message=FALSE}
chi <- tibble(
  x = rchisq(10000, 3))

chi_pop <- ggplot(chi, aes(x=x)) +
  geom_histogram(color = "white", fill = baseColor) +
  ggtitle("Population") +
      theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

chi1 <- chi %>%
  rep_sample_n(size = 10, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarise(x = mean(x)) %>%
  ggplot(., aes(x = x)) + 
  xlim(0,15) +
  geom_histogram(color = "white", fill = baseColor) + 
  labs(x = "\n Mean of X") +
  ggtitle("N = 10") +
      theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

chi2 <- chi %>%
  rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarise(x = mean(x)) %>%
  ggplot(., aes(x = x)) + 
  xlim(0,15) +
  geom_histogram(color = "white", fill = baseColor) + 
  labs(x = "\n Mean of X") +
  ggtitle("N = 50") +
      theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

chi3 <- chi %>%
  rep_sample_n(size = 100, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarise(x = mean(x)) %>%
  ggplot(., aes(x = x)) + 
  xlim(0,15) +
  geom_histogram(color = "white", fill = baseColor) + 
  labs(x = "\n Mean of X") +
  ggtitle("N = 100") +
      theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```
.pull-left[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=5, fig.height=2.5}
chi_pop
uni2
```
]

.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=5, fig.height=2.5}
chi1
chi3
```
]

---
## t-distribution

.pull-left[
- Continuous probability distribution.
- Symmetric and uni-modal (similar to the normal distribution).
  - "Heavier tails" = greater chance of observing a value further from the mean
]

.pull-right[
```{r, echo=FALSE, fig.height=5}
ggplot(data.frame(x=c(-3,3)), aes(x = x)) +
  stat_function(fun=dt,
                geom = "line",
                args = list(df=1)) +
    stat_function(fun=dt,
                geom = "line",
                col = "red",
                args = list(df=5)) +
      stat_function(fun=dt,
                geom = "line",
                col = "blue",
                args = list(df=25)) +
  xlab("\n X") +
  ylab("Probability \n") +
  ggtitle("t-distributions with 1 (black), 5 (red) & 25 (blue) df")
```
]

---
## t-distribution
```{r, echo=FALSE, warning=FALSE, message=FALSE}
t_df <- tibble(
  x = rt(10000, 5))

t_pop <- ggplot(t_df, aes(x=x)) +
  geom_histogram(color = "white", fill = baseColor) +
  ggtitle("Population") +
  xlim(-5,5) +
      theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

t1 <- t_df %>%
  rep_sample_n(size = 10, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarise(x = mean(x)) %>%
  ggplot(., aes(x = x)) + 
  xlim(-5,5) +
  geom_histogram(color = "white", fill = baseColor) + 
  labs(x = "\n Mean of X") +
  ggtitle("N = 10") +
      theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

t2 <- t_df %>%
  rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarise(x = mean(x)) %>%
  ggplot(., aes(x = x)) + 
  xlim(-5,5) +
  geom_histogram(color = "white", fill = baseColor) + 
  labs(x = "\n Mean of X") +
  ggtitle("N = 50") +
      theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

t3 <- t_df %>%
  rep_sample_n(size = 100, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarise(x = mean(x)) %>%
  ggplot(., aes(x = x)) + 
  xlim(-5,5) +
  geom_histogram(color = "white", fill = baseColor) + 
  labs(x = "\n Mean of X") +
  ggtitle("N = 100") +
      theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

.pull-left[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=5, fig.height=2.5}
t_pop
t2
```
]

.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=5, fig.height=2.5}
t1
t3
```
]

---
## Central Limit Theorem

- These examples all demonstrate the Central Limit Theorem
- When $n$ is large enough, $\bar{x}s$ approximate a normal distribution around $\mu$, regardless of the underlying population distribution

.center[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=3, fig.height=2}
uni_pop
chi_pop
t_pop
```
]

.center[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=3, fig.height=2}
uni2
chi2
t2
```
]

---
## Features of samples
- Is our sample...
    - Biased?
    - Representative?
    - Random?

--

- If a sample of $n$ is drawn at random, it is likely to be unbiased and representative of $N$

- Our point estimates from such samples will be good guesses at the population parameter.

---
# Summary of today
- Samples are used to estimate the population. 
- Samples provide point estimates of population parameters. 
- Properties of samples and sampling distributions.
- Properties of good samples.

---
# Next tasks

+ This week:
  + Complete your lab
  + Come to office hours
  + No weekly quiz - submit group Formative Report B by Friday 12 noon.

+ Next week, there will be no new lecture material:
  + No lectures on Monday and Tuesday next week
  + Use next week to review and catch up on weeks 7-11.
  + In semester 2 we will begin looking at inference.

+ Labs will still take place next week:
  + Make sure you still go to labs next week
  + You will receive in-person feedback on Formative Report B

