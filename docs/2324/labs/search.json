[
  {
    "objectID": "rd2_06.html",
    "href": "rd2_06.html",
    "title": "One sample t-test",
    "section": "",
    "text": "A one-sample t-test is a statistical procedure to test whether or not the mean of a population (\\(\\mu\\)) is equal to some hypothesised value (\\(\\mu_0\\)). Examples when you would use a one-sample t-test include:"
  },
  {
    "objectID": "rd2_06.html#one-sample-t-test-procedure",
    "href": "rd2_06.html#one-sample-t-test-procedure",
    "title": "One sample t-test",
    "section": "\n1 One-sample t-test procedure",
    "text": "1 One-sample t-test procedure\n\n1.1 Null hypothesis\nA one-sample t-test always has the null hypothesis:\n\\[ H_0: \\mu = \\mu_0 \\]\nI.e. the population mean is equal to some hypothesised value, \\(\\mu_0\\).\n\n1.2 Alternative hypothesis\nThe alternative hypotheses can be two-sided, or one-sided (left-tailed or right-tailed).\n\n\nTwo-sided/Two-tailed: \\(H_1 : \\mu \\neq \\mu_0\\)\n(population mean is not equal to some hypothesised value \\(\\mu_0\\))\n\n\nLeft-sided/Left-tailed: \\(H_1 : \\mu &lt; \\mu_0\\)\n(population mean is less than some hypothesised value \\(\\mu_0\\))\n\n\nRight-sided/Right-tailed: \\(H_1 : \\mu &gt; \\mu_0\\)\n(population mean is greater than some hypothesised value \\(\\mu_0\\))"
  },
  {
    "objectID": "rd2_06.html#t-statistic",
    "href": "rd2_06.html#t-statistic",
    "title": "One sample t-test",
    "section": "\n2 t-statistic",
    "text": "2 t-statistic\nThe test statistic is the t-statistic:\n\\[t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} \\]\nwhere\n\n\n\\(\\bar{x}\\) = sample mean\n\n\\(\\mu_0\\) = hypothesised value\n\n\\(s\\) = standard deviation of the sample\n\n\\(n\\) = sample size\n\nAs we have produced a sample, we are using the sample statistics to estimate the population parameters. When the population standard deviation (\\(\\sigma\\)) is unknown, we estimate it using the sample standard deviation (\\(s\\)).\n\n2.1 The t-distribution\nThe t-distribution is a family of distributions that look almost identical to the standard normal distribution. The t-distribution is very important when working with small sample sizes, e.g. \\(n &lt; 30\\). As the sample size increases, the more the t-distribution will look like the standard normal distribution.\nThe particular shape of the t-distribution is determined by the degrees of freedom, or df in short. You can see the \\(t\\)-distribution for different degrees of freedom below.\n\n\n\n\nThe t-distribution. (Source: https://en.wikipedia.org/wiki/Student%27s_t-distribution)\n\n\n\nNotice that as the degrees of freedom (\\(\\nu\\) in the plot below) gets bigger (so as \\(n\\) gets bigger), the more the \\(t\\)-distribution fits a normal distribution.\n\n\n\n\n\n\nOptional: If you want to know more about degrees of freedom\n\n\n\n\n\nDegrees of freedom are the number of independent observations in a set of data. When we are estimating a mean from a single sample, the degrees of freedom is equal to the sample size minus one (\\(n-1\\)).\nThis means that the sampling distribution of \\(t\\)-statistics from samples of size 10, would follow a \\(t\\)-distribution with \\(10-1\\) degrees of freedom.\nWhy do we subtract 1 from the number of observations?1\nDegrees of freedom can alternatively be seen as the number of values that are free to vary in a data set. For example, if we have 3 sets of numbers which have a mean of 10:\n\n\n\\(a\\): {9,10,11}\n\n\\(b\\): {8,10,12}\n\n\\(c\\): {5,10,15}\n\nOnce you have chosen the first two numbers in the set, the third number is fixed. I.e. you cannot choose the third item in the set. If you choose set \\(a\\), once you’ve chosen numbers 9 and 10, the next number must be 11 in order to get a mean of 10. The only numbers that are free to vary are the first two, thus the degrees of freedom for a set of three numbers, is two."
  },
  {
    "objectID": "rd2_06.html#critical-values-and-significance",
    "href": "rd2_06.html#critical-values-and-significance",
    "title": "One sample t-test",
    "section": "\n3 Critical values and significance",
    "text": "3 Critical values and significance\nIn order to test the significance of a given \\(t\\)-statistic, we need to assess the probability of obtaining our \\(t\\)-statistic (or one at least as extreme) against a \\(t\\)-distribution with degrees of freedom \\(n-1\\).\nWe can do this in R using the pt() function with pt(x, df).\nRemember that we have used the function pnorm(x, mean, sd) to compute the area to the left of x in a normal curve centred at mean and having standard deviation sd.\nSimilarly, pt(x, df) computes the area to the left of x in a \\(t\\)-distribution curve with degrees of freedom df.\n\n\n\n\n\n\nQuestion. Looking at the plot above, for a \\(t\\)-distribution with degrees of freedom of 5 (the blue line), what proportion of the curve is to the left of -2?\n\n\n\n\npt(-2, df = 5)\n\n[1] 0.05096974\n\n\nFrom this, we can say that assuming the null hypothesis to be true, the probability of obtaining a \\(t\\)-statistic with 5 degrees of freedom of \\(\\leq -2\\) is 0.051.\nWe can also find the critical values of a \\(t\\)-distribution using the function qt(p, df). This will return the values of \\(t\\) for which \\(p\\) of the distribution lies to the left.\nThis way, we can find the values of \\(t\\) at which we will reject the null hypothesis (for a given \\(\\alpha\\) level).\n\n\n\n\n\n\nQuestion. At what value of \\(t\\) does 5% of the \\(t\\)-distribution with 5 degrees of freedom lie to the left?\n\n\n\n\nqt(.05, df = 5)\n\n[1] -2.015048\n\n\nIf we perform a one-tailed test of \\(\\mu_1 &lt; \\mu_{0}\\) on a sample of 6 (so our degrees of freedom is 5), we will reject the null hypothesis (\\(\\mu_1 = \\mu_{0}\\)) if our corresponding \\(t\\)-statistic is \\(\\leq -2.015\\).\n\n\n\n\n\n\nQuestion. At what values of \\(t\\) do 5% of the \\(t\\)-distribution with 5 degrees of freedom lie in either tail?\n\n\n\n\nqt(.025, df = 5)\n\n[1] -2.570582\n\nqt(.975, df = 5) \n\n[1] 2.570582\n\n\nRemember that the t-distribution is symmetric and centred on 0!\nIf we perform a two-tailed test of \\(\\mu_1 \\neq \\mu_{0}\\) on a sample of 6, we will reject the null hypothesis (\\(\\mu_1 = \\mu_{0}\\)) if the absolute magnitude of our corresponding \\(t\\)-statistic is \\(\\geq 2.571\\)."
  },
  {
    "objectID": "rd2_06.html#recap-and-moving-forward-body-temperature-data",
    "href": "rd2_06.html#recap-and-moving-forward-body-temperature-data",
    "title": "One sample t-test",
    "section": "\n4 Recap and moving forward: Body Temperature data",
    "text": "4 Recap and moving forward: Body Temperature data\nWe will now recap the Body Temperature example, putting everything together. Recall the goal is to answer this question:\n\nHas the average body temperature for healthy humans changed from the long-thought 37 °C?\n\nAgain, we will be using the data2 comprising measurements on body temperature and pulse rate for a sample of \\(n = 50\\) healthy subjects. The data are stored at the following address: https://uoepsy.github.io/data/BodyTemp.csv\nFor a one-sample t-test we are evaluating if the average body temperature is significantly different from the population mean of 37°C. It would be extremely time-consuming, costly (and near impossible) to take everyone’s body temperature. Instead, we might take a simple random sample of healthy humans and use the mean body temperature of this sample to estimate the true population mean.\n\n\n\n\n\n\nSimple random sampling (SRS)\nSimple random sampling is a type of sampling technique. Sampling techniques are used by companies, researchers and individuals for a variety of reasons. Sampling strategies are useful when conducting surveys and answering questions about populations. There are many different methods researchers can use to obtain individuals to be in a sample. These are known as sampling methods.\nSimple random sampling is, unsurprisingly, the simplest form of probability sampling: every member in the population has an equal chance of being selected in the sample. Individuals are usually selected by a random number generator or some other mean of random sampling.\nThe biggest benefit of SRS is it removes bias, as everyone has an equal chance of being selected. Furthermore, the sample is representative of the population.\n\n\n\nFirst, we need to write the null and alternative hypotheses.\n\\[H_0 : \\mu = 37 °C\\] \\[H_1 : \\mu \\neq 37 °C\\]\nNext, we read the data into R and calculate the average body temperature (sample statistic) of the sample group.\n\nlibrary(tidyverse)\ntemp_data &lt;- read_csv('https://uoepsy.github.io/data/BodyTemp.csv')\ndim(temp_data)\n\n[1] 50  2\n\nhead(temp_data)\n\n# A tibble: 6 × 2\n  BodyTemp Pulse\n     &lt;dbl&gt; &lt;dbl&gt;\n1     36.4    69\n2     37.4    77\n3     37.2    75\n4     37.1    84\n5     36.7    71\n6     37.2    76\n\n\nWe have measurements of the body temperature (in Celsius) and pulse rate for a sample of 50 healthy individuals.\nThe average body temperature in the sample is:\n\nxbar &lt;- mean(temp_data$BodyTemp)\nxbar\n\n[1] 36.81111\n\n\nDo we know the population standard deviation (\\(\\sigma\\))? That is, do we know the standard deviation of body temperate of all healthy individuals?\nNo, so we estimate it with the sample standard deviation (\\(s\\)).\n\ns &lt;- sd(temp_data$BodyTemp)\ns\n\n[1] 0.4251776\n\n\nWe also know that:\n\nn &lt;- nrow(temp_data)\nn\n\n[1] 50\n\nmu0 &lt;- 37\nmu0\n\n[1] 37\n\n\nNow, we have all the data to perform the t-test. Insert our values into the t-statistic formula, defined to be:\n\\[\nt  = \\ \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\n\\]\nAnd to calculate, we can use R as a calculator:\n\nt_obs &lt;- (xbar - mu0) / (s / sqrt(n))\nt_obs\n\n[1] -3.141384\n\n\n\n\n\n\n\n\nWarning!\n\n\n\nNote that you should only round your numbers at the very end of calculations!\n\n\nGoing back to the start, we can use our skills with summarise() to calculate all the terms we need for our \\(t\\)-statistic:\n\nterms &lt;- temp_data %&gt;%\n  summarise(\n    xbar = mean(BodyTemp),\n    s = sd(BodyTemp),\n    mu0 = 37,\n    n = n()\n  )\n\nterms\n\n# A tibble: 1 × 4\n   xbar     s   mu0     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  36.8 0.425    37    50\n\n\nAnd then we can plug in these numbers to our equation.\n\n4.1 Critical t-value\nUsing the qt() function, calculate the critical value for \\(\\alpha\\) = 0.05. This is the smallest absolute value of \\(t\\) at which you will reject the null hypothesis.\n\nYou’ll need to work out the degrees of freedom\n\nYou’ll also need to think about whether we are performing a two-tailed test or a one-tailed test. If a two-tailed test, then remember that the \\(\\alpha\\) is split between the two tails (and so we would reject anything in the most extreme 2.5%)\n\nThe degrees of freedom are \\(n-1 = 50-1 = 49\\)\nWe’re performing a two-tailed test as we do not know if the mean body temperature will be below or above the stipulated 37°C. So we will reject a \\(t\\)-statistic which falls in the upper or lower 2.5% of the distribution.\n\nqt(c(.025, 0.975), df = 49) # 2.5% in the lower and upper tails. \n\n[1] -2.009575  2.009575\n\n\nWe will reject a \\(t\\)-statistic which is less than -2.009575 or greater than 2.009575.\n\n4.2 P-value\nWe have our observed \\(t\\)-statistic of -3.14. We know that this is more extreme than the critical value of -2.009575.\nWhat is the probability of obtaining a \\(t\\)-statistic at least as extreme as -3.14, assuming the null hypothesis to be true? In other words, what is the p-value?\nBecause our alternative hypothesis is two-tailed, we will reject the null hypothesis for extreme \\(t\\)-statistics in either direction. So we are calculating the probability of observing a value at least as extreme in either direction, and must multiply the one tail by 2.\n\n2 * pt(abs(t_obs), df = 50-1, lower.tail = FALSE)\n\n[1] 0.002850509\n\n\nAnd so, the probability of observing a value as extreme as -3.14 is 0.003 (i.e. very low!). Therefore, we have enough evidence to reject the null hypothesis and conclude that the mean body temperature of healthy adults is significantly lower than the long-held value of 37°C.\n\n4.3 Calculating a one-sample t-test with one function\nNow that we’ve gone through all that, you’ll be happy to know that we can do all of what we just did above (and more!) using just one simple function in R, called t.test().\nThe t.test() function takes several arguments, but for the current purposes, we are interested in t.test(x, mu, alternative, conf.level).\n\n\nx is the data\n\n\nmu is the hypothesized value of the mean in \\(H_0\\)\n\n\nalternative is either \"two.sided\" (default), \"less\", or \"greater\", and specifies the direction of the alternative hypothesis.\n\nconf.level confidence level of the interval. By default, this is 0.95 but you can change it to be for example 0.90 or 0.99.\n\n\nresult &lt;- t.test(temp_data$BodyTemp, mu = 37, alternative = \"two.sided\")\nresult\n\n\n    One Sample t-test\n\ndata:  temp_data$BodyTemp\nt = -3.1414, df = 49, p-value = 0.002851\nalternative hypothesis: true mean is not equal to 37\n95 percent confidence interval:\n 36.69028 36.93195\nsample estimates:\nmean of x \n 36.81111 \n\n\nAs we can see, the output of the t.test gives us our t-value (-3.14), our degrees of freedom (df = 49), our p-value (0.002851). It also provides us with 95% CIs of the population mean and the mean of our sample body temp (36.81). It looks like everything matches up with our calculations above!"
  },
  {
    "objectID": "rd2_06.html#assumptions",
    "href": "rd2_06.html#assumptions",
    "title": "One sample t-test",
    "section": "\n5 Assumptions",
    "text": "5 Assumptions\nOne last important thing to note is that when we perform a one sample mean tests, we assume a few basic things:\n\nThe data are continuous;\nThe data are independent;\nThe data are normally distributed OR the sample size is large enough (rule-of-thumb \\(n \\geq 30 30\\)) and the data are not strongly skewed;\n\nIf any of these assumptions are not met, the results of the test are unreliable.\nTo check the assumptions: 1) The dependent variable should be measured at the interval or ratio level 2) Data is independent (i.e., not correlated/related), which means that there is no relationship between the observations. This is more of a study design issue than something you can test for, but it is an important assumption of the one-sample t-test. 3) Can be checked visually with plots:\n\nggplot(temp_data, aes(x=BodyTemp))+\n  geom_histogram()+\n  labs(title=\"Histogram\")\n\n\n\n\n\n\nggplot(temp_data, aes(x=BodyTemp))+\n  geom_density()+\n  labs(title=\"Density plot\")\n\n\n\n\n\n\n\nWe can also use a plot called a QQplot (Quantile-Quantile plot), which orders the data and plots it against the equivalent quantile of the normal distribution:\n\nggplot(temp_data, aes(sample = BodyTemp))+\n  geom_qq()+\n  geom_qq_line()+\n  labs(title=\"QQ-plot\", \n       subtitle=\"The closer the data fit to the line the more normally distributed they are.\",\n       x = \"Theoretical quantiles\",\n       y = \"Sample quantiles\")\n\n\n\n\n\n\n\nWe can also conduct a formal hypothesis test for normality, such as the Shapiro-Wilk test.\nThe null hypothesis of the Shapiro-Wilk test is that the sample came from a population that is normally distributed.\nThe alternative hypothesis is that the sample came from a population that is not normally distributed.\nThe test returns a test statistic \\(W\\), and a p-value \\(p\\). The \\(W\\) test statistic is slightly complex to compute by hand, so we will use R to compute it for us. The test statistic \\(W\\) measures how much the sample data depart from normality (the null hypothesis).\nThe p-value corresponds to the probability of observing data of this shape of distribution, assuming the data are drawn from a normally distributed population (i.e., assuming the null hypothesis to be true).\nIn R:\n\nshapiro.test(temp_data$BodyTemp)\n\n\n    Shapiro-Wilk normality test\n\ndata:  temp_data$BodyTemp\nW = 0.97322, p-value = 0.3115\n\n\nThe p-value here is 0.31, which is greater than \\(\\alpha = 0.05\\). We therefore fail to reject the null hypothesis of the Shapiro-Wilk test that the sample came from a population that is normally distributed.\nSo our assumption for the one sample mean test holds!"
  },
  {
    "objectID": "rd2_06.html#worked-example",
    "href": "rd2_06.html#worked-example",
    "title": "One sample t-test",
    "section": "\n6 Worked example",
    "text": "6 Worked example\n\n6.1 Pets’ weights\nData for a sample of 2000 licensed pets from the city of Seattle, USA, can be found at the following url: https://uoepsy.github.io/data/seattlepets.csv.\nIt contains information on the license numbers, issue date and zip-code, as well as data on the species, breeds and weights (in kg) of each pet.\nWe are interested in whether the average weight of a Seattle dog is greater than 20kg.\nDefinition of null and alternative hypotheses\nLet \\(\\mu\\) denote the mean weight (in kg) of all dogs in Seattle. We wish to test:\n\\[H_0: \\mu = 20\\] \\[H_1: \\mu &gt; 20\\]\n\n\n\n\n\n\nQuestion\n\nRead the data into R.\nUse summary() to have a look at your data.\nWhich variables are you going to need for our analysis?\n\nDoes anything jump out as relevant?\n\n\n\n\n\npets &lt;- read_csv(\"https://uoepsy.github.io/data/seattlepets.csv\")\n\nsummary(pets)\n\n license_issue_date license_number     animals_name         species         \n Length:2000        Length:2000        Length:2000        Length:2000       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n primary_breed      secondary_breed      zip_code           weight_kg       \n Length:2000        Length:2000        Length:2000        Min.   :  0.3941  \n Class :character   Class :character   Class :character   1st Qu.:  4.6970  \n Mode  :character   Mode  :character   Mode  :character   Median : 16.3954  \n                                                          Mean   : 15.2411  \n                                                          3rd Qu.: 22.4816  \n                                                          Max.   :103.4838  \n                                                          NA's   :15        \n\n\nWe’re going to need the weight_kg variable. Notice that there are some missing values (you can see that there are 15 NA’s). We will need to decide what to do with them.\nAlso, there are some cats in our data as well as the dogs which we are interested in. There are even a couple of goats! We will want to get rid of them..\n\n\n\n\n\n\nQuestion. Create a new dataset and call it dogs, which only has the dogs in it.\n\n\n\n\ndogs &lt;- pets %&gt;% \n    filter(species == \"Dog\")\n\n\n\n\n\n\n\nQuestion. Remove the rows having a missing weight.\n\n\n\nThere are two completely equivalent options. Pick only one of these two options to remove the missing entries, as both will achieve the same goal.\nOption 1:\n\ndogs &lt;- dogs %&gt;% \n  drop_na(weight_kg)\n\nOption 2:\nLook at the help documentation for is.na (search in the bottom right window of RStudio, or type ?is.na)\n\ndogs &lt;- dogs %&gt;% \n  filter(!is.na(weight_kg))\n\nIt takes the dogs dataset, and it filters so that it will keep any rows where !is.na(weight_kg) is TRUE.\nThe is.na(weight_kg) will be TRUE wherever weight_kg is an NA and FALSE otherwise.\nThe ! before it flips the TRUEs and FALSEs, so that we have TRUE wherever weight_kg is a value other than NA, and FALSE if it is NA.\nYou can read it as “keep all rows where there isn’t an NA in the weight_kg variable”.\n\n\n\n\n\n\nQuestion. Using summarise(), calculate \\(\\bar{x}\\), \\(s\\) and \\(n\\).\nWhat is \\(\\mu_{0}\\), and what are our degrees of freedom (\\(df\\))?\n\n\n\n\ndogstats &lt;- dogs %&gt;% \n  summarise(\n    xbar = mean(weight_kg),\n    s = sd(weight_kg),\n    n = n()\n  )\ndogstats\n\n# A tibble: 1 × 3\n   xbar     s     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  20.4  6.23  1333\n\n\n\\(\\mu_{0}\\) is 20kg, and our degrees of freedom is \\(n-1\\), which is \\(1333-1 = 1332\\).\n\n\n\n\n\n\nQuestion. Calculate the standardised statistic \\(t\\), using $ to access the numbers you just calculated above.\n\n\n\n\nt_obs &lt;- (dogstats$xbar - 20) / (dogstats$s / sqrt(dogstats$n))\nt_obs\n\n[1] 2.202048\n\n\n\n\n\n\n\n\nQuestion. Calculate the p-value using pt().\n\nOur degrees of freedom are \\(n-1\\)\n\nRemember that the total area under a probability curve is equal to 1. pt() gives us the area to the left, but we want the area in the smaller tail (if \\(\\bar{x}\\) is greater than \\(\\mu_{0}\\), we want the area to the right of \\(t_{obs}\\).\nIs our hypothesis one- or two-sided? If it is two-sided, what do we need to do to get our p-value?\n\n\n\n\n\nOur sample statistic (\\(\\bar{x}\\) = 20.38kg) is greater than the hypothesised mean (\\(\\mu_{0}\\) = 20kg), so we want the area to the right.\nReminder: For a probability distribution, the area under the curve to the right of x is 1 minus the area to the left. This is equivalent to saying that the probability of observing a value greater than x is 1 minus the probability of observing a value less than x.\n\np_righttail = 1 - pt(t_obs, df = 1332)\np_righttail\n\n[1] 0.01391638\n\n\n\n\n\n\n\n\nQuestion. Finally, use the t.test() function.\nCheck that the results match the ones you just calculated.\n\n\n\n\nt.test(x = dogs$weight_kg, mu = 20, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  dogs$weight_kg\nt = 2.202, df = 1332, p-value = 0.01392\nalternative hypothesis: true mean is greater than 20\n95 percent confidence interval:\n 20.09496      Inf\nsample estimates:\nmean of x \n 20.37604 \n\n\n\n\n\n\n\n\nQuestion. Compute the effect size\n\nby hand;\nand by using the cohens_d() function from the effectsize package that was shown in the lectures.\n\n\n\n\n\nD &lt;- (dogstats$xbar - 20) / dogstats$s\nD\n\n[1] 0.06031312\n\n\n\nlibrary(effectsize)\ncohens_d(dogs$weight_kg, mu = 20)\n\nCohen's d |       95% CI\n------------------------\n0.06      | [0.01, 0.11]\n\n- Deviation from a difference of 20.\n\n\nAs you can see, we have a Cohen’D of 0.06, which indicates a small/negligible effect size.\n\n6.2 Cat weights!\n\n\n\n\n\n\nQuestion. Without looking at the data (and without googling either), do you think that the average weight of a pet cat is more than/less than/equal to 4.5kg?\nWrite out your null and alternative hypotheses, and conduct the appropriate test. If the result is significant, don’t forget to compute the effect size.\n\n\n\nMy own hypotheses are the following. Perhaps yours are different if your belief is that they weigh less than 4.5kg.\n\\[H_0 : \\mu = 4.5\\] \\[H_1 : \\mu &gt; 4.5\\]\nThe data:\n\ncats &lt;- pets %&gt;% \n  filter(species == \"Cat\", !is.na(weight_kg))\n\nShapiro-Wilk normality test:\n\nshapiro.test(cats$weight_kg)\n\n\n    Shapiro-Wilk normality test\n\ndata:  cats$weight_kg\nW = 0.99825, p-value = 0.7611\n\n\nQQ-plot:\n\nqqnorm(cats$weight_kg)\n\n\n\n\n\n\n\nPerform the t-test:\n\nt.test(cats$weight_kg, mu=4.5, alternative=\"greater\")\n\n\n    One Sample t-test\n\ndata:  cats$weight_kg\nt = -1.2444, df = 649, p-value = 0.8931\nalternative hypothesis: true mean is greater than 4.5\n95 percent confidence interval:\n 4.463972      Inf\nsample estimates:\nmean of x \n 4.484496"
  },
  {
    "objectID": "rd2_06.html#glossary",
    "href": "rd2_06.html#glossary",
    "title": "One sample t-test",
    "section": "\n7 Glossary",
    "text": "7 Glossary\n\n\nPopulation. The entire collection of units of interest.\n\nSample. A subset of the entire population.\n\nDegrees of freedom. number of independent observations in a set of data, (\\(n-1\\))\n\nSimple random sample (SRS). Every member of a population has an equal chance of being selected to be in the sample.\n\nAssumptions Requirements of the data in order to ensure that our test is appropriate. Violation of assumptions changes the conclusion of the research and interpretation of the results.\n\nShapiro-Wilks Tests whether sample is drawn from a population which is normally distributed.\n\n\nQQplot/Quantile-Quantile plot Displays the theoretical quantiles of a normal distribution against the sample quantiles. If the data points fall on the diagonal line, the sample is normally distributed."
  },
  {
    "objectID": "rd2_06.html#footnotes",
    "href": "rd2_06.html#footnotes",
    "title": "One sample t-test",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/degrees-of-freedom/↩︎\nShoemaker, A. L. (1996). What’s Normal: Temperature, Gender and Heartrate. Journal of Statistics Education, 4(2), 4.↩︎"
  },
  {
    "objectID": "rd2_04.html",
    "href": "rd2_04.html",
    "title": "Connecting hypothesis testing and confidence intervals",
    "section": "",
    "text": "Consider the two-sided hypothesis testing case\n\\[H_0 : \\mu = \\mu_0\\] \\[H_1 : \\mu \\neq \\mu_0\\]\nWhere the test statistic used in order to test the above claim is:\n\\[\nt = \\frac{\\bar x - \\mu_0}{s / \\sqrt{n}}\n\\]\nAt the 5% significance level:\n\nwe reject the null hypothesis \\(H_0\\) whenever the observed t-statistic lies beyond the critical values:\n\n\\[t \\leq -t^* \\qquad \\text{or} \\qquad t \\geq +t^*\\]\n\n\n\n\n\n\n\n\n\nwe do not reject the null hypothesis \\(H_0\\) whenever the observed t-statistic lies within the critical values:\n\n\\[-t^* &lt; t &lt; +t^*\\]"
  },
  {
    "objectID": "rd2_04.html#hypothesis-testing",
    "href": "rd2_04.html#hypothesis-testing",
    "title": "Connecting hypothesis testing and confidence intervals",
    "section": "",
    "text": "Consider the two-sided hypothesis testing case\n\\[H_0 : \\mu = \\mu_0\\] \\[H_1 : \\mu \\neq \\mu_0\\]\nWhere the test statistic used in order to test the above claim is:\n\\[\nt = \\frac{\\bar x - \\mu_0}{s / \\sqrt{n}}\n\\]\nAt the 5% significance level:\n\nwe reject the null hypothesis \\(H_0\\) whenever the observed t-statistic lies beyond the critical values:\n\n\\[t \\leq -t^* \\qquad \\text{or} \\qquad t \\geq +t^*\\]\n\n\n\n\n\n\n\n\n\nwe do not reject the null hypothesis \\(H_0\\) whenever the observed t-statistic lies within the critical values:\n\n\\[-t^* &lt; t &lt; +t^*\\]"
  },
  {
    "objectID": "rd2_04.html#confidence-interval",
    "href": "rd2_04.html#confidence-interval",
    "title": "Connecting hypothesis testing and confidence intervals",
    "section": "\n2 Confidence interval",
    "text": "2 Confidence interval\nA 95% confidence interval for the population mean is given by\n\\[\\left[ \\bar x - t^* \\times \\frac{s}{\\sqrt n}, \\ \\ \\bar x + t^* \\times \\frac{s}{\\sqrt n} \\right]\\]\nThis is often written as\n\\[\n\\bar x \\pm t^* \\times \\frac{s}{\\sqrt n}\n\\]\nwhere \\(\\pm t^*\\) are the quantiles of a t-distribution jointly cutting an overall probability of \\(\\alpha\\) in the tails."
  },
  {
    "objectID": "rd2_04.html#from-ht-to-ci",
    "href": "rd2_04.html#from-ht-to-ci",
    "title": "Connecting hypothesis testing and confidence intervals",
    "section": "\n3 From HT to CI",
    "text": "3 From HT to CI\nIn the hypothesis test, we do not reject the null hypothesis at the 5% significance level whenever \\(\\mu_0\\) lies inside of the 95% CI:\n\\[\n\\begin{aligned}\n\\textbf{Do not reject } H_0 : \\mu = \\mu_0 \\textbf{ if} \\\\ \\quad \\\\\n-t^* &&lt; t &lt; +t^* \\\\\n-t^* &&lt; \\frac{\\bar x - \\mu_0}{\\frac{s}{\\sqrt n}} &lt; +t^* \\\\\n-t^* \\times \\frac{s}{\\sqrt n} &&lt; \\bar x - \\mu_0 &lt; +t^* \\times \\frac{s}{\\sqrt n} \\\\\n-\\bar x -t^* \\times \\frac{s}{\\sqrt n} &&lt; - \\mu_0 &lt; -\\bar x +t^* \\times \\frac{s}{\\sqrt n} \\\\\n\\bar x + t^* \\times \\frac{s}{\\sqrt n} &&gt; \\mu_0 &gt; \\bar x - t^* \\times \\frac{s}{\\sqrt n} \\\\\n\\bar x - t^* \\times \\frac{s}{\\sqrt n} &&lt; \\mu_0 &lt; \\bar x + t^* \\times \\frac{s}{\\sqrt n} \\\\\n\\mu_0 \\text{ inside of } &\\left[ \\bar x - t^* \\times \\frac{s}{\\sqrt n}, \\ \\ \\bar x + t^* \\times \\frac{s}{\\sqrt n} \\right] \\\\\n\\mu_0 \\text{ inside of } &\\text{95\\% CI}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rd2_02.html",
    "href": "rd2_02.html",
    "title": "Hypothesis testing: p-values",
    "section": "",
    "text": "The data for the entire population is often not available. However, researchers typically want to answer questions about population characteristics.\nBy characteristics of a population we mean population parameters, i.e. numerical summaries. Examples are the mean, the standard deviation, a proportion, etc.\nLast week we learned how to provide an estimate of the population mean starting from a random sample, as well as a measure of the precision of our estimate.\n\nThe sample mean \\(\\bar x\\) is the estimate of the unknown population mean.\nThe standard error of the mean \\(s / \\sqrt{n}\\) provides the reader with a measure of precision in our estimate, or better, the size of a typical estimation error.\n\nThis week we will learn how to test a claim about the population mean, starting from sample data.\n\nIn statistics, a hypothesis is a claim, in the form of a precise mathematical statement, about the value of a population parameter.\nExamples:\n\n\\(\\mu = 100\\)\nwhere \\(\\mu\\) is the population mean IQ score.\n\\(\\sigma = 15\\)\nwhere \\(\\sigma\\) is the standard deviation of IQ scores in the population.\n\nNote:\n\nA hypothesis is a claim about a population parameter and not about a sample statistic.\nWe can compute the value of the sample statistic, so there is no need to perform a test on what its value might be. We can simply look at the value, unlike population parameters. If your sample has a mean \\(\\bar x = 2\\), would it make sense to ask whether the sample mean is 0? No, clearly the sample mean is 2 which is different from 0.\nWe make hypotheses about things that are unknown, such as parameters.\n\nTo perform a hypothesis test we need:\n\n\nThe null hypothesis, denoted \\(H_0\\).\n\nThis is typically a skeptical reaction to a research hypothesis.\n\n\n\nThe alternative hypothesis, denoted \\(H_1\\).\n\nThis is the claim we seek evidence for.\n\n\n\nThe test statistic. This is used to measure how consistent the data are with the null hypothesis. For testing a mean we use the t-statistic, denoted \\(t\\).\n\nThe t-statistic measures measures how many SEs away from the hypothesised mean is the observed sample mean. That is, it measures how much the sample data differ from what expected when the null hypothesis is true.\n\n\n\nThe significance level, denoted \\(\\alpha\\)\n\nThe significance level is a cutoff chosen by the researcher (you!). Typical values are 0.10, 0.05, or 0.01.\nA value of 0.05 would mean that you would obtain such result if \\(H_0\\) is true only in 1 out of 20 samples (or, equivalently, 5 out of 100).\n\n\n\nThe p-value\n\nThe probability of obtaining a test statistic at least as extreme as the observed test statistic, if the null hypothesis is true.\nIf the p-value \\(\\leq \\alpha\\), we reject \\(H_0\\) as the data provide enough evidence (at the chosen \\(\\alpha\\) level) against \\(H_0\\) and in favour of \\(H_1\\)\n\nIf the p-value \\(&gt; \\alpha\\), we fail to reject \\(H_0\\) as the data do not provide sufficient evidence against \\(H_0\\) and in favour of \\(H_1\\)\n\nIf \\(H_1 : \\mu &gt; \\mu_0\\), compute the p-value by finding \\(P(T &gt; t)\\)\n\nIf \\(H_1 : \\mu &lt; \\mu_0\\), compute the p-value by finding \\(P(T &lt; t)\\)\n\nIf \\(H_1 : \\mu \\neq \\mu_0\\), compute the p-value by finding \\(P(T &lt; - |t|) + P(T &gt; + |t|)\\) or, equivalently, \\(2 \\times P(T &gt; |t|)\\)."
  },
  {
    "objectID": "rd2_02.html#hypothesis-testing",
    "href": "rd2_02.html#hypothesis-testing",
    "title": "Hypothesis testing: p-values",
    "section": "",
    "text": "The data for the entire population is often not available. However, researchers typically want to answer questions about population characteristics.\nBy characteristics of a population we mean population parameters, i.e. numerical summaries. Examples are the mean, the standard deviation, a proportion, etc.\nLast week we learned how to provide an estimate of the population mean starting from a random sample, as well as a measure of the precision of our estimate.\n\nThe sample mean \\(\\bar x\\) is the estimate of the unknown population mean.\nThe standard error of the mean \\(s / \\sqrt{n}\\) provides the reader with a measure of precision in our estimate, or better, the size of a typical estimation error.\n\nThis week we will learn how to test a claim about the population mean, starting from sample data.\n\nIn statistics, a hypothesis is a claim, in the form of a precise mathematical statement, about the value of a population parameter.\nExamples:\n\n\\(\\mu = 100\\)\nwhere \\(\\mu\\) is the population mean IQ score.\n\\(\\sigma = 15\\)\nwhere \\(\\sigma\\) is the standard deviation of IQ scores in the population.\n\nNote:\n\nA hypothesis is a claim about a population parameter and not about a sample statistic.\nWe can compute the value of the sample statistic, so there is no need to perform a test on what its value might be. We can simply look at the value, unlike population parameters. If your sample has a mean \\(\\bar x = 2\\), would it make sense to ask whether the sample mean is 0? No, clearly the sample mean is 2 which is different from 0.\nWe make hypotheses about things that are unknown, such as parameters.\n\nTo perform a hypothesis test we need:\n\n\nThe null hypothesis, denoted \\(H_0\\).\n\nThis is typically a skeptical reaction to a research hypothesis.\n\n\n\nThe alternative hypothesis, denoted \\(H_1\\).\n\nThis is the claim we seek evidence for.\n\n\n\nThe test statistic. This is used to measure how consistent the data are with the null hypothesis. For testing a mean we use the t-statistic, denoted \\(t\\).\n\nThe t-statistic measures measures how many SEs away from the hypothesised mean is the observed sample mean. That is, it measures how much the sample data differ from what expected when the null hypothesis is true.\n\n\n\nThe significance level, denoted \\(\\alpha\\)\n\nThe significance level is a cutoff chosen by the researcher (you!). Typical values are 0.10, 0.05, or 0.01.\nA value of 0.05 would mean that you would obtain such result if \\(H_0\\) is true only in 1 out of 20 samples (or, equivalently, 5 out of 100).\n\n\n\nThe p-value\n\nThe probability of obtaining a test statistic at least as extreme as the observed test statistic, if the null hypothesis is true.\nIf the p-value \\(\\leq \\alpha\\), we reject \\(H_0\\) as the data provide enough evidence (at the chosen \\(\\alpha\\) level) against \\(H_0\\) and in favour of \\(H_1\\)\n\nIf the p-value \\(&gt; \\alpha\\), we fail to reject \\(H_0\\) as the data do not provide sufficient evidence against \\(H_0\\) and in favour of \\(H_1\\)\n\nIf \\(H_1 : \\mu &gt; \\mu_0\\), compute the p-value by finding \\(P(T &gt; t)\\)\n\nIf \\(H_1 : \\mu &lt; \\mu_0\\), compute the p-value by finding \\(P(T &lt; t)\\)\n\nIf \\(H_1 : \\mu \\neq \\mu_0\\), compute the p-value by finding \\(P(T &lt; - |t|) + P(T &gt; + |t|)\\) or, equivalently, \\(2 \\times P(T &gt; |t|)\\)."
  },
  {
    "objectID": "rd2_02.html#example",
    "href": "rd2_02.html#example",
    "title": "Hypothesis testing: p-values",
    "section": "\n2 Example",
    "text": "2 Example\n\n2.1 Imaginary case\nSuppose you have an imaginary toy population of 5 individuals, on which you collect a score, and the mean score is actually equal to 10.\n\npop &lt;- c(6, 8, 10, 12, 14)\nmu &lt;- mean(pop)\nmu\n\n[1] 10\n\n\nIf we could only afford to sample \\(n = 2\\) individuals, and take all possible random samples, what would all the possible sample means look like?\n\n\n\n\n\n\n\nsample_id\n      sample\n      xbar\n    \n\n\n1\n(8, 6)\n7\n\n\n2\n(6, 8)\n7\n\n\n3\n(10, 6)\n8\n\n\n4\n(6, 10)\n8\n\n\n5\n(12, 6)\n9\n\n\n6\n(10, 8)\n9\n\n\n7\n(8, 10)\n9\n\n\n8\n(6, 12)\n9\n\n\n9\n(14, 6)\n10\n\n\n10\n(12, 8)\n10\n\n\n11\n(8, 12)\n10\n\n\n12\n(6, 14)\n10\n\n\n13\n(14, 8)\n11\n\n\n14\n(12, 10)\n11\n\n\n15\n(10, 12)\n11\n\n\n16\n(8, 14)\n11\n\n\n17\n(14, 10)\n12\n\n\n18\n(10, 14)\n12\n\n\n19\n(14, 12)\n13\n\n\n20\n(12, 14)\n13\n\n\n\n\n\n\n\nAs you see above, each sample leads to a different sample mean. We can plot those means with a histogram, to find what sample means happen more often, and which sample means happen less frequently, when the population mean is in fact \\(\\mu = 10\\). Remember? I gave you a population where the mean was exactly equal to 10.\n\n\n\n\n\n\n\n\nWe notice a few things. When the population mean is \\(\\mu = 10\\):\n\nMost samples will have a sample mean which is close to the true population mean\nVery few samples will have a sample mean that is far away from the true population mean\n\nIn this case,\n\n4 samples only have a sample mean \\(\\leq\\) 8\n12 samples have a sample mean between 9 and 11\n4 samples only have a sample mean \\(\\leq\\) 12\n\nSo, if the population mean is truly \\(\\mu = 10\\), it is less likely to obtain a sample with a mean of 7, and it is more likely to obtain a sample with a mean of 9, 10, or 11 for example.\nWe can also add a density above:\n\n\n\n\n\n\n\n\nThe plot above shows us the possible values that the sample means can take, assuming that the population mean is equal to some value (in this case assuming \\(\\mu = 10\\))\n\nKey question\nIf you obtained a sample with a mean of 20, would you find this consistent with a population having a mean of 10, or would you start doubting this and perhaps find it more likely that the population mean was different from 10?\n\nIf your sample had a mean of 20, when in fact this is a value you don’t expect to happen that often in the above distribution, we would find this a surprising result, and we would doubt the population has a mean of 10.\nNow we must go back to reality and realise that we cannot really take all possible samples from a population. We can only afford one and we must work with that single sample we have.\n\n2.2 Real life: One sample only\nSuppose your sample is (6, 12).\nLet’s compute the observed sample mean:\n\nx &lt;- c(6, 12)\nxbar &lt;- mean(x)\nxbar\n\n[1] 9\n\n\nWe wish to test whether the population the sample came from has a mean different from 10.\n\\[H_0: \\mu = 10\\] \\[H_1: \\mu \\neq 10\\]\nKey question:\nThe mean in our sample is 9. Could a sample mean of 9 easily come from a population that has a mean of 10? Or is it very unlikely for a population with mean 10 to give rise to a sample with mean 9?\n\n2.3 t-distribution\nInstead of working with the sample means directly, we work with the standardised sample means, using the t-statistic to standardise them. (Call it t-score if you prefer, to remind you of the z-score). The formula is:\n\\[\nt = \\frac{\\bar x - \\mu_0}{s / \\sqrt n}\n\\]\nFirst let’s compute the observed value of the t-statistic, which uses the mean from the observed sample. Furthermore, remember that in our case \\(H_0 : \\mu = 10\\).\n\nxbar &lt;- mean(x)\nxbar\n\n[1] 9\n\ns &lt;- sd(x)\nn &lt;- 2\nSE &lt;- s / sqrt(n)\nSE\n\n[1] 3\n\ntobs &lt;- (xbar - 10) / SE\ntobs\n\n[1] -0.3333333\n\n\nSo:\n\\[\nt = \\frac{9 - 10}{3} = -0.33\n\\]\nWe know that the t-statistic follows a \\(t(n-1)\\) distribution. So, in our case a \\(t(1)\\) distribution.\n\n\n\n\n\n\nVisualise the t-statistics\n\n\n\n\n\nWe can use the previous histogram of sample means and, instead of plotting the actual means, we can plot the t-score of each mean from 10. This will show how far is each sample mean from the hypothesised value (10), in SE units:\n\n\n\n\n\n\n\n\nLet’s add the observed t-score on the plot\n\n\n\n\n\n\n\n\n\n\n\nHow likely is it to obtain a t-score at least as extreme as -0.33?\nIn this case \\(H_1\\) has the \\(\\neq\\) symbol. Something is different from 10 when it’s either much larger or much smaller. So we find the probability of t-statistics that are either more distant from the hypothesised value of 10 on the left tail, but also on the right tail.\nHow likely is it to obtain a t-score either lower than -0.33 \\((=-|t|)\\) or larger than 0.33 \\((= +|t|)\\)?\n\npvalue &lt;- pt(-abs(tobs), 1, lower.tail = TRUE) + \n          pt(+abs(tobs), 1, lower.tail = FALSE)\npvalue\n\n[1] 0.7951672\n\n\nComparing the p-value with \\(\\alpha\\) = 0.05, we find that the p-value is larger than the significance level and as such we do not reject \\(H_0\\).\nThe sample data do not provide sufficient evidence to reject the null hypothesis that the sample \\((6, 12)\\) came from a population with mean 10.\n\n2.4 Test your knowledge\nWhat about if the sample you collected was (172, 194)? Could this come from a population having mean = 10?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nx &lt;- c(172, 194)\nxbar &lt;- mean(x)\ns &lt;- sd(x)\nn &lt;- 2\nSE &lt;- s / sqrt(n)\nSE\n\n[1] 11\n\ntobs &lt;- (xbar - 10) / SE\ntobs\n\n[1] 15.72727\n\npt(-abs(tobs), df = 1) + (1 - pt(+abs(tobs), df = 1))\n\n[1] 0.0404243\n\n\nor\n\n2 * pt(abs(tobs), df = 1, lower.tail = FALSE)\n\n[1] 0.0404243\n\n\nThe p-value is \\(\\leq 0.05\\) so we reject \\(H_0\\)\nAt the 5% significance level, the sample data provide strong evidence against the null hypothesis that the sample \\((172, 194)\\) came from a population with a mean of 10, and in favour of the alternative hypothesis that the sample came from a population with a mean different from 10."
  },
  {
    "objectID": "rd2_02.html#summary",
    "href": "rd2_02.html#summary",
    "title": "Hypothesis testing: p-values",
    "section": "\n3 Summary",
    "text": "3 Summary\n\nWe have learned to assess how much evidence the sample data bring against the null hypothesis and in favour of the alternative hypothesis.\nThe null hypothesis, denoted \\(H_0\\), is a claim about a population parameter that is initially assumed to be true. It typically represents “no effect” or “no difference between groups”.\nThe alternative hypothesis, denoted \\(H_1\\), is the claim we seek evidence for.\n\nWe performed a hypothesis test against \\(H_0\\) (and in favour of \\(H_1\\)) following these steps:\n\nFormally state your null and alternative hypotheses using precise symbols\nConsider the distribution of the t-statistics when \\(H_0\\) is true\nCompute the observed value of the t-statistic in our sample\nObtain the p-value: the probability, computed assuming that \\(H_0\\) is true, of obtaining a t-statistic at least as extreme as that observed. Note: as extreme means “in the direction specified by \\(H_1\\)”."
  },
  {
    "objectID": "rd2_02.html#worked-example",
    "href": "rd2_02.html#worked-example",
    "title": "Hypothesis testing: p-values",
    "section": "\n4 Worked example",
    "text": "4 Worked example\nA 2011 study by Courchesne et al.1 examined brain tissue of seven autistic male children between the ages of 2 and 16. The mean number of neurons in the prefrontal cortex in non-autistic male children of the same age is about 1.15 billion. The prefrontal cortex is the part of the brain most disrupted in autism, as it deals with language and social communication.\nIn the exercises you will perform a test of significance to assess whether this sample provides evidence that autistic male children have more neurons (on average) in the prefrontal cortex than non-autistic children.\n\n\n\n\n\n\nData codebook\n\n\n\n\n\nDownload link\nThe data can be found at this address: https://uoepsy.github.io/data/NeuronCounts.csv\nPreview\n\n\n\n\n\n\n\nCase\n      Age\n      PFC_NC\n    \n\n\n1\n2\n2.42\n\n\n2\n3\n1.80\n\n\n3\n3\n2.21\n\n\n4\n4\n2.18\n\n\n5\n7\n1.28\n\n\n6\n8\n1.59\n\n\n7\n16\n2.09\n\n\n\n\n\n\nCodebook\n\nThe first column, Case, is an anonymised index used to identify each child.\nThe second column, Age, records the age of each child.\nThe last column, PFC_NC, contains the prefrontal cortex neuron counts (in billions).\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1. Read the data into R.\n\n\n\n\nlibrary(tidyverse)\nautism &lt;- read_csv('https://uoepsy.github.io/data/NeuronCounts.csv')\nautism\n\n# A tibble: 7 × 3\n   Case   Age PFC_NC\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1     2   2.42\n2     2     3   1.8 \n3     3     3   2.21\n4     4     4   2.18\n5     5     7   1.28\n6     6     8   1.59\n7     7    16   2.09\n\n\n\n\n\n\n\n\nQuestion 2. Compute and interpret a table of descriptive statistics.\n\n\n\n\nstats &lt;- autism %&gt;%\n    summarise(\n        SampleSize = n(),\n        M_Age = mean(Age), SD_Age = sd(Age),\n        M_PFC_NC = mean(PFC_NC), SD_PFC_NC = sd(PFC_NC)\n    ) %&gt;%\n    round(2)\nstats\n\n# A tibble: 1 × 5\n  SampleSize M_Age SD_Age M_PFC_NC SD_PFC_NC\n       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1          7  6.14   4.88     1.94       0.4\n\n\nIn the sample of seven autistic children, the mean age was 6.14 years, with a SD of 4.88 years, and the mean number of neurons in the prefrontal cortex was 1.94 billion with a standard deviation of 0.40 billion.\n\n\n\n\n\n\nQuestion 3. State the null and alternative hypotheses.\n\n\n\nWe wish to test the claim that the mean number of neurons in the prefrontal cortex is larger than 1.15 billion (the value for non-autistic male children). At this is our research hypothesis, we must place this into the alternative hypothesis.\nRemember: A hypothesis test looks for evidence against a null hypothesis and in favour of the alternative.\nThe null hypothesis is typically a skeptical reaction to a research hypothesis. In our case, a skeptic would say that there is no difference in mean number of neurons, so the mean number of neurons for autistic children will be equal to that of non-autistic children (1.15).\nThe answer is then:\n\\[H_0 : \\mu = 1.15\\] \\[H_1 : \\mu &gt; 1.15\\]\nwhere \\(\\mu\\) is the mean number of neurons (in billions) in the prefrontal cortex for all autistic male children.\n\n\n\n\n\n\nQuestion 4. Compute the value of the t-statistic from the sample mean number of neurons.\n\n\n\nRecall the formula for the t-statistic:\n\\[\nt = \\frac{\\bar x - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\]\nwhere\n\n\n\\(\\bar x\\) is the sample average number of neurons in the prefrontal cortex\n\n\\(\\mu_0\\) is the hypothesised value for the population parameter found in \\(H_0\\)\n\n\n\\(s\\) is the sample standard deviation\n\n\\(n\\) is the sample size\n\nHence, the value of the t-statistic for the observed sample is given by:\n\nxbar &lt;- mean(autism$PFC_NC)\nxbar\n\n[1] 1.938571\n\ns &lt;- sd(autism$PFC_NC)\ns\n\n[1] 0.4002261\n\nn &lt;- nrow(autism)\nn\n\n[1] 7\n\nmu0 &lt;- 1.15\n\ntobs &lt;- (xbar - mu0) / (s / sqrt(n))\ntobs\n\n[1] 5.212963\n\n\nThe sample mean neuron count, 1.94 billion, is 5.21 standard errors larger than the hypothesised value.\n\n\n\n\n\n\nQuestion 5. Identify the null distribution.\n\n\n\nWhen \\(H_0: \\mu = 1.15\\) is true, the t values will follow a \\(t(6)\\) distribution.\n\n\n\n\n\n\nQuestion 6. Compute the p-value for the test.\n\n\n\nThe alternative hypothesis is \\(H_1 : \\mu &gt; 1.15\\). This involves the “greater than sign”, so we compute the p-value by finding the probability of observing a t-statistic at least as large as the observed t value:\n\n# P(T &gt;= tobs)\npt(tobs, df = n-1, lower.tail = FALSE)\n\n[1] 0.0009948463\n\n\nor\n\n# P(T &gt;= tobs) = 1 - P(T &lt;= tobs)\n1 - pt(tobs, df = n-1)\n\n[1] 0.0009948463\n\n\n\n\n\n\n\n\nQuestion 7. Using a 5% significance level, i.e. \\(\\alpha = 0.05\\), report whether or not you reject the null hypothesis.\n\n\n\nThe p-value is smaller than \\(\\alpha = 0.05\\), so we reject \\(H_0\\).\n\n\n\n\n\n\nQuestion 8. Write up your results in the context of the research question.\n\n\n\nThe estimated mean number of neurons in the prefrontal cortex of male autistic children is 1.94 billion, with a standard error of 0.15 billion.\nAt the 5% significance level, we performed a one-sided test of significance against the null hypothesis that the mean number of neurons in the prefrontal cortex of all male autistic children was equal to that of all non-autistic male children. The sample results indicate that there is very strong evidence that the mean number of neurons in the prefrontal cortex may be larger for autistic compared to non-autistic male children: \\(t(6) = 5.21, p &lt; .001\\), one-sided."
  },
  {
    "objectID": "rd2_02.html#footnotes",
    "href": "rd2_02.html#footnotes",
    "title": "Hypothesis testing: p-values",
    "section": "Footnotes",
    "text": "Footnotes\n\nCourchesne, E., et al., “Neuron Number and Size in Prefrontal Cortex of Children with Autism,” Journal of the American Medical Association, November 2011;306(18): 2001–2010.↩︎"
  },
  {
    "objectID": "rd1_11.html",
    "href": "rd1_11.html",
    "title": "Sampling distributions",
    "section": "",
    "text": "This section contains essential terminology and functions that are needed to complete the exercises provided.\n\n\n\n\n\n\nPopulation vs sample\n\n\n\n\n\nTypically:\n\nWe do not have data for the entire population. There are different possible reasons:\n\nIt’s too expensive to collect them\nBecause of deadlines, there is not sufficient time\nIt’s not possible to reach the entire population\n\n\nIt’s much easier to obtain data by taking a sample from that population of interest and measuring only the units chosen in the sample.\n\nNote that units do not necessarily have to be individuals, but they could be schools, companies, etc.\n\n\nWe wish to use the sample data to:\n\nInvestigate a claim about the whole population.\nTest an hypothesis about the entire population.\nAnswer a question about the whole population.\n\n\n\nThe process of using information from a sample (the part) in order to draw conclusions about the entire population (the whole) is known as statistical inference.\n\n\n\n\n\n\n\n\n\nParameters vs statistics\n\n\n\n\n\nAs we do not typically have the data for the entire population, the population is considered as “unknown” with respect the data that we wish to investigate. We could shortly say that the population data are unknown.\nFor example, if we are interested in the average IQ in the population, we don’t have the resources to go to every single individual and test their IQ score. So, in this respect, the population IQ scores are unknown.\nAs a consequence of this, any numerical summary of the population data is also unknown. In the above example, the population mean IQ score is unknown and needs to be estimated.\nSample data are more readily available or feasible to collect. Imagine collecting a sample of 50 individuals, chosen at random from the population, and testing each to obtain their IQ score. If you performed the random experiment, you would then obtain a sequence of 50 IQ measurements.\nAt the same time, it is also feasible to compute any numerical summary of the sample data. For example, you can compute the mean IQ score for those 50 individuals in the sample.\n\n\n\n\n\n\nWe typically use this terminology to distinguish a numerical summary when computed in the population (unknown) or in the collected sample (known).\n\nA parameter is a numerical summary of a population.\nA statistic is a numerical summary of the sample.\n\nA statistic is often used as a “best guess” or “estimate” for the unknown parameter. That is, we use the (sample) statistic to estimate a (population) parameter.\n\n\n\nIn the above example, the population mean IQ score is the parameter of interest, while the sample mean IQ score is the statistic.\nIt is typical to use special notation to distinguish between parameters and statistics in order to convey with a single letter: (1) which numerical summary is being computed, and (2) if it is computed on the population or on the sample data.\nThe following table summarizes standard notation for some population parameters, typically unknown, and the corresponding estimates computed on a sample.\n\nNotation for common parameters and statistics.\n\n\n\n\n\n\nNumerical summary\nPopulation parameter\nSample statistic\n\n\n\nMean\n\\(\\mu\\)\n\n\\(\\bar{x}\\) or \\(\\hat{\\mu}\\)\n\n\n\nStandard deviation\n\\(\\sigma\\)\n\n\\(s\\) or \\(\\hat{\\sigma}\\)\n\n\n\nProportion\n\\(p\\)\n\\(\\hat{p}\\)\n\n\n\nThe Greek letter \\(\\mu\\) (mu) represents the population mean (parameter), while \\(\\bar{x}\\) (x-bar) or \\(\\hat{\\mu}\\) (mu-hat) is the mean computed from the sample data (sample statistic).\nThe Greek letter \\(\\sigma\\) (sigma) represents the population standard deviation (parameter), while \\(s\\) or \\(\\hat{\\sigma}\\) (sigma-hat) is the standard deviation computed from the sample data (sample statistic).\nThe Greek letter \\(p\\) represents the population proportion (parameter), while \\(\\hat{p}\\) (p-hat) is the proportion computed from the sample data (sample statistic).\n\nThe process of sampling \\(n\\) people at random from the population is a random experiment, as it leads to an uncertain outcome. Hence, a statistic is a numerical summary of a random experiment and for this reason it is a random variable (random number).\nBefore actually performing the random experiment and picking individuals for the sample, the sample mean is a random number, as its value is uncertain. Once we actually perform the random experiment and measure the individuals in the sample, we have an observed value for the sample mean, i.e. a known number.\nWe will then distinguish between:\n\na statistic - a random variable (random number) that is uncertain because it involves a random experiment.\nan observed statistic - the actual number that is computed once the sample data have been collected by performing the chance experiment.\n\nNotation-wise we distinguish between a statistic (random variable) and an observed statistic (observed number) by using, respectively, an uppercase letter or a lowercase letter.\nUppercase letters refer to random variables. Recall that a random variable represents a well defined number, but whose value is uncertain as it involves an experiment of chance in order to reach to an actual value.\nFor example, \\(\\bar X\\) = “mean IQ score in a random sample of 50 individuals” is a random variable. It is clearly defined in operational terms by: (1) take a sample of 50 individuals at random, (2) measure their IQ score, and (3) compute the mean of their 50 IQ scores. However, the actual value that we can obtain is uncertain as it is the result of an experiment of chance involving random sampling from a population. There are many possible values we could obtain, so we are not sure which one we will see.\nAnother example: \\(X\\) = “number of heads in 10 flips of a coin”. This is also a clearly defined experiment: (1) flip a coin 10 times and (2) count the number of heads. However, the result is a random number, which is uncertain and will be unknown until we actually perform the experiment and flip a coin 10 times.\nLowercase letters refer to observed (i.e., realised) values of the random variable. An observed value of a random variable is just a number.\nFor example, once we actually collect 50 individuals and measure their IQ scores, we can sum those 50 numbers and divide the sum by 50 to obtain the observed sample mean. Say the sample mean IQ score is 102.3, we would write \\(\\bar x = 102.3\\).\nIn short, we would use for a sample mean:\n\nStatistic (sample mean): an uppercase letter before the value is actually known: \\(\\bar X\\)\n\nObserved statistic (observed sample mean): a lowercase letter once the value is actually known: \\(\\bar x\\)\n\n\n\n\n\n\n\n\n\n\n\nAvoiding bias due to sampling\n\n\n\n\n\nSampling bias occurs when the method used to select which units enter the sample causes the sample to not be a good representation of the population.\nIf sampling bias exists, we cannot generalise our sample conclusions to the population.\n\n\n\n\n\n\n\n\nTo be able to draw conclusions about the population, we need a representative sample. The key in choosing a representative sample is random sampling. Imagine an urn with tickets, where each ticket has the name of each population unit. Random sampling would involve mixing the urn and blindly drawing out some tickets from the urn. Random sampling is a strategy to avoid sampling bias.\n\n\n\n\n\n\nSimple random sampling\nWhen we select the units entering the sample via simple random sampling, each unit in the population has an equal chance of being selected, meaning that we avoid sampling bias.\nWhen instead some units have a higher chance of entering the same, we have misrepresentation of the population and sampling bias.\n\n\n\nIn general, we have bias when the method of collecting data causes the data to inaccurately reflect the population.\n\n\n\n\n\n\n\n\n\nSampling distribution\n\n\n\n\n\nThe natural gestation period (in days) for human births is normally distributed in the population with mean 266 days and standard deviation 16 days. This is a special case which rarely happens in practice: we actually know what the distribution looks like in the population.\nWe will use this unlikely example to study how well does the sample mean estimate the population mean and, to do so, we need to know what the population mean is so that we can compare the estimate and the true value. Remember, however, that in practice the population parameter would not be known.\nWe will consider data about the gestation period of the 49,863 women who gave birth in Scotland in 2019. These can be found at the following address: https://uoepsy.github.io/data/pregnancies.csv\nFirst, we read the population data:\n\nlibrary(tidyverse)\ngest &lt;- read_csv('https://uoepsy.github.io/data/pregnancies.csv')\ndim(gest)\n\n[1] 49863     2\n\n\nThe data set contains information about 49,863 cases. For each case an identifier and the length of pregnancy Look at the top six rows of the data set (the “head”):\n\nhead(gest)\n\n# A tibble: 6 × 2\n     id gest_period\n  &lt;dbl&gt;       &lt;dbl&gt;\n1     1        256.\n2     2        269.\n3     3        253.\n4     4        292.\n5     5        271.\n6     6        253.\n\n\nWe now want to investigate how much the sample means will vary from sample to sample. To do so, we will take many samples from the population of all gestation periods, and compute for each sample the mean.\nTo do so, we will load a function which we prepared for you called rep_sample_n(). This function is used to take a sample of \\(n\\) units from the population, and it lets you repeat this process many times.\nTo get the function in your computer, run this code:\n\nsource('https://uoepsy.github.io/files/rep_sample_n.R')\n\n\n\n\n\n\n\nNOTE\nYou need to copy and paste the line\n\nsource('https://uoepsy.github.io/files/rep_sample_n.R')\n\nat the top of each file in which you want to use the rep_sample_n() function.\n\n\n\nThe function takes the following arguments:\nrep_sample_n(data, n = &lt;sample size&gt;, samples = &lt;how many samples&gt;)\n\ndata is the population\nn is the sample size\nsamples is how many samples of size \\(n\\) you want to take\n\nBefore doing anything involving random sampling, it is good practice to set the random seed. This is to ensure reproducibility of the results. Random number generation in R works by specifying a starting seed, and then numbers are generated starting from there.\nSet the random seed to any number you wish. Depending on the number, you will get the same results as me or not:\n\nset.seed(1234)\n\nObtain 10 samples of \\(n = 5\\) individuals each:\n\nsamples &lt;- rep_sample_n(gest, n = 5, samples = 10)\nsamples\n\n# A tibble: 50 × 3\n   sample    id gest_period\n    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1      1 40784        255.\n 2      1 40854        275.\n 3      1 41964        281.\n 4      1 15241        246.\n 5      1 33702        247.\n 6      2 35716        267.\n 7      2 17487        289.\n 8      2 15220        266.\n 9      2 19838        238.\n10      2  2622        281.\n# ℹ 40 more rows\n\n\nThe samples data frame contains 3 columns:\n\nsample, telling us which sample each row refers to\nid, telling us the units chosen to enter each sample\ngest_period, telling us the gestation period (in days) of each individual\n\nNote that the tibble samples has 50 rows, which is given by 5 individuals in each sample * 10 samples.\nYou can inspect the sample data in the following interactive table in which the data corresponding to each sample have been colour-coded so that you can distinguish the rows belonging to the 1st, 2nd, …, and 10th sample:\n\n\n\n\n\n\n\nNow, imagine computing the mean of the five observation in each sample. This will lead to 10 means, one for each of the 10 samples (of 5 individuals each).\n\nsample_means &lt;- samples %&gt;%\n  group_by(sample) %&gt;%\n  summarise(mean_gest = mean(gest_period))\n\nsample_means\n\n# A tibble: 10 × 2\n   sample mean_gest\n    &lt;dbl&gt;     &lt;dbl&gt;\n 1      1      261.\n 2      2      268.\n 3      3      273.\n 4      4      269.\n 5      5      261.\n 6      6      271.\n 7      7      252.\n 8      8      262.\n 9      9      258.\n10     10      267.\n\n\nAs you can see this leads to a tibble having 10 rows (one for each sample), where each row is a mean computed from the 5 individuals which were chosen to enter the sample.\nThe gestation period (in days) for the first five women sampled were\n\n255.15, 275.34, 281.04, 245.62, 247.5\n\nThis sample has a mean of \\(\\bar x\\) = 260.93 days.\nThe second sample of 5 women had gestation periods\n\n267.03, 288.74, 265.56, 238, 280.56\n\nThe second sample has a mean gestation period of \\(\\bar x\\) = 267.98 days.\nIn Figure 1 we display the individual gestation periods in each sample as dots, along with the means gestation period \\(\\bar x\\) of each sample. The position of the sample mean is given by a red vertical bar.\nWe then increased the sample size to 50 women and took 10 samples each of 50 individuals. This set of samples together with their means is also plotted in Figure 1.\n\n\n\n\nFigure 1: Gestation period (in days) of samples of individuals.\n\n\n\nTwo important points need to be made from Figure Figure 1. First, each sample (and therefore each sample mean) is different. This is due to the randomness of which individuals end up being in each sample. The sample means vary in an unpredictable way, illustrating the fact that \\(\\bar X\\) is a summary of a random experiment (randomly choosing a sample) and hence is a random variable. Secondly, as we increase the sample size from 5 to 50, there appears to be a decrease in the variability of sample means (compare the variability in the vertical bars in panel (a) and panel (b)). That is, with a larger sample size, the sample means fluctuate less and are more “consistent”.\nTo further investigate the variability of sample means, we will now generate many more sample means computed on:\n\n1,000 samples of \\(n = 5\\) women\n1,000 samples of \\(n = 50\\) women\n1,000 samples of \\(n = 500\\) women\n\nWe will also add at the end of each tibble a column specifying the sample size. In the first tibble, mutate(n = 5) creates a column called n where all values will be 5, to remind ourselves that those means were computed with samples of size \\(n = 5\\). Remember that mutate() takes a tibble and creates a new column or changes an existing one.\n\n# (a) 1,000 means from 1,000 samples of 5 women each\nsample_means_5 &lt;- rep_sample_n(gest, n = 5, samples = 1000) %&gt;%\n  group_by(sample) %&gt;%\n  summarise(mean_gest = mean(gest_period)) %&gt;%\n  mutate(n = 5)\nhead(sample_means_5)\n\n# A tibble: 6 × 3\n  sample mean_gest     n\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1      267.     5\n2      2      263.     5\n3      3      255.     5\n4      4      269.     5\n5      5      267.     5\n6      6      266.     5\n\n# (b) 1,000 means from 1,000 samples of 50 women each\nsample_means_50 &lt;- rep_sample_n(gest, n = 50, samples = 1000) %&gt;%\n  group_by(sample) %&gt;%\n  summarise(mean_gest = mean(gest_period)) %&gt;%\n  mutate(n = 50)\nhead(sample_means_50)\n\n# A tibble: 6 × 3\n  sample mean_gest     n\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1      263.    50\n2      2      262.    50\n3      3      269.    50\n4      4      266.    50\n5      5      267.    50\n6      6      268.    50\n\n# (c) 1,000 means from 1,000 samples of 500 women each\nsample_means_500 &lt;- rep_sample_n(gest, n = 500, samples = 1000) %&gt;%\n  group_by(sample) %&gt;%\n  summarise(mean_gest = mean(gest_period)) %&gt;%\n  mutate(n = 500)\nhead(sample_means_500)\n\n# A tibble: 6 × 3\n  sample mean_gest     n\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1      265.   500\n2      2      265.   500\n3      3      267.   500\n4      4      266.   500\n5      5      265.   500\n6      6      265.   500\n\n\nWe now combine the above datasets of sample means for different sample sizes into a unique tibble. The function bind_rows() takes multiple tibbles and stacks them under each other.\n\nsample_means_n &lt;- bind_rows(sample_means_5, \n                            sample_means_50, \n                            sample_means_500)\n\nWe now plot three different density histograms showing the distribution of 1,000 sample means computed from samples of size 5, 50, and 500.\nThis would correspond to creating a histogram of the “red vertical bars” from Figure Figure 1, the only difference is that we have many more samples (1,000).\n\nggplot(sample_means_n) +\n  geom_histogram(aes(mean_gest, after_stat(density)), \n                 color = 'white', binwidth = 1) +\n  facet_grid(n ~ ., labeller = label_both) +\n  theme_bw() + \n  labs(x = 'Sample mean of gestation period (days)', y = 'Density')\n\n\n\nFigure 2: Density histograms of the sample means from 1,000 samples of women (\\(n\\) women per sample).\n\n\n\nEach of the density histograms above displays the distribution of the sample mean, computed on samples of the same size and from the same population.\nSuch a distribution is called the sampling distribution of the sample mean.\n\n\n\n\n\n\nSampling distribution\nThe sampling distribution of a statistic is the distribution of a sample statistic computed on many different samples of the same size from the same population.\nA sampling distribution shows how the statistic varies from sample to sample due to sampling variation.\n\n\n\n\n\n\n\n\n\n\n\n\nCentre and spread of a sampling distribution\n\n\n\n\n\nWhat is the mean and standard deviation of each histogram?\n\nsample_means_n %&gt;%\n  group_by(n) %&gt;%\n  summarise(mean_xbar = mean(mean_gest),\n            sd_xbar = sd(mean_gest))\n\n# A tibble: 3 × 3\n      n mean_xbar sd_xbar\n  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1     5      266.   7.38 \n2    50      266.   2.23 \n3   500      266.   0.734\n\n\nCompare these quantities to the population mean and standard deviation: \\(\\mu\\) = 266 and \\(\\sigma\\) = 16.1.\nRegardless of the size of the samples we were drawing (5, 50, or 500), the average of the sample means was equal to the population mean. However, the standard deviation of the sample means was smaller than the population mean. The variability in sample means also decreases as the sample size increases.\nThere is an interesting patter in the decrease, which we will now verify. It can be proved that the standard deviation of the sample mean \\(\\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{n}}\\), i.e. the population standard deviation divided by \\(\\sqrt{n}\\) with \\(n\\) being the sample size.\nObtain the population standard deviation. Remember the entire population data were called gest and in this case we are very lucky to have the data for the entire population, typically we wouldn’t have those and neither the population standard deviation.\n\nsigma &lt;- sd(gest$gest_period)\n\nNow compute add a column that compares the SD from sampling with the theory-based one:\n\nsample_means_n %&gt;%\n  group_by(n) %&gt;%\n  summarise(mean_xbar = mean(mean_gest),\n            sd_xbar = sd(mean_gest)) %&gt;%\n  mutate(sd_theory = sigma / sqrt(n))\n\n# A tibble: 3 × 4\n      n mean_xbar sd_xbar sd_theory\n  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1     5      266.   7.38      7.20 \n2    50      266.   2.23      2.28 \n3   500      266.   0.734     0.720\n\n\nThe last two columns will be closer and closer as we increase the number of different samples we take from the population (e.g. 5,000 or 10,000 or even more samples.)\nThe following result holds: \\[\n\\begin{aligned}\n\\mu_{\\bar X} &= \\mu = \\text{Population mean} \\\\\n\\sigma_{\\bar X} &= \\frac{\\sigma}{\\sqrt{n}} = \\frac{\\text{Population standard deviation}}{\\sqrt{\\text{Sample size}}}\n\\end{aligned}\n\\]\nBecause on average the sample mean (i.e. the estimate) is equal to the population mean (i.e. the parameter), the sample mean \\(\\bar X\\) is an unbiased estimator of the population mean. In other words, it does not consistently “miss” the target. (However, if your sampling method is biased, the sample mean will be biased too.)\nThe standard deviation of the sample means tells us that the variability in the sample means gets smaller smaller as the sample size increases. Because \\(\\sqrt{4} = 2\\) we halve \\(\\sigma_{\\bar X}\\) by making the sample size 4 times as large. Similarly, as \\(\\sqrt{9} = 3\\), we reduce \\(\\sigma_{\\bar X}\\) by one third by making the sample size 9 times as large.\nThe variability, or spread, of the sampling distribution shows how much the sample statistics tend to vary from sample to sample. This is key in understanding how accurate our estimate of the population parameter, based on just one sample, will be.\nRecall that the standard deviation tells us the size of a typical deviation from the mean. Here, the mean is the population parameter \\(\\mu\\), and a deviation of \\(\\bar x\\) from \\(\\mu\\) is called an estimation error. Hence, the standard deviation of the sample mean is called the standard error of the mean. This tells us the typical estimation error that we commit when we estimate a population mean with a sample mean.\n\n\n\n\n\n\nStandard error\nThe standard error of a statistic, denoted \\(SE\\), is the standard deviation of its sampling distribution.\n\n\n\n\n\n\n\n\nSo, the standard error of the mean can be either computed as the standard deviation of the sampling distribution, or using the formula \\[\nSE = \\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\n\nThe sample mean is normally distributed\n\n\n\n\n\nWe also notice that the density histograms in Figure 2 are symmetric and bell-shaped. Hence, they follow the shape of the normal curve.\n\n\n\n\n\n\n\n\nThe random variable \\(\\bar X\\) follows a normal distribution: \\[\n\\bar X \\sim N(\\mu,\\ SE)\n\\]\nWe can also compute a z-score. We have that: \\[\nZ = \\frac{\\bar X - \\mu}{SE} \\sim N(0, 1)\n\\]\nWe know that for a normally distributed random variable, approximately 95% of all values fall within two standard deviations of its mean. Thus, for approximately 95% of all samples, the sample means falls within \\(\\pm 2 SE\\) of the population mean \\(\\mu\\). Similarly, since \\(P(-3 &lt; Z &lt; 3) = 0.997\\), it is even more rare to get a sample mean which is more than three standard errors away from the population mean (only 0.3% of the times).\nThis suggests that:\n\nThe standard error \\(SE\\) is a measure of precision of \\(\\bar x\\) as an estimate of \\(\\mu\\).\nIf is a pretty safe bet to say that the true value of \\(\\mu\\) lies somewhere between \\(\\bar x - 2 SE\\) and \\(\\bar x + 2 SE\\).\nWe will doubt any hypothesis specifying that the population mean is \\(\\mu\\) when the value \\(\\mu\\) is more than \\(2 SE\\) away from the sample mean we got from our data, \\(\\bar x\\). We shall be even more suspicious when the hypothesised value \\(\\mu\\) is more than \\(3 SE\\) away from \\(\\bar x\\).\n\n\n\n\n\n\n\nCentre and shape of a sampling distribution\n\nCentre: If samples are randomly selected, the sampling distribution will be centred around the population parameter. (No bias)\nShape: For most of the statistics we consider, if the sample size is large enough, the sampling distribution will follow a normal distribution, i.e. it is symmetric and bell-shaped. (Central Limit Theorem)\n\n\n\n\nClearly, we can compute sampling distributions for other statistics too: the proportion, the standard deviation, …\nThis requires the following steps:\n\nObtaining multiple samples, all of the same size, from the same population;\nFor each sample, calculate the value of the statistic;\nPlot the distribution of the computed statistics.\n\n\n\n\n\n\n\n\n\n\nWhy sample size matters\n\n\n\n\n\nYou might be wondering: why did we take multiple samples of size \\(n\\) from the population when, in practice, we can only afford to take one?\nThis is a good question. We have taken multiple samples to show how the estimation error varies with the sample size. We saw in Figure 2, shown again below, that smaller sample sizes lead to more variable statistics, while larger sample sizes lead to more precise statistics, i.e. the estimates are more concentrated around the true parameter value.\n\n\n\n\nDensity histograms of the sample means from 5,000 samples of women (\\(n\\) women per sample).\n\n\n\nThis teaches us that, when we have to design a study, it is better to obtain just one sample with size \\(n\\) as large as we can afford.\n\n\n\n\n\n\nThink about it\n\n\n\n\n\nWhat would the sampling distribution of the mean look like if we could afford to take samples as big as the entire population, i.e. of size \\(n = N\\)?\n\n\n\n\n\n\n\n\n\nIf you can, it is best to measure the entire population.\nIf we could afford to measure the entire population, then we would find the exact value of the parameter all the time. By taking multiple samples of size equal to the entire population, every time we would obtain the population parameter exactly, so the distribution would look like a histogram with a single bar on top of the true value: we would find the true parameter with a probability of one, and the estimation error would be 0.\n\npop_means &lt;- gest %&gt;%\n  rep_sample_n(n = nrow(gest), samples = 10) %&gt;%\n  group_by(sample) %&gt;%\n  summarise(mean_gest = mean(gest_period))\npop_means\n\n# A tibble: 10 × 2\n   sample mean_gest\n    &lt;dbl&gt;     &lt;dbl&gt;\n 1      1      266.\n 2      2      266.\n 3      3      266.\n 4      4      266.\n 5      5      266.\n 6      6      266.\n 7      7      266.\n 8      8      266.\n 9      9      266.\n10     10      266.\n\n\nThe following is a dotplot of the means computed above:\n\n\n\n\n\n\n\n\n\n\n\nTo summarize:\n\nWe have high precision when the estimates are less variable, and this happens for a large sample size.\nWe have no bias when we select samples that are representative of the population, and this happens when we do random sampling. No bias means that the estimates will be centred at the true population parameter to be estimated."
  },
  {
    "objectID": "rd1_11.html#fundamentals-of-inference",
    "href": "rd1_11.html#fundamentals-of-inference",
    "title": "Sampling distributions",
    "section": "",
    "text": "This section contains essential terminology and functions that are needed to complete the exercises provided.\n\n\n\n\n\n\nPopulation vs sample\n\n\n\n\n\nTypically:\n\nWe do not have data for the entire population. There are different possible reasons:\n\nIt’s too expensive to collect them\nBecause of deadlines, there is not sufficient time\nIt’s not possible to reach the entire population\n\n\nIt’s much easier to obtain data by taking a sample from that population of interest and measuring only the units chosen in the sample.\n\nNote that units do not necessarily have to be individuals, but they could be schools, companies, etc.\n\n\nWe wish to use the sample data to:\n\nInvestigate a claim about the whole population.\nTest an hypothesis about the entire population.\nAnswer a question about the whole population.\n\n\n\nThe process of using information from a sample (the part) in order to draw conclusions about the entire population (the whole) is known as statistical inference.\n\n\n\n\n\n\n\n\n\nParameters vs statistics\n\n\n\n\n\nAs we do not typically have the data for the entire population, the population is considered as “unknown” with respect the data that we wish to investigate. We could shortly say that the population data are unknown.\nFor example, if we are interested in the average IQ in the population, we don’t have the resources to go to every single individual and test their IQ score. So, in this respect, the population IQ scores are unknown.\nAs a consequence of this, any numerical summary of the population data is also unknown. In the above example, the population mean IQ score is unknown and needs to be estimated.\nSample data are more readily available or feasible to collect. Imagine collecting a sample of 50 individuals, chosen at random from the population, and testing each to obtain their IQ score. If you performed the random experiment, you would then obtain a sequence of 50 IQ measurements.\nAt the same time, it is also feasible to compute any numerical summary of the sample data. For example, you can compute the mean IQ score for those 50 individuals in the sample.\n\n\n\n\n\n\nWe typically use this terminology to distinguish a numerical summary when computed in the population (unknown) or in the collected sample (known).\n\nA parameter is a numerical summary of a population.\nA statistic is a numerical summary of the sample.\n\nA statistic is often used as a “best guess” or “estimate” for the unknown parameter. That is, we use the (sample) statistic to estimate a (population) parameter.\n\n\n\nIn the above example, the population mean IQ score is the parameter of interest, while the sample mean IQ score is the statistic.\nIt is typical to use special notation to distinguish between parameters and statistics in order to convey with a single letter: (1) which numerical summary is being computed, and (2) if it is computed on the population or on the sample data.\nThe following table summarizes standard notation for some population parameters, typically unknown, and the corresponding estimates computed on a sample.\n\nNotation for common parameters and statistics.\n\n\n\n\n\n\nNumerical summary\nPopulation parameter\nSample statistic\n\n\n\nMean\n\\(\\mu\\)\n\n\\(\\bar{x}\\) or \\(\\hat{\\mu}\\)\n\n\n\nStandard deviation\n\\(\\sigma\\)\n\n\\(s\\) or \\(\\hat{\\sigma}\\)\n\n\n\nProportion\n\\(p\\)\n\\(\\hat{p}\\)\n\n\n\nThe Greek letter \\(\\mu\\) (mu) represents the population mean (parameter), while \\(\\bar{x}\\) (x-bar) or \\(\\hat{\\mu}\\) (mu-hat) is the mean computed from the sample data (sample statistic).\nThe Greek letter \\(\\sigma\\) (sigma) represents the population standard deviation (parameter), while \\(s\\) or \\(\\hat{\\sigma}\\) (sigma-hat) is the standard deviation computed from the sample data (sample statistic).\nThe Greek letter \\(p\\) represents the population proportion (parameter), while \\(\\hat{p}\\) (p-hat) is the proportion computed from the sample data (sample statistic).\n\nThe process of sampling \\(n\\) people at random from the population is a random experiment, as it leads to an uncertain outcome. Hence, a statistic is a numerical summary of a random experiment and for this reason it is a random variable (random number).\nBefore actually performing the random experiment and picking individuals for the sample, the sample mean is a random number, as its value is uncertain. Once we actually perform the random experiment and measure the individuals in the sample, we have an observed value for the sample mean, i.e. a known number.\nWe will then distinguish between:\n\na statistic - a random variable (random number) that is uncertain because it involves a random experiment.\nan observed statistic - the actual number that is computed once the sample data have been collected by performing the chance experiment.\n\nNotation-wise we distinguish between a statistic (random variable) and an observed statistic (observed number) by using, respectively, an uppercase letter or a lowercase letter.\nUppercase letters refer to random variables. Recall that a random variable represents a well defined number, but whose value is uncertain as it involves an experiment of chance in order to reach to an actual value.\nFor example, \\(\\bar X\\) = “mean IQ score in a random sample of 50 individuals” is a random variable. It is clearly defined in operational terms by: (1) take a sample of 50 individuals at random, (2) measure their IQ score, and (3) compute the mean of their 50 IQ scores. However, the actual value that we can obtain is uncertain as it is the result of an experiment of chance involving random sampling from a population. There are many possible values we could obtain, so we are not sure which one we will see.\nAnother example: \\(X\\) = “number of heads in 10 flips of a coin”. This is also a clearly defined experiment: (1) flip a coin 10 times and (2) count the number of heads. However, the result is a random number, which is uncertain and will be unknown until we actually perform the experiment and flip a coin 10 times.\nLowercase letters refer to observed (i.e., realised) values of the random variable. An observed value of a random variable is just a number.\nFor example, once we actually collect 50 individuals and measure their IQ scores, we can sum those 50 numbers and divide the sum by 50 to obtain the observed sample mean. Say the sample mean IQ score is 102.3, we would write \\(\\bar x = 102.3\\).\nIn short, we would use for a sample mean:\n\nStatistic (sample mean): an uppercase letter before the value is actually known: \\(\\bar X\\)\n\nObserved statistic (observed sample mean): a lowercase letter once the value is actually known: \\(\\bar x\\)\n\n\n\n\n\n\n\n\n\n\n\nAvoiding bias due to sampling\n\n\n\n\n\nSampling bias occurs when the method used to select which units enter the sample causes the sample to not be a good representation of the population.\nIf sampling bias exists, we cannot generalise our sample conclusions to the population.\n\n\n\n\n\n\n\n\nTo be able to draw conclusions about the population, we need a representative sample. The key in choosing a representative sample is random sampling. Imagine an urn with tickets, where each ticket has the name of each population unit. Random sampling would involve mixing the urn and blindly drawing out some tickets from the urn. Random sampling is a strategy to avoid sampling bias.\n\n\n\n\n\n\nSimple random sampling\nWhen we select the units entering the sample via simple random sampling, each unit in the population has an equal chance of being selected, meaning that we avoid sampling bias.\nWhen instead some units have a higher chance of entering the same, we have misrepresentation of the population and sampling bias.\n\n\n\nIn general, we have bias when the method of collecting data causes the data to inaccurately reflect the population.\n\n\n\n\n\n\n\n\n\nSampling distribution\n\n\n\n\n\nThe natural gestation period (in days) for human births is normally distributed in the population with mean 266 days and standard deviation 16 days. This is a special case which rarely happens in practice: we actually know what the distribution looks like in the population.\nWe will use this unlikely example to study how well does the sample mean estimate the population mean and, to do so, we need to know what the population mean is so that we can compare the estimate and the true value. Remember, however, that in practice the population parameter would not be known.\nWe will consider data about the gestation period of the 49,863 women who gave birth in Scotland in 2019. These can be found at the following address: https://uoepsy.github.io/data/pregnancies.csv\nFirst, we read the population data:\n\nlibrary(tidyverse)\ngest &lt;- read_csv('https://uoepsy.github.io/data/pregnancies.csv')\ndim(gest)\n\n[1] 49863     2\n\n\nThe data set contains information about 49,863 cases. For each case an identifier and the length of pregnancy Look at the top six rows of the data set (the “head”):\n\nhead(gest)\n\n# A tibble: 6 × 2\n     id gest_period\n  &lt;dbl&gt;       &lt;dbl&gt;\n1     1        256.\n2     2        269.\n3     3        253.\n4     4        292.\n5     5        271.\n6     6        253.\n\n\nWe now want to investigate how much the sample means will vary from sample to sample. To do so, we will take many samples from the population of all gestation periods, and compute for each sample the mean.\nTo do so, we will load a function which we prepared for you called rep_sample_n(). This function is used to take a sample of \\(n\\) units from the population, and it lets you repeat this process many times.\nTo get the function in your computer, run this code:\n\nsource('https://uoepsy.github.io/files/rep_sample_n.R')\n\n\n\n\n\n\n\nNOTE\nYou need to copy and paste the line\n\nsource('https://uoepsy.github.io/files/rep_sample_n.R')\n\nat the top of each file in which you want to use the rep_sample_n() function.\n\n\n\nThe function takes the following arguments:\nrep_sample_n(data, n = &lt;sample size&gt;, samples = &lt;how many samples&gt;)\n\ndata is the population\nn is the sample size\nsamples is how many samples of size \\(n\\) you want to take\n\nBefore doing anything involving random sampling, it is good practice to set the random seed. This is to ensure reproducibility of the results. Random number generation in R works by specifying a starting seed, and then numbers are generated starting from there.\nSet the random seed to any number you wish. Depending on the number, you will get the same results as me or not:\n\nset.seed(1234)\n\nObtain 10 samples of \\(n = 5\\) individuals each:\n\nsamples &lt;- rep_sample_n(gest, n = 5, samples = 10)\nsamples\n\n# A tibble: 50 × 3\n   sample    id gest_period\n    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1      1 40784        255.\n 2      1 40854        275.\n 3      1 41964        281.\n 4      1 15241        246.\n 5      1 33702        247.\n 6      2 35716        267.\n 7      2 17487        289.\n 8      2 15220        266.\n 9      2 19838        238.\n10      2  2622        281.\n# ℹ 40 more rows\n\n\nThe samples data frame contains 3 columns:\n\nsample, telling us which sample each row refers to\nid, telling us the units chosen to enter each sample\ngest_period, telling us the gestation period (in days) of each individual\n\nNote that the tibble samples has 50 rows, which is given by 5 individuals in each sample * 10 samples.\nYou can inspect the sample data in the following interactive table in which the data corresponding to each sample have been colour-coded so that you can distinguish the rows belonging to the 1st, 2nd, …, and 10th sample:\n\n\n\n\n\n\n\nNow, imagine computing the mean of the five observation in each sample. This will lead to 10 means, one for each of the 10 samples (of 5 individuals each).\n\nsample_means &lt;- samples %&gt;%\n  group_by(sample) %&gt;%\n  summarise(mean_gest = mean(gest_period))\n\nsample_means\n\n# A tibble: 10 × 2\n   sample mean_gest\n    &lt;dbl&gt;     &lt;dbl&gt;\n 1      1      261.\n 2      2      268.\n 3      3      273.\n 4      4      269.\n 5      5      261.\n 6      6      271.\n 7      7      252.\n 8      8      262.\n 9      9      258.\n10     10      267.\n\n\nAs you can see this leads to a tibble having 10 rows (one for each sample), where each row is a mean computed from the 5 individuals which were chosen to enter the sample.\nThe gestation period (in days) for the first five women sampled were\n\n255.15, 275.34, 281.04, 245.62, 247.5\n\nThis sample has a mean of \\(\\bar x\\) = 260.93 days.\nThe second sample of 5 women had gestation periods\n\n267.03, 288.74, 265.56, 238, 280.56\n\nThe second sample has a mean gestation period of \\(\\bar x\\) = 267.98 days.\nIn Figure 1 we display the individual gestation periods in each sample as dots, along with the means gestation period \\(\\bar x\\) of each sample. The position of the sample mean is given by a red vertical bar.\nWe then increased the sample size to 50 women and took 10 samples each of 50 individuals. This set of samples together with their means is also plotted in Figure 1.\n\n\n\n\nFigure 1: Gestation period (in days) of samples of individuals.\n\n\n\nTwo important points need to be made from Figure Figure 1. First, each sample (and therefore each sample mean) is different. This is due to the randomness of which individuals end up being in each sample. The sample means vary in an unpredictable way, illustrating the fact that \\(\\bar X\\) is a summary of a random experiment (randomly choosing a sample) and hence is a random variable. Secondly, as we increase the sample size from 5 to 50, there appears to be a decrease in the variability of sample means (compare the variability in the vertical bars in panel (a) and panel (b)). That is, with a larger sample size, the sample means fluctuate less and are more “consistent”.\nTo further investigate the variability of sample means, we will now generate many more sample means computed on:\n\n1,000 samples of \\(n = 5\\) women\n1,000 samples of \\(n = 50\\) women\n1,000 samples of \\(n = 500\\) women\n\nWe will also add at the end of each tibble a column specifying the sample size. In the first tibble, mutate(n = 5) creates a column called n where all values will be 5, to remind ourselves that those means were computed with samples of size \\(n = 5\\). Remember that mutate() takes a tibble and creates a new column or changes an existing one.\n\n# (a) 1,000 means from 1,000 samples of 5 women each\nsample_means_5 &lt;- rep_sample_n(gest, n = 5, samples = 1000) %&gt;%\n  group_by(sample) %&gt;%\n  summarise(mean_gest = mean(gest_period)) %&gt;%\n  mutate(n = 5)\nhead(sample_means_5)\n\n# A tibble: 6 × 3\n  sample mean_gest     n\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1      267.     5\n2      2      263.     5\n3      3      255.     5\n4      4      269.     5\n5      5      267.     5\n6      6      266.     5\n\n# (b) 1,000 means from 1,000 samples of 50 women each\nsample_means_50 &lt;- rep_sample_n(gest, n = 50, samples = 1000) %&gt;%\n  group_by(sample) %&gt;%\n  summarise(mean_gest = mean(gest_period)) %&gt;%\n  mutate(n = 50)\nhead(sample_means_50)\n\n# A tibble: 6 × 3\n  sample mean_gest     n\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1      263.    50\n2      2      262.    50\n3      3      269.    50\n4      4      266.    50\n5      5      267.    50\n6      6      268.    50\n\n# (c) 1,000 means from 1,000 samples of 500 women each\nsample_means_500 &lt;- rep_sample_n(gest, n = 500, samples = 1000) %&gt;%\n  group_by(sample) %&gt;%\n  summarise(mean_gest = mean(gest_period)) %&gt;%\n  mutate(n = 500)\nhead(sample_means_500)\n\n# A tibble: 6 × 3\n  sample mean_gest     n\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1      265.   500\n2      2      265.   500\n3      3      267.   500\n4      4      266.   500\n5      5      265.   500\n6      6      265.   500\n\n\nWe now combine the above datasets of sample means for different sample sizes into a unique tibble. The function bind_rows() takes multiple tibbles and stacks them under each other.\n\nsample_means_n &lt;- bind_rows(sample_means_5, \n                            sample_means_50, \n                            sample_means_500)\n\nWe now plot three different density histograms showing the distribution of 1,000 sample means computed from samples of size 5, 50, and 500.\nThis would correspond to creating a histogram of the “red vertical bars” from Figure Figure 1, the only difference is that we have many more samples (1,000).\n\nggplot(sample_means_n) +\n  geom_histogram(aes(mean_gest, after_stat(density)), \n                 color = 'white', binwidth = 1) +\n  facet_grid(n ~ ., labeller = label_both) +\n  theme_bw() + \n  labs(x = 'Sample mean of gestation period (days)', y = 'Density')\n\n\n\nFigure 2: Density histograms of the sample means from 1,000 samples of women (\\(n\\) women per sample).\n\n\n\nEach of the density histograms above displays the distribution of the sample mean, computed on samples of the same size and from the same population.\nSuch a distribution is called the sampling distribution of the sample mean.\n\n\n\n\n\n\nSampling distribution\nThe sampling distribution of a statistic is the distribution of a sample statistic computed on many different samples of the same size from the same population.\nA sampling distribution shows how the statistic varies from sample to sample due to sampling variation.\n\n\n\n\n\n\n\n\n\n\n\n\nCentre and spread of a sampling distribution\n\n\n\n\n\nWhat is the mean and standard deviation of each histogram?\n\nsample_means_n %&gt;%\n  group_by(n) %&gt;%\n  summarise(mean_xbar = mean(mean_gest),\n            sd_xbar = sd(mean_gest))\n\n# A tibble: 3 × 3\n      n mean_xbar sd_xbar\n  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1     5      266.   7.38 \n2    50      266.   2.23 \n3   500      266.   0.734\n\n\nCompare these quantities to the population mean and standard deviation: \\(\\mu\\) = 266 and \\(\\sigma\\) = 16.1.\nRegardless of the size of the samples we were drawing (5, 50, or 500), the average of the sample means was equal to the population mean. However, the standard deviation of the sample means was smaller than the population mean. The variability in sample means also decreases as the sample size increases.\nThere is an interesting patter in the decrease, which we will now verify. It can be proved that the standard deviation of the sample mean \\(\\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{n}}\\), i.e. the population standard deviation divided by \\(\\sqrt{n}\\) with \\(n\\) being the sample size.\nObtain the population standard deviation. Remember the entire population data were called gest and in this case we are very lucky to have the data for the entire population, typically we wouldn’t have those and neither the population standard deviation.\n\nsigma &lt;- sd(gest$gest_period)\n\nNow compute add a column that compares the SD from sampling with the theory-based one:\n\nsample_means_n %&gt;%\n  group_by(n) %&gt;%\n  summarise(mean_xbar = mean(mean_gest),\n            sd_xbar = sd(mean_gest)) %&gt;%\n  mutate(sd_theory = sigma / sqrt(n))\n\n# A tibble: 3 × 4\n      n mean_xbar sd_xbar sd_theory\n  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1     5      266.   7.38      7.20 \n2    50      266.   2.23      2.28 \n3   500      266.   0.734     0.720\n\n\nThe last two columns will be closer and closer as we increase the number of different samples we take from the population (e.g. 5,000 or 10,000 or even more samples.)\nThe following result holds: \\[\n\\begin{aligned}\n\\mu_{\\bar X} &= \\mu = \\text{Population mean} \\\\\n\\sigma_{\\bar X} &= \\frac{\\sigma}{\\sqrt{n}} = \\frac{\\text{Population standard deviation}}{\\sqrt{\\text{Sample size}}}\n\\end{aligned}\n\\]\nBecause on average the sample mean (i.e. the estimate) is equal to the population mean (i.e. the parameter), the sample mean \\(\\bar X\\) is an unbiased estimator of the population mean. In other words, it does not consistently “miss” the target. (However, if your sampling method is biased, the sample mean will be biased too.)\nThe standard deviation of the sample means tells us that the variability in the sample means gets smaller smaller as the sample size increases. Because \\(\\sqrt{4} = 2\\) we halve \\(\\sigma_{\\bar X}\\) by making the sample size 4 times as large. Similarly, as \\(\\sqrt{9} = 3\\), we reduce \\(\\sigma_{\\bar X}\\) by one third by making the sample size 9 times as large.\nThe variability, or spread, of the sampling distribution shows how much the sample statistics tend to vary from sample to sample. This is key in understanding how accurate our estimate of the population parameter, based on just one sample, will be.\nRecall that the standard deviation tells us the size of a typical deviation from the mean. Here, the mean is the population parameter \\(\\mu\\), and a deviation of \\(\\bar x\\) from \\(\\mu\\) is called an estimation error. Hence, the standard deviation of the sample mean is called the standard error of the mean. This tells us the typical estimation error that we commit when we estimate a population mean with a sample mean.\n\n\n\n\n\n\nStandard error\nThe standard error of a statistic, denoted \\(SE\\), is the standard deviation of its sampling distribution.\n\n\n\n\n\n\n\n\nSo, the standard error of the mean can be either computed as the standard deviation of the sampling distribution, or using the formula \\[\nSE = \\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\n\nThe sample mean is normally distributed\n\n\n\n\n\nWe also notice that the density histograms in Figure 2 are symmetric and bell-shaped. Hence, they follow the shape of the normal curve.\n\n\n\n\n\n\n\n\nThe random variable \\(\\bar X\\) follows a normal distribution: \\[\n\\bar X \\sim N(\\mu,\\ SE)\n\\]\nWe can also compute a z-score. We have that: \\[\nZ = \\frac{\\bar X - \\mu}{SE} \\sim N(0, 1)\n\\]\nWe know that for a normally distributed random variable, approximately 95% of all values fall within two standard deviations of its mean. Thus, for approximately 95% of all samples, the sample means falls within \\(\\pm 2 SE\\) of the population mean \\(\\mu\\). Similarly, since \\(P(-3 &lt; Z &lt; 3) = 0.997\\), it is even more rare to get a sample mean which is more than three standard errors away from the population mean (only 0.3% of the times).\nThis suggests that:\n\nThe standard error \\(SE\\) is a measure of precision of \\(\\bar x\\) as an estimate of \\(\\mu\\).\nIf is a pretty safe bet to say that the true value of \\(\\mu\\) lies somewhere between \\(\\bar x - 2 SE\\) and \\(\\bar x + 2 SE\\).\nWe will doubt any hypothesis specifying that the population mean is \\(\\mu\\) when the value \\(\\mu\\) is more than \\(2 SE\\) away from the sample mean we got from our data, \\(\\bar x\\). We shall be even more suspicious when the hypothesised value \\(\\mu\\) is more than \\(3 SE\\) away from \\(\\bar x\\).\n\n\n\n\n\n\n\nCentre and shape of a sampling distribution\n\nCentre: If samples are randomly selected, the sampling distribution will be centred around the population parameter. (No bias)\nShape: For most of the statistics we consider, if the sample size is large enough, the sampling distribution will follow a normal distribution, i.e. it is symmetric and bell-shaped. (Central Limit Theorem)\n\n\n\n\nClearly, we can compute sampling distributions for other statistics too: the proportion, the standard deviation, …\nThis requires the following steps:\n\nObtaining multiple samples, all of the same size, from the same population;\nFor each sample, calculate the value of the statistic;\nPlot the distribution of the computed statistics.\n\n\n\n\n\n\n\n\n\n\nWhy sample size matters\n\n\n\n\n\nYou might be wondering: why did we take multiple samples of size \\(n\\) from the population when, in practice, we can only afford to take one?\nThis is a good question. We have taken multiple samples to show how the estimation error varies with the sample size. We saw in Figure 2, shown again below, that smaller sample sizes lead to more variable statistics, while larger sample sizes lead to more precise statistics, i.e. the estimates are more concentrated around the true parameter value.\n\n\n\n\nDensity histograms of the sample means from 5,000 samples of women (\\(n\\) women per sample).\n\n\n\nThis teaches us that, when we have to design a study, it is better to obtain just one sample with size \\(n\\) as large as we can afford.\n\n\n\n\n\n\nThink about it\n\n\n\n\n\nWhat would the sampling distribution of the mean look like if we could afford to take samples as big as the entire population, i.e. of size \\(n = N\\)?\n\n\n\n\n\n\n\n\n\nIf you can, it is best to measure the entire population.\nIf we could afford to measure the entire population, then we would find the exact value of the parameter all the time. By taking multiple samples of size equal to the entire population, every time we would obtain the population parameter exactly, so the distribution would look like a histogram with a single bar on top of the true value: we would find the true parameter with a probability of one, and the estimation error would be 0.\n\npop_means &lt;- gest %&gt;%\n  rep_sample_n(n = nrow(gest), samples = 10) %&gt;%\n  group_by(sample) %&gt;%\n  summarise(mean_gest = mean(gest_period))\npop_means\n\n# A tibble: 10 × 2\n   sample mean_gest\n    &lt;dbl&gt;     &lt;dbl&gt;\n 1      1      266.\n 2      2      266.\n 3      3      266.\n 4      4      266.\n 5      5      266.\n 6      6      266.\n 7      7      266.\n 8      8      266.\n 9      9      266.\n10     10      266.\n\n\nThe following is a dotplot of the means computed above:\n\n\n\n\n\n\n\n\n\n\n\nTo summarize:\n\nWe have high precision when the estimates are less variable, and this happens for a large sample size.\nWe have no bias when we select samples that are representative of the population, and this happens when we do random sampling. No bias means that the estimates will be centred at the true population parameter to be estimated."
  },
  {
    "objectID": "rd1_11.html#glossary",
    "href": "rd1_11.html#glossary",
    "title": "Sampling distributions",
    "section": "\n2 Glossary",
    "text": "2 Glossary\n\n\nStatistical inference. The process of drawing conclusions about the population from the data collected in a sample.\n\nPopulation. The entire collection of units of interest.\n\nSample. A subset of the entire population.\n\nRandom sample. A subset of the entire population, picked at random, so that any conclusion made from the sample data can be generalised to the entire population.\n\nRepresentation bias. Happens when some units of the population are systematically underrepresented in samples.\n\nGeneralisability. When information from the sample can be used to draw conclusions about the entire population. This is only possible if the sampling procedure leads to samples that are representative of the entire population (such as those drawn at random).\n\nParameter. A fixed but typically unknown quantity describing the population.\n\nStatistic. A quantity computed on a sample.\n\nSampling distribution. The distribution of the values that a statistic takes on different samples of the same size and from the same population.\n\nStandard error. The standard error of a statistic is the standard deviation of the sampling distribution of the statistic."
  },
  {
    "objectID": "rd1_09.html",
    "href": "rd1_09.html",
    "title": "Discrete random variables",
    "section": "",
    "text": "Consider throwing three fair coins. The sample space of this random experiment is\n\\[\nS = \\{\n    TTT, \\\n    TTH, \\\n    THT, \\\n    HTT, \\\n    THH, \\\n    HTH, \\\n    HHT, \\\n    HHH\n\\}\n\\]\nEach outcome has an equal chance of occurring of \\(1 / 8 = 0.125\\), computed as one outcome divided by the total number of possible outcomes.\nOften, we are only interested in a numerical summary of the random experiment. One such summary could be the total number of heads.\n\nRandom variable\nWe call a numerical summary of a random process a random variable.\nRandom variables are typically denoted using the last uppercase letters of the alphabet (\\(X, Y, Z\\)). Sometimes we might also use an uppercase letter with a subscript to distinguish them, e.g. \\(X_1, X_2, X_3\\).\n\nA random variable, like a random experiment, also has a sample space and this is called the support or range of \\(X\\), written \\(R_X\\). This represents the set of possible values that the random variable can take."
  },
  {
    "objectID": "rd1_09.html#random-variables",
    "href": "rd1_09.html#random-variables",
    "title": "Discrete random variables",
    "section": "",
    "text": "Consider throwing three fair coins. The sample space of this random experiment is\n\\[\nS = \\{\n    TTT, \\\n    TTH, \\\n    THT, \\\n    HTT, \\\n    THH, \\\n    HTH, \\\n    HHT, \\\n    HHH\n\\}\n\\]\nEach outcome has an equal chance of occurring of \\(1 / 8 = 0.125\\), computed as one outcome divided by the total number of possible outcomes.\nOften, we are only interested in a numerical summary of the random experiment. One such summary could be the total number of heads.\n\nRandom variable\nWe call a numerical summary of a random process a random variable.\nRandom variables are typically denoted using the last uppercase letters of the alphabet (\\(X, Y, Z\\)). Sometimes we might also use an uppercase letter with a subscript to distinguish them, e.g. \\(X_1, X_2, X_3\\).\n\nA random variable, like a random experiment, also has a sample space and this is called the support or range of \\(X\\), written \\(R_X\\). This represents the set of possible values that the random variable can take."
  },
  {
    "objectID": "rd1_09.html#discrete-vs-continuous-random-variables",
    "href": "rd1_09.html#discrete-vs-continuous-random-variables",
    "title": "Discrete random variables",
    "section": "\n2 Discrete vs continuous random variables",
    "text": "2 Discrete vs continuous random variables\nThere are two different types of random variables, and the type is defined by their range.\nWe call a variable discrete or continuous depending on the “gappiness” of its range, i.e. depending on whether or not there are gaps between successive possible values of a random variable.\n\n\n\n\n\n\n\n\n\nA discrete random variable has gaps in between its possible values. An example is the number of children in a randomly chosen family (0, 1, 2, 3, …). Clearly, you can’t have 2.3 children…\nA continuous random variable has no gaps in between the its possible values. An example is the height in cm of a randomly chosen individual.\n\nIn this week’s exercises we will study discrete random variables."
  },
  {
    "objectID": "rd1_09.html#three-coins-example-continued",
    "href": "rd1_09.html#three-coins-example-continued",
    "title": "Discrete random variables",
    "section": "\n3 Three coins example (continued)",
    "text": "3 Three coins example (continued)\nIn the 3 coins example, the possible values of the random variable \\(X\\) = “number of heads in 3 tosses” are \\[\nR_X = \\{0, 1, 2, 3\\}\n\\]\nmeaning that \\(X\\) is a discrete random variable.\nWe denote a potential value of the random variable using a lowercase \\(x\\) and a subscript to number the possible values.\nThis is obtained as follows:\n\n\n\n\n\n\n\n\nAs we can see, each value of the random variable is computed from the underlying random experiment. There is one outcome only (TTT) leading to zero heads, i.e. \\(X = 0\\). There are three outcomes (TTH, THT, HTT) leading to one head, i.e. \\(X = 1\\). And so on…\nThe experiment’s outcomes are all equally likely, each having a \\(1/8\\) chance of occurring. However, since the random variable aggregates the experiment’s outcomes, the probability of the random variable taking a particular value is computed by summing the probabilities of the outcomes leading to that value.\nLet’s try and obtain the same diagram as that shown above using R. We will be using the function expand_grid, which creates the sample space by listing all possible combinations.\n\nlibrary(tidyverse)\n\nexperiment &lt;- expand_grid(coin1 = c('T', 'H'),\n                          coin2 = c('T', 'H'),\n                          coin3 = c('T', 'H'))\nexperiment\n\n# A tibble: 8 × 3\n  coin1 coin2 coin3\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 T     T     T    \n2 T     T     H    \n3 T     H     T    \n4 T     H     H    \n5 H     T     T    \n6 H     T     H    \n7 H     H     T    \n8 H     H     H    \n\nexperiment &lt;- experiment %&gt;% \n    mutate(\n        prob = rep( 1/n(), n() )\n    )\nexperiment\n\n# A tibble: 8 × 4\n  coin1 coin2 coin3  prob\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n1 T     T     T     0.125\n2 T     T     H     0.125\n3 T     H     T     0.125\n4 T     H     H     0.125\n5 H     T     T     0.125\n6 H     T     H     0.125\n7 H     H     T     0.125\n8 H     H     H     0.125\n\nrv &lt;- experiment %&gt;%\n    mutate(\n        value = (coin1 == 'H') + (coin2 == 'H') + (coin3 == 'H')\n    ) %&gt;%\n    group_by(value) %&gt;%\n    summarise(prob = sum(prob))\nrv\n\n# A tibble: 4 × 2\n  value  prob\n  &lt;int&gt; &lt;dbl&gt;\n1     0 0.125\n2     1 0.375\n3     2 0.375\n4     3 0.125\n\n\nwhere \\(1/8 = 0.125\\) and \\(3/8 = 0.375\\).\n\nWe can provide a concise representation of a random variable \\(X\\), the set of all its possible values, and the probabilities of those values by providing the probability distribution of \\(X\\). You can think of the probability distribution of a random variable as a succinct way to provide a global picture of the random variable.\n\n\n\n\n\n\nProbability distribution\n\n\n\nThe probability distribution of a discrete random variable \\(X\\) provides the possible values of the random variable and their corresponding probabilities.\nA probability distribution can be in the form of a table, graph, or mathematical formula.\n\n\nWe visualise the distribution of a discrete random variable via a line graph. This graph gives us, with just a glance, an immediate representation of the distribution of that random variable.\n\nggplot(data = rv) +\n    geom_segment(aes(x = value, xend = value, y = 0, yend = prob)) +\n    geom_point(aes(x = value, y = prob)) +\n    labs(x = \"Possible values, x\", y = \"Probabilities, P(X = x)\")\n\n\n\n\n\n\n\nAs you can see, a line graph has gaps in between the possible values the random variable can take, exactly to remind us that the random variable can’t take values that are different from 0, 1, 2, and 3.\nAlternatively, you could provide the probability distribution of the random variable in tabular form:\n\n\n\n\nx\nP(X = x)\n\n\n\n0\n1/8\n\n\n1\n3/8\n\n\n2\n3/8\n\n\n3\n1/8\n\n\n\n\n\nStatisticians have also spent lots of time trying to find a mathematical formula for that probability distribution. The formula is the most concise way to obtain the probabilities as it gives you a generic rule which you can use to compute the probability of any possible value of that random variable. All you have to do is substitute to \\(x\\) the value you are interested in, e.g. 0, 1, 2, or 3.\n\n\n\n\n\n\nProbability mass function\n\n\n\nThe probability mass function (pmf) of \\(X\\) assigns a probability, between 0 and 1, to every value of the discrete random variable \\(X\\).\nEither of the following symbols are often used: \\[\nf(x) = P(x) = P(X = x) \\qquad \\text{for all possible }x\n\\] where \\(P(X = x)\\) reads as “the probability that the random variable \\(X\\) equals \\(x\\)”.\nThe sum of all of these probabilities must be one, i.e. \\[\n\\sum_{i} f(x_i) = \\sum_{i} P(X = x_i) = 1\n\\]\n\n\nBefore we define the mathematical function, I need to tell you what a number followed by an exclamation mark means.\nIn mathematics \\(n!\\), pronounced “\\(n\\) factorial”, is the product of all the integers from 1 to \\(n\\). For example, \\(4! = 4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\\), and \\(3! = 3 \\cdot 2 \\cdot 1 = 6\\). By convention, mathematician have decided that \\(0! = 1\\).\nThe probability function of \\(X\\) = “number of heads in 3 tosses” makes use of the following numbers:\n\n\n\\(3\\), representing the number of coin flips\n\n\\(\\frac{1}{2}\\), the probability of observing heads in a single flip of a fair coin\n\nFor the three coins example, the probability function of \\(X\\) is \\[\nP(X = x) = \\frac{3!}{x!\\ (3-x)!} \\cdot (1/2)^x \\cdot (1/2)^{3-x}\n\\]\nLet’s see if the formula gives back the table we created above.\n\n\nFor \\(x=0\\) we have:\n\\[\nP(X = 0) = \\frac{3!}{0!\\ 3!} \\cdot (1/2)^0 \\cdot (1/2)^3 = \\frac{6}{6} \\cdot 1 \\cdot (1/8) = 1/8\n\\]\n\n\nAnd so on… If you want to see the rest, check the optional box below.\n\n\n\n\n\n\nOptional: I want to see the other probabilities\n\n\n\n\n\n\n\nFor \\(x = 1\\) we have\n\\[\nP(X = 1) = \\frac{3!}{1!\\ 2!} \\cdot (1/2)^1 \\cdot (1/2)^2 = \\frac{6}{2} \\cdot (1/2) \\cdot (1/4) = 3 \\cdot (1/8) = 3/8\n\\]\n\n\nFor \\(x = 2\\) we have\n\\[\nP(X = 2) = \\frac{3!}{2!\\ 1!} \\cdot (1/2)^2 \\cdot (1/2)^1= \\frac{6}{2} \\cdot (1/4) \\cdot (1/2) = 3 \\cdot (1/8) = 3/8\n\\]\n\n\nFor \\(x=3\\) we have\n\\[\nP(X = 3) = \\frac{3!}{3!\\ 0!} \\cdot (1/2)^3 \\cdot (1/2)^0 = \\frac{6}{6} \\cdot (1/8) \\cdot 1 = 1/8\n\\]\n\n\n\n\n\nAs you can see, this formula will provide you the same values that are listed in the tabular representation of the probability distribution."
  },
  {
    "objectID": "rd1_09.html#centre-the-expected-value",
    "href": "rd1_09.html#centre-the-expected-value",
    "title": "Discrete random variables",
    "section": "\n4 Centre: the expected value",
    "text": "4 Centre: the expected value\nConsider a discrete random variable with range \\(R_X = \\{x_1, x_2, \\dots, x_n\\}\\)\nThe expected value (or mean) of a random variable \\(X\\), denoted by \\(E(X)\\), \\(\\mu\\), or \\(\\mu_X\\), describes where the probability distribution of \\(X\\) is centred.\nWe tend to prefer the name “expected value” to “mean” as the random variable is not something the has happened yet, it’s a potentially observable value. So, the expected value is the typical value we expect to observe.\nThe expected value of \\(X\\) is computed by multiplying each value by its probability and then summing everything:\n\\[\n\\begin{aligned}\n\\mu = E(X) &= x_1 \\cdot P(x_1) + x_2 \\cdot P(x_2) + \\cdots + x_n \\cdot P(x_n) \\\\\n&= \\sum_{i} x_i \\cdot P(x_i)\n\\end{aligned}\n\\]\nFor the three coins, the expected value is: \\[\n\\mu = 0 \\cdot \\frac{1}{8} + 1 \\cdot \\frac{3}{8} + 2 \\cdot \\frac{3}{8} + 3 \\cdot \\frac{1}{8} = \\frac{3}{2} = 1.5\n\\]\nAs you can see, 1.5 is not one of the possible values that \\(X\\) can take in that case, as it lies in the gap between the values 1 and 2. However, it is a fictitious number which seems to well represent the centre of that distribution and hence a typical value from that distribution."
  },
  {
    "objectID": "rd1_09.html#spread-the-standard-deviation",
    "href": "rd1_09.html#spread-the-standard-deviation",
    "title": "Discrete random variables",
    "section": "\n5 Spread: the standard deviation",
    "text": "5 Spread: the standard deviation\nThe variability of a random variable \\(X\\) is measured by its standard deviation.\n\n\n\n\n\n\nVariance and standard deviation\n\n\n\nIf \\(X\\) has expected value \\(\\mu\\), the variance of \\(X\\) is \\[\n\\sigma^2 = \\sum_i (x_i - \\mu)^2 \\cdot P(x_i)\n\\] and the standard deviation is defined as \\[\n\\sigma = \\sqrt{\\sigma^2}\n\\]"
  },
  {
    "objectID": "rd1_09.html#underlying-random-experiments",
    "href": "rd1_09.html#underlying-random-experiments",
    "title": "Discrete random variables",
    "section": "\n6 Underlying random experiments",
    "text": "6 Underlying random experiments\nAs we saw, each random variable is a numerical summary of a random experiment and, as such, it arises from an underlying random experiment.\nIn this section we will analyse different random experiments, also called models, commonly arising in every day situations.\n\n6.1 Binomial model\n\n\n\n\n\n\nNotation\n\n\n\n\\(p\\) is the probability of a success on any one trial, and \\(n\\) is the number of trials.\n\n\nSuppose you have a series of trials that satisfy these conditions:\n\nB: They are Bernoulli — that is, each trial must have one of two different outcomes, one called a “success” and the other a “failure.”\nI: Each trial is independent of the others — that is, the probability of a success doesn’t change depending on what has happened before.\nN: There is a fixed number, \\(n\\), of trials.\nS: The probability, \\(p\\), of a success is the same on each trial, with \\(0 \\leq p \\leq 1\\).\n\nThen the distribution of the random variable \\(X\\) that counts the number of successes in \\(n\\) trials (each with a probability of success = \\(p\\)) is called a binomial distribution.\nThe numbers \\(n\\) and \\(p\\) are called the parameters of the binomial distribution. We write that \\(X\\) follows a binomial distribution with parameters \\(n\\) and \\(p\\) as follows: \\[\nX \\sim \\text{Binomial}(n,p)\n\\]\nFurther, the probability that you get exactly \\(X = x\\) successes is \\[\nP(X = x) = \\frac{n!}{x!\\ (n-x)!} \\cdot p^x \\cdot (1-p)^{n-x}, \\qquad R_X = \\{0, 1, 2, ..., n\\}\n\\] where \\(n! = n (n-1) (n-2) \\cdots 3 \\cdot 2 \\cdot 1\\).\nDo you recognise it from the coins example?\nVisual exploration\nThe figure below displays different binomial distributions as \\(n\\) and \\(p\\) vary:\n\n\n\n\nThe binomial probability distribution as n and p vary.\n\n\n\nCentre and spread\nFor a random variable \\(X\\) having a binomial distribution with \\(n\\) trials and probability of success \\(p\\), the mean (expected value) and standard deviation for the distribution are given by \\[\n\\mu_X = n p \\qquad \\text{and} \\qquad \\sigma_X = \\sqrt{n p (1 - p)}\n\\]\nBinomial distribution in R\nThe function to compute the binomial probability distribution is\ndbinom(x, size, prob)\nwhere:\n\n\nx is the values for which we want to compute the probabilities\n\nsize is \\(n\\) in our notation, the number of trials\n\nprob is \\(p\\) in our notation, the probability of success in each trial.\nExample\nA student is attempting a 10-questions multiple choice test. Each question has four different options. If the student answers at random, what is the chance that they correctly answers 2 out of the 10 questions?\nAs we know that the student is randomly guessing the answers, the probability of a correct answer is \\(p = 1/4\\). The probability of answering 2 questions correctly out of the 10 in the test is \\(P(X = 2)\\):\n\ndbinom(x = 2, size = 10, prob = 1/4)\n\n[1] 0.2815676\n\n\nIn a multiple choice test comprising 10 questions having each 4 possible answers, there is a 28% chance of answering exactly 2 questions out of 10 correctly just by random guessing.\n\nNote that you can also compute the probabilities for all possible values of \\(X\\) at once:\n\ntibble(\n    values = 0:10,\n    prob = dbinom(x = 0:10, size = 10, prob = 1/4)\n)\n\n# A tibble: 11 × 2\n   values        prob\n    &lt;int&gt;       &lt;dbl&gt;\n 1      0 0.0563     \n 2      1 0.188      \n 3      2 0.282      \n 4      3 0.250      \n 5      4 0.146      \n 6      5 0.0584     \n 7      6 0.0162     \n 8      7 0.00309    \n 9      8 0.000386   \n10      9 0.0000286  \n11     10 0.000000954\n\n\n\n6.2 Geometric model\nSuppose you have a series of trials that satisfy these conditions:\n\nThey are Bernoulli — that is, each trial must have one of two different outcomes, one called a “success” and the other a “failure”.\nEach trial is independent of the others; that is, the probability of a success doesn’t change depending on what has happened before.\nThe trials continue until the first success.\nThe probability, \\(p\\), of a success is the same on each trial, \\(0 \\leq p \\leq 1\\).\n\nThen the distribution of the random variable \\(X\\) that counts the number of failures before the first “success” is called a geometric distribution.\nThe probability that the first success occurs after \\(X = x\\) failures is \\[\nP(X = x) = (1 - p)^{x} p, \\qquad R_X = \\{0, 1, 2, ...\\}\n\\]\nWe write that \\(X\\) follow a geometric distribution with parameter \\(p\\) as follows: \\[\nX \\sim \\text{Geometric}(p)\n\\]\nVisual exploration\nThe figure below displays different geometric distributions as \\(p\\) varies:\n\n\n\n\n\n\n\n\nCentre and spread\nA random variable \\(X\\) that has a geometric distribution with probability of success \\(p\\) has an expected value (mean) and standard deviation of \\[\n\\mu_X = \\frac{1 - p}{p} \\qquad \\text{and} \\qquad \\sigma_X = \\sqrt{\\frac{1-p}{p^2}}\n\\]\nGeometric distribution in R\nThe function to compute the geometric probability distribution is\ndgeom(x, prob)\nwhere:\n\n\nx is the number of failures before the first success\n\nprob is \\(p\\) in our notation, the probability of success in each trial.\nExample\nConsider rolling a fair six-sided die until a five appears. What is the probability of rolling the first five on the third roll?\nFirst, note that the probability of “success” (observing a five) is \\(p = 1/6\\). We are asked to compute the probability of having the first “success” on the 3rd trial. We want to compute \\(P(X = 2)\\) because we need to have 2 failures followed by a success:\n\ndgeom(x = 2, prob = 1/6)\n\n[1] 0.1157407\n\n\nThus, there is a 12% chance of obtaining the first five on the 3rd roll of a die."
  },
  {
    "objectID": "rd1_09.html#glossary",
    "href": "rd1_09.html#glossary",
    "title": "Discrete random variables",
    "section": "\n7 Glossary",
    "text": "7 Glossary\n\n\nRandom variable. A numerical summary of a random experiment.\n\nRange of a random variable. The set of possible values the random variable can take.\n\nProbability distribution. A table, graph, or formula showing how likely each possible value of a random variable is to occur.\n\nProbability (mass) function. A function providing the probabilities, between 0 and 1, for each value that the random variable can take. These probabilities must sum to 1.\n\nBinomial random variable. \\(X\\) represents the number of successes in \\(n\\) trials where the probability of success, \\(p\\), is constant from trial to trial. It has range \\(R_X = \\{0, 1, 2, ..., n\\}\\).\n\nGeometric random variable. \\(X\\) represents the number of failures until the first success, where the probability of success, \\(p\\), is constant from trial to trial. It has range \\(R_X = \\{0, 1, 2, ...\\}\\)."
  },
  {
    "objectID": "rd1_07.html",
    "href": "rd1_07.html",
    "title": "Probability 1",
    "section": "",
    "text": "Think about flipping a coin once. Can you predict the outcome?\nThink about flipping a coin many times, one million say. Are you able to predict roughly how many heads or tails will show up?\nIt’s hard to guess the outcome of just one coin flip because the outcome could be one of two possible outcomes. Hence, we say that flipping a coin is a random experiment or random process.\nIf you flip it over and over, however, you can predict the proportion of heads you’re likely to see in the long run. In the long run simply means if you were to repeat the same experiment over and over many times under the same conditions.\nThis discussion leads us to define the specific type of randomness that we will be studying in this course.\n\n\n\n\n\n\nWhat is randomness?\nWe will say that a repeatable process is random if its outcome is\n\nunpredictable in the short run, and\npredictable in the long run.\n\n\n\n\nIt is this long-term predictability of randomness that we will use throughout the rest of the course. To do that, we will need to talk about the probabilities of different outcomes and learn some rules for dealing with them."
  },
  {
    "objectID": "rd1_07.html#introduction",
    "href": "rd1_07.html#introduction",
    "title": "Probability 1",
    "section": "",
    "text": "Think about flipping a coin once. Can you predict the outcome?\nThink about flipping a coin many times, one million say. Are you able to predict roughly how many heads or tails will show up?\nIt’s hard to guess the outcome of just one coin flip because the outcome could be one of two possible outcomes. Hence, we say that flipping a coin is a random experiment or random process.\nIf you flip it over and over, however, you can predict the proportion of heads you’re likely to see in the long run. In the long run simply means if you were to repeat the same experiment over and over many times under the same conditions.\nThis discussion leads us to define the specific type of randomness that we will be studying in this course.\n\n\n\n\n\n\nWhat is randomness?\nWe will say that a repeatable process is random if its outcome is\n\nunpredictable in the short run, and\npredictable in the long run.\n\n\n\n\nIt is this long-term predictability of randomness that we will use throughout the rest of the course. To do that, we will need to talk about the probabilities of different outcomes and learn some rules for dealing with them."
  },
  {
    "objectID": "rd1_07.html#video-activity",
    "href": "rd1_07.html#video-activity",
    "title": "Probability 1",
    "section": "\n2 Video activity",
    "text": "2 Video activity\nPlease watch the following video, explaining you to the concept of “randomness”."
  },
  {
    "objectID": "rd1_07.html#random-experiments-and-probability",
    "href": "rd1_07.html#random-experiments-and-probability",
    "title": "Probability 1",
    "section": "\n3 Random experiments and probability",
    "text": "3 Random experiments and probability\n\n3.1 Example 1: Flipping a coin\nThe process of flipping a coin is an example of a random experiment as its outcome is uncertain. We do not know beforehand whether the coin will land heads (H) or tails (T).\nThe collection of all possible outcomes is known as the outcome space or sample space. We typically denote the sample space by \\(S\\). In the coin example, this is: \\[\nS = \\{ H, T \\}\n\\]\nOne particular repetition (i.e. instance) of such experiment is known as a trial.\n\n3.2 Example 2: Throwing a die\nAnother example of a random experiment is throwing a six-faced die as, for each trial, we can not exactly predict which face will appear.\nThe list of possible outcomes for the die experiment is \\(1, 2, ..., 6\\). Hence, the sample space can be written: \\[\nS = \\{1, 2, 3, 4, 5, 6 \\}\n\\]\n\n3.3 Events\nConsider again the die experiment. Often, we are not interested in the probability of observing a particular outcome, such as 3, but rather in a collection of outcomes together. For example, we might be interested in the probability of observing an even number.\nSuch collections of outcomes are called events. More formally, an event is a set of outcomes.\nEach individual outcome is also considered an event. To distinguish, some people call simple events the individual outcomes, and compound events a collection of two or more outcomes.\nThe event “an even number appears” is simply the collection of even outcomes. We could call it “E” for “even” and write it as: \\[E = \\{ 2, 4, 6\\}\\]\nTwo important events are:\n\nthe empty set: the set of no outcomes, denoted \\(\\emptyset\\);\nthe sample space: the set of all outcomes, denoted \\(S\\).\n\nIn general, a finite sample space is written \\[\nS = \\{s_1, s_2, ..., s_n\\}\n\\]\nwhere each \\(s_i\\) represents an outcome or simple event.\n\n3.4 Example 3: Flipping 2 coins\nConsider flipping 2 coins simultaneously. The sample space is: \\[\nS = \\{ (H,H), (H,T), (T,H), (T,T)\\}\n\\]\nTypical events could be:\n\nObserving tails at least once, \\(A = \\{(H,T), (T,H), (T,T)\\}\\)\n\nObserving the same face twice, \\(B = \\{(H,H), (T,T)\\}\\)"
  },
  {
    "objectID": "rd1_07.html#defining-probability",
    "href": "rd1_07.html#defining-probability",
    "title": "Probability 1",
    "section": "\n4 Defining probability",
    "text": "4 Defining probability\nConsider repeating the random process of throwing a die many times, 500 say, and recording whether or not an even number appears. We will now create a table listing the result of each trial.\nFirst, we load the tidyverse package:\n\nlibrary(tidyverse)\n\nNext, we create the sample space:\n\nS &lt;- 1:6\nS\n\n[1] 1 2 3 4 5 6\n\n\nThe following code defines the event “an even number appears”:\n\nE &lt;- c(2, 4, 6)\nE\n\n[1] 2 4 6\n\n\nSay, now, that the outcome of a roll is 3. How can we check whether the outcome belongs to the event of interest \\(E\\)?\nWe can use the %in% function to ask R: “Is 3 in E?” The answer can be TRUE or FALSE.\n\n3 %in% E\n\n[1] FALSE\n\n\nSuppose instead, that the outcome of a roll is 2. Is 2 in E?\n\n2 %in% E\n\n[1] TRUE\n\n\nWe now specify how many trials we will be performing:\n\nnum_trials &lt;- 500\n\nNext, we repeat the experiment num_trials times and compute the accumulated percentage of even outcomes:\n\nexperiment &lt;- tibble(\n    trial = 1:num_trials,\n    outcome = sample(S, num_trials, replace = TRUE),\n    is_even = outcome %in% E,\n    cumul_even = cumsum(is_even),\n    cumul_perc_even = 100 * cumsum(is_even) / trial\n)\n\nThe following code displays the top 10 rows of the experiment:\n\nhead(experiment, n = 10)\n\n# A tibble: 10 × 5\n   trial outcome is_even cumul_even cumul_perc_even\n   &lt;int&gt;   &lt;int&gt; &lt;lgl&gt;        &lt;int&gt;           &lt;dbl&gt;\n 1     1       5 FALSE            0             0  \n 2     2       2 TRUE             1            50  \n 3     3       4 TRUE             2            66.7\n 4     4       4 TRUE             3            75  \n 5     5       2 TRUE             4            80  \n 6     6       3 FALSE            4            66.7\n 7     7       4 TRUE             5            71.4\n 8     8       2 TRUE             6            75  \n 9     9       5 FALSE            6            66.7\n10    10       2 TRUE             7            70  \n\n\n\nUnderstanding the code. Let’s inspect each column in turn:\n\n\ntrial records the number of each trial: 1, 2, …, 500;\n\noutcome lists the result of each trial: 1, or 2, …, or 6;\n\nis_even checks whether the outcome of each trial belongs to the event \\(E\\) (=TRUE) or not (=FALSE);\n\ncumul_even computes the cumulative sum of is_even.\n\nAs we saw, the is_even column contains either TRUE or FALSE. This was created with the function %in%, which is equivalent to asking a question: Is the outcome in \\(E\\)? The result will be either TRUE or FALSE. We note that, when summed, R considers a TRUE as 1, and a FALSE as 0. For example, the cumulative sum of c(TRUE, FALSE, TRUE, TRUE, FALSE) is c(1, 1, 2, 3, 3). Finally, the column cumul_perc_even computes the accumulate percentage of even outcomes.\n\nThe first trial’s outcome was 5, which is not an even number. Hence the cumulative percentage of even outcomes is 0 out of 100, or 0%. The next four trials lead to 2, 4, 4, and 2 respectively, which all are even numbers. The cumulative percentages will be 1 out of 2 (50%), 2 out of 3 (66.67%), 3 out of 4 (75%), and 4 out of 5 (80%). Next, we observe 3, which is odd, hence the cumulative percentage of even outcomes is now 4 out of 66.67%, and so on.\nFinally, we plot the accumulated percentage of even numbers against the trial number:\n\nggplot(experiment, aes(x = trial, \n                       y = cumul_perc_even)) +\n    geom_point(color = 'darkolivegreen4') +\n    geom_line(color = 'darkolivegreen4') +\n    geom_hline(aes(yintercept = 50), color = 'red', linetype = 2) +\n    labs(x = \"Trial number\", y = \"Accumulated percent even\")\n\n\n\n\nAs the number of trials increase, we see that the curve approaches 50%, which is 0.5, and that the cumulative percentage of even outcomes keeps fluctuating around 50%.\n\n\n\n\n\n\nCheckpoint\n\n\n\nWhat’s the probability of obtaining an even number when throwing a fair die?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIf you answered 0.5 (or 50%), then you are on the right track!\n\n\n\nBased on the graph, it looks like the relative frequency of an even number settles down to about 50%, so saying that the probability is about 0.5 seems like a reasonable answer.\nBut do random experiments always behave well enough for this definition of probability to always apply? Perhaps the relative frequency of an event can bounce back and forth between two values forever, never settling on just one number?"
  },
  {
    "objectID": "rd1_07.html#the-law-of-large-numbers",
    "href": "rd1_07.html#the-law-of-large-numbers",
    "title": "Probability 1",
    "section": "\n5 The law of large numbers",
    "text": "5 The law of large numbers\nFortunately, Jacob Bernoulli proved the Law of large numbers (LLN) in the 18th century, giving us the peace of mind that we need.\n\n\n\n\n\n\nLaw of large numbers\nThe law of large numbers (LLN) states that as we repeat a random experiment over and over, the proportion of times that an event occurs does settle down to a single number. We call this number the probability of that event.\n\n\n\nHowever, it is not that simple. The LLN requires two key assumptions:\n\n\nIdentical distribution: The outcomes of the random experiment must have the same probabilities of occurring in each trial. That is, we can not have in trial 1 a 50% chance of observing an even number, and then a 80% chance in trial 2. The underlying chance needs to be the same. This is accomplished by not changing the random experiment we are studying, the repeated trials must happen in the same conditions.\n\n\nIndependence: The outcome of one trial must not affect the outcomes of other trials.\n\nFor the die experiment, we can now write that the probability of observing an even number is 0.5 as follows. First, we need to define the event of interest, \\(E = \\{2, 4, 6\\}\\), and then we can write: \\[\nP(E) = 0.5\n\\]\nIf you do not give a name to the event, you must specify it inside of the parentheses. Note the use of round parentheses for probability \\(P()\\) and the curly brackets to list the outcomes of interest. \\[\nP(\\{2, 4, 6\\}) = 0.5\n\\]\nWe reached this definition of the probability of the event \\(E\\). In the long run, \\[\nP(E) = \\frac{\\text{number of times outcome was in the event } E}{\\text{total number of trials}}\n\\]\n\n\n\n\n\n\nNotation\n\n\n\nWe typically use the first few capital letters of the alphabet to name events. The letter \\(P\\) will always be reserved for probability.\nWhen we write \\(P(A) = 0.5\\) we mean “the probability of the event \\(A\\) is 0.5”.\nWe use proportions (or decimal numbers) when reporting probability values in a formal situation like writing a report or a paper. However, when discussing probability informally, we often use percentages."
  },
  {
    "objectID": "rd1_07.html#law-of-averages-unicorn",
    "href": "rd1_07.html#law-of-averages-unicorn",
    "title": "Probability 1",
    "section": "\n6 Law of averages = unicorn",
    "text": "6 Law of averages = unicorn\nYou might have heard from friends or TV shows that sometimes the random experiment “owes” you a particular outcome. Let’s try to entangle this in more detail and understand where the pitfall of this reasoning is.\nThe law of large numbers tells us that the probability of an event is the proportion of times we would observe it in the long run. The long run is really long - infinitely long. We, as humans and finite entities, can not generate an infinitely long sequence of trials and memorise it.\nMany people believe that if you flip a fair coin, where fair means that the chance of getting heads is the same as the chance of getting tails (0.5) we expect the coin to “even out” the results in the coming trials if heads has not appeared in the recent ones.\nSay, for example, that in 10 trials you only observed 1 head. This is quite a low proportion, 0.1 (1 out of 10) compared to the 0.5 (5 out of 10) that the player expected. Does this mean that the coin due to show heads in the near future, as the coin “owes” us some heads to even out the proportions?\nThe answer is no.\nThe long run means that the proportions will eventually even out in the infinite sequence of trials, but you will not know when this happens and there is absolutely no requirement for the coin to show heads again in the upcoming trials in order to keep a probability of 0.5."
  },
  {
    "objectID": "rd1_07.html#modelling-probability",
    "href": "rd1_07.html#modelling-probability",
    "title": "Probability 1",
    "section": "\n7 Modelling probability",
    "text": "7 Modelling probability\nTo assign a probability value to different events, we should make sure that these coherence principles are satisfied:\nRule 1: Probability assignment rule\nThe probability of an impossible event (an event which never occurs) is 0 and the probability of a certain event (an event which always occurs) is 1.\nHence, we have that the probability is a number between 0 and 1: \\[\\text{for any event }A, \\\\ 0 \\leq P(A) \\leq 1\\]\nRule 2: Total probability rule\nIf an experiment has a single possible outcome, it is not random as that outcome will happen with certainty (i.e. probability 1).\nWhen dealing with two or more possible outcomes, we need to be sure to distribute the entire probability among all of the possible outcomes in the sample space \\(S\\).\nThe sample space must have probability 1: \\[P(S) = 1\\]\nIt must be that the will observe one of the outcomes listed within the collection of all possible outcomes of the experiment.\nRule 3: Complement rule\nIf the probability of observing the face “2” in a die is 1/6 = 0.17, what’s the probability of not observing the face “2”? It must be 1 - 1/6 = 5/6 = 0.83.\nIf \\(A = \\{2\\}\\), the event not A is written \\(\\sim A\\), which is a shortcut for \\(S\\) without \\(A\\), that is \\(\\{1, 3, 4, 5, 6\\}\\).\n\\[P(\\sim A) = 1 - P(A)\\]\nRule 4: Addition rule for disjoint events\nSuppose the probability that a randomly picked person in a town is \\(A\\) = “a high school student” is \\(P(A) = 0.3\\) and that the probability of being \\(B\\) = “a university student” is \\(P(B) = 0.5\\).\nWhat is the probability that a randomly picked person from the town is either a high school student or a university student? We write the event “either A or B” as \\(A \\cup B\\), pronounced “A union B”.\nIf you said 0.8, because it is 0.3 + 0.5, then you just applied the addition rule: \\[\n\\text{If }A \\text{ and } B \\text{ are mutually exclusive events,}\\\\\nP(A \\cup B) = P(A) + P(B)\n\\]\nRule 5: Multiplication rule for independent events\nWe saw that probability of observing an even number (\\(E\\)) when throwing a die is 0.5.\nYou also know that the probability of observing heads (\\(H\\)) when throwing a fair coin is 0.5.\nWhat’s the probability of observing an even number and heads (that is, \\(E\\) and \\(H\\), written \\(E \\cap H\\)) when throwing both items together?\nThe rule simply says that in this case we multiply the two probabilities together: 0.5 * 0.5 = 0.25.\nThe multiplication rule for independent events says: \\[\n\\text{If }A \\text{ and } B \\text{ are independent events,}\\\\\nP(A \\cap B) = P(A) \\times P(B)\n\\]\n\n7.1 Probability in the case of equally likely outcomes\nConsider a sample space of \\(n\\) outcomes\n\\[\nS = \\{s_1, s_2, \\dots, s_n \\}\n\\]\nand suppose these are all equally likely, with \\(p\\) denoting the probability of each outcome: \\[\nP(\\{s_1\\}) = P(\\{s_2\\}) = \\cdots = P(\\{s_n\\}) = p\n\\]\nAs the outcomes in the sample space are mutually exclusive events,1 we can compute the probability of the sample space as\n\\[\n1 = P(S) = P(\\{s_1\\}) + P(\\{s_2\\}) + \\cdots + P(\\{s_n\\}) = p + p + \\cdots + p = n p\n\\]\nwhich leads to \\[\np = P(\\{s_i\\}) = \\frac{1}{n}\n\\]\nNext, consider an event \\(A\\) comprising a few of the outcomes from \\(S\\)\n\\[\nA = \\{s_2, s_5, s_9\\}\n\\] which can be also written as the union of disjoint events\n\\[\nA = \\{s_2\\} \\cup \\{s_5\\} \\cup \\{s_9\\}\n\\]\nWe can compute the probability of \\(A\\) as follows\n\\[\n\\begin{aligned}\nP(A) &= P(\\{s_2\\}) + P(\\{s_5\\}) + P(\\{s_9\\}) \\\\\n&= p + p + p \\\\\n&= 3 p \\\\\n&= 3 \\left(\\frac{1}{n}\\right) \\\\\n&= \\frac{3}{n} \\\\\n&= \\frac{n_A}{n} \\\\\n&= \\frac{\\text{number of outcomes within }A}{\\text{number of possible outcomes}}\n\\end{aligned}\n\\] where \\(n_A\\) is the number of outcomes within \\(A\\) and \\(n\\) is the total number of possible outcomes in \\(S\\)."
  },
  {
    "objectID": "rd1_07.html#glossary",
    "href": "rd1_07.html#glossary",
    "title": "Probability 1",
    "section": "\n8 Glossary",
    "text": "8 Glossary\n\n\nRandom experiment. A process or phenomenon which can have two ore more possible outcomes.\n\nTrial. A single repetition of the experiment\n\nOutcome. The value observed after running a trial\n\nSample space. The collection of all possible outcomes. The sample space is denoted \\(S\\) and has probability 1.\n\nEvent. Either a single outcome (simple event) or a collection of outcomes (compound event).\n\nProbability. A number reporting how likely is an even to occur when performing a trial (that is, obtaining an outcome within that event or satisfying the proposition of the event)\n\nDisjoint (or mutually exclusive) events. Two events \\(A\\) and \\(B\\) are disjoint if they share no outcomes in common. For disjoint events, knowing that one event occurs tells us that the other cannot occur.\n\nIndependent events. Two events are independent if learning that one event occurs does not change the probability that the other event occurs.\n\nProbability assignment rule. Says that the probability of any event must be between 0 (the probability of an impossible event) and 1 (the probability of a certain event).\n\nTotal probability rule. The probability that the experiment’s outcome is in the sample space must be 1.\n\nComplement rule. If \\(P(A)\\) denotes the probability of the event \\(A\\) occurring, the probability of “not A” is \\(P(\\sim A) = 1 - P(A)\\).\n\nAddition rule for disjoint events. If \\(A\\) and \\(B\\) are disjoint events, then \\(P(A \\cup B) = P(A) + P(B)\\).\n\nMultiplication rule for independent events. If A and B are independent events, then \\(P(A \\cap B) = P(A) \\times P(B)\\)."
  },
  {
    "objectID": "rd1_07.html#footnotes",
    "href": "rd1_07.html#footnotes",
    "title": "Probability 1",
    "section": "Footnotes",
    "text": "Footnotes\n\nIn the die example, 1 has nothing in common with 2, 2 has nothing in common with 3, and so on.↩︎"
  },
  {
    "objectID": "rd1_04.html",
    "href": "rd1_04.html",
    "title": "Visualising and describing relationships",
    "section": "",
    "text": "In the previous couple of weeks, we looked at how to handle different types of data, and how to describe and visualise categorical and numeric distributions. More often than not, research involves investigating relationships between variables, rather than studying variables in isolation.\n\n\n\n\n\n\nIf we are using one variable to help us understand or predict values of another variable, we call the former the explanatory variable and the latter the outcome variable.\nOther names\n\noutcome variable = dependent variable = response variable = Y\nexplanatory variable = independent variable = predictor variable = X\n\n(referring to outcome/explanatory variables as Y and X respectively matches up with how we often want to plot them - the outcome variable on the y-axis, and the explanatory variable on the x-axis)\n\n\n\nThe distinction between explanatory and outcome variables is borne out in how we design experimental studies - the researcher manipulates the explanatory variable for each unit before the response variable is measured (for instance, we might randomly allocate participants to one of two conditions). This contrasts with observational studies in which the researcher does not control the value of any variable, but simply observes the values as they naturally exist.\nWe’re going to use data from a Stroop task.\n\nThe data we are going to use for these exercises is from an experiment using one of the best known tasks in psychology, the “Stroop task”. 130 participants completed an online task in which they saw two sets of coloured words. Participants spoke out loud the colour of each word, and timed how long it took to complete each set. In the first set of words, the words matched the colours they were presented in (e.g., word “blue” was coloured blue). In the second set of words, the words mismatched the colours (e.g., the word “blue” was coloured red, see Figure @ref(fig:stroop)). Participants’ recorded their times for each set (matching and mismatching).\nParticipants were randomly assigned to either do the task once only, or to record their times after practicing the task twice.\nYou can try out the experiment at https://faculty.washington.edu/chudler/java/ready.html.\nThe data is available at https://uoepsy.github.io/data/strooptask.csv\n\n\n\n\nStroop Task - Color word interference. Images from https://faculty.washington.edu/chudler/java/ready.html\n\n\n\n\nlibrary(tidyverse)\nstroopdata &lt;- read_csv(\"https://uoepsy.github.io/data/strooptask.csv\")\n\n# calculate the \"stroop effect\" - the difference in time taken to complete\n# the matching vs mismatching sets \nstroopdata &lt;- \n    stroopdata %&gt;% \n        mutate(\n            stroop_effect = mismatching - matching\n        )\n\nThe data is experimental - researchers controlled the presentation of the stimuli (coloured words) and the assignment of whether or not participants received practice.\nThe researchers are interested in two relationships:\n\nthe relationship between receiving practice (categorical) and the stroop-effect (numeric)\nthe relationship between age (numeric) and the stroop-effect (numeric)"
  },
  {
    "objectID": "rd1_04.html#outcome-vs-explanatory",
    "href": "rd1_04.html#outcome-vs-explanatory",
    "title": "Visualising and describing relationships",
    "section": "",
    "text": "In the previous couple of weeks, we looked at how to handle different types of data, and how to describe and visualise categorical and numeric distributions. More often than not, research involves investigating relationships between variables, rather than studying variables in isolation.\n\n\n\n\n\n\nIf we are using one variable to help us understand or predict values of another variable, we call the former the explanatory variable and the latter the outcome variable.\nOther names\n\noutcome variable = dependent variable = response variable = Y\nexplanatory variable = independent variable = predictor variable = X\n\n(referring to outcome/explanatory variables as Y and X respectively matches up with how we often want to plot them - the outcome variable on the y-axis, and the explanatory variable on the x-axis)\n\n\n\nThe distinction between explanatory and outcome variables is borne out in how we design experimental studies - the researcher manipulates the explanatory variable for each unit before the response variable is measured (for instance, we might randomly allocate participants to one of two conditions). This contrasts with observational studies in which the researcher does not control the value of any variable, but simply observes the values as they naturally exist.\nWe’re going to use data from a Stroop task.\n\nThe data we are going to use for these exercises is from an experiment using one of the best known tasks in psychology, the “Stroop task”. 130 participants completed an online task in which they saw two sets of coloured words. Participants spoke out loud the colour of each word, and timed how long it took to complete each set. In the first set of words, the words matched the colours they were presented in (e.g., word “blue” was coloured blue). In the second set of words, the words mismatched the colours (e.g., the word “blue” was coloured red, see Figure @ref(fig:stroop)). Participants’ recorded their times for each set (matching and mismatching).\nParticipants were randomly assigned to either do the task once only, or to record their times after practicing the task twice.\nYou can try out the experiment at https://faculty.washington.edu/chudler/java/ready.html.\nThe data is available at https://uoepsy.github.io/data/strooptask.csv\n\n\n\n\nStroop Task - Color word interference. Images from https://faculty.washington.edu/chudler/java/ready.html\n\n\n\n\nlibrary(tidyverse)\nstroopdata &lt;- read_csv(\"https://uoepsy.github.io/data/strooptask.csv\")\n\n# calculate the \"stroop effect\" - the difference in time taken to complete\n# the matching vs mismatching sets \nstroopdata &lt;- \n    stroopdata %&gt;% \n        mutate(\n            stroop_effect = mismatching - matching\n        )\n\nThe data is experimental - researchers controlled the presentation of the stimuli (coloured words) and the assignment of whether or not participants received practice.\nThe researchers are interested in two relationships:\n\nthe relationship between receiving practice (categorical) and the stroop-effect (numeric)\nthe relationship between age (numeric) and the stroop-effect (numeric)"
  },
  {
    "objectID": "rd1_04.html#numeric-and-categorical",
    "href": "rd1_04.html#numeric-and-categorical",
    "title": "Visualising and describing relationships",
    "section": "\n2 Numeric and Categorical",
    "text": "2 Numeric and Categorical\nRecall that the “stroop-effect” is the difference (in seconds) between participants’ times on the mismatching set of words vs the matching set. We know how to describe a numeric variable such as the stroop-effect, for instance by calculating the mean and standard deviation, or median and IQR. We saw how to produce visualisations of numeric variables in the form of density curves, histogram, and boxplots.\n\n# take the \"stroopdata\" dataframe %&gt;%\n# summarise() it, such that there is a value called \"mean_stroop\", which\n# is the mean() of the \"stroop_effect\" variable, and a value called \"sd_stroop\", which\n# is the standard deviation of the \"stroop_effect\" variable.\nstroopdata %&gt;% \n  summarise(\n    mean_stroop = mean(stroop_effect),\n    sd_stroop = sd(stroop_effect)\n  )\n\n# A tibble: 1 × 2\n  mean_stroop sd_stroop\n        &lt;dbl&gt;     &lt;dbl&gt;\n1        2.40      5.02\n\n\n\nggplot(data = stroopdata, aes(x = stroop_effect)) + \n  geom_histogram()\n\n\n\n\nTo understand the relationship between categorical (practice) and the numeric (stroop effect), for now we will simply calculate these summary statistics for the numeric variable when it is split by the different levels in the categorical variable.\nIn other words, we want to calculate the mean and standard deviation of the stroop_effect variable separately for those observations where practice is “no”, and for those where practice is “yes”:\n\n\n\n\npractice\nstroop_effect\n\n\n\nno\n9.69\n\n\nno\n10.07\n\n\nyes\n-2.97\n\n\nyes\n-0.23\n\n\nyes\n-5.59\n\n\nno\n3.67\n\n\nyes\n1.41\n\n\nyes\n2.1\n\n\nyes\n-0.33\n\n\n…\n…\n\n\n\n\n\nWe can do this using the group_by() function.\n\n\n\n\n\n\ngroup_by()\nThe group_by() function creates a grouping in the dataframe, so that subsequent functions will be computed on each group.\nIt is most useful in combination with summarise(), to reduce a variable into a summary value for each group in a grouping variable:\n\n# take the data %&gt;%\n# make it grouped by each unique value in the \"grouping_variable\" %&gt;%\n# summarise() it FOR EACH GROUP, creating a value called \"summary_value\" ()\ndata %&gt;% \n  group_by(grouping_variable) %&gt;%\n  summarise(\n    summary_value = ...\n  )\n\n\n\n\nLet’s do this for the Stroop Task data - we will summarise() the stroop_effect variable, after grouping the data by the practice variable:\n\n# take the \"stroopdata\" %&gt;%\n# and group it by each unique value in the \"practice\" variable (yes/no) %&gt;%\n# then summarise() it FOR EACH GROUP, creating summary values called \n# \"mean_stroop\" and \"sd_stroop\" which are the means and standard deviations of \n# the \"stroop_effect\" variable entries for each group of \"practice\".\nstroopdata %&gt;%\n  group_by(practice) %&gt;%\n  summarise(\n    mean_stroop = mean(stroop_effect),\n    sd_stroop = sd(stroop_effect)\n  )\n\n# A tibble: 2 × 3\n  practice mean_stroop sd_stroop\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 no            4.54        4.25\n2 yes           0.0229      4.75\n\n\nVisualising - Colours\nGiven the output above, which of the following visualisations is most representative of these statistics?\n\n\n\n\n\n\n\n\n\n\n\nWe know that the stroop effect for those with practice (blue line) was on average less than those without practice (red line). Both figures A and C don’t fit with this.\nIn both of the figures B and D, the blue (with practice) distribution peaks at about 0, and the red (without practice) distribution peaks at about 5. However, in the figure D, the red distribution is much flatter and wider. It has a larger standard deviation than the blue distribution. In our calculations above, the distributions have very similar standard deviations.\nSo the best visualisation of the two means and standard deviations we calculated is figure B.\n\n\n\nWe can visualise the data using the same code we had before, but with one small addition - we tell ggplot to colour the data according to the different values in the practice variable.\nNote we add this inside the aes() mappings, because we are mapping something on the plot (the colour) to something in the data (the practice variable). If we just wanted to make the line blue, we could put col = \"blue\" outside the aes().\n\nggplot(data = stroopdata, aes(x = stroop_effect, col = practice)) +\n  geom_density()\n\n\n\n\nVisualising - Facets\nInterpreting two density curves on top of one another works well, but overlaying two histograms on top of one another doesn’t. Instead, we might want to create separate histograms for each set of values (the stroop_effect variable values for each of practice/no practice groups).\nfacet_wrap() is a handy part of ggplot which allows us to easily split one plot into many:\n\nggplot(data = stroopdata, aes(x = stroop_effect)) +\n  geom_histogram() +\n  facet_wrap(~practice)"
  },
  {
    "objectID": "rd1_04.html#numeric-and-numeric",
    "href": "rd1_04.html#numeric-and-numeric",
    "title": "Visualising and describing relationships",
    "section": "\n3 Numeric and Numeric",
    "text": "3 Numeric and Numeric\nWhen we are interested in the relationship between two numeric variables, such as the one we have between age and the stroop-effect, the most easily interpreted visualisation of this relationship is in the form of a scatterplot:\n\n# make a ggplot with the stroopdata\n# put the possible values of the \"age\" variable on the x axis,\n# and put the possible values of the \"stroop_effect\" variable on the y axis.\n# for each entry in the data, add a \"tomato1\" coloured geom_point() to the plot, \nggplot(data = stroopdata, aes(x = age, y = stroop_effect)) +\n  geom_point(col=\"tomato1\")\n\n\n\n\nThe visual pattern that these points make on the plot tells us something about the data - it looks like the older participants tended to have a greater stroop-effect.\nBut we can also have relationships between two numeric variables that look the opposite, or have no obvious pattern, or have a more consistent patterning (see Figure @ref(fig:numnumrels))\n\n\n\n\nRelationships between two numeric variables can look very different\n\n\n\nAs a means of summarising these different types of relationships, we can calculate the covariance to describe in what direction, and how strong (i.e., how clear and consistent) the pattern is.\nCovariance\nWe know that variance is the measure of how much a single numeric variable varies around its mean.Covariance is a measure of how two numeric variables vary together, and can express the directional relationship between them.\n\n\n\n\n\n\nCovariance is the measure of how two variables vary together.\nFor samples, covariance is calculated using the following formula:\n\\[\\mathrm{cov}(x,y)=\\frac{1}{n-1}\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})\\]\nwhere:\n\n\n\\(x\\) and \\(y\\) are two variables;\n\n\\(i\\) denotes the observational unit, such that \\(x_i\\) is value that the \\(x\\) variable takes on the \\(i\\)th observational unit, and similarly for \\(y_i\\);\n\n\\(n\\) is the sample size.\n\n\n\n\nIt often helps to understand covariance by working through a visual explanation. Consider the following scatterplot:\n\n\n\n\n\n Now let’s superimpose a vertical dashed line at the mean of \\(x\\) (\\(\\bar{x}\\)) and a horizontal dashed line at the mean of \\(y\\) (\\(\\bar{y}\\)):\n\n\n\n\n\n Now let’s pick one of the points, call it \\(x_i\\), and show \\((x_{i}-\\bar{x})\\) and \\((y_{i}-\\bar{y})\\).\nNotice that this makes a rectangle.\nAs \\((x_{i}-\\bar{x})\\) and \\((y_{i}-\\bar{y})\\) are both positive values, their product - \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) - is positive.\n\n\n\n\n\n In fact, for all these points in red, the product \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) is positive (remember that a negative multiplied by a negative gives a positive):\n\n\n\n\n\n And for these points in blue, the product \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) is negative:\n\n\n\n\n\n Now take another look at the formula for covariance:\n\\[\\mathrm{cov}(x,y)=\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1}\\]\nIt is the sum of all these products divided by \\(n-1\\). It is the average of the products! We can easily calculate the covariance between variables in R using the cov() function. cov() takes two variables cov(x = , y = ).\nWe can either use the $ to pull out the variables from the datset:\n\ncov(stroopdata$age, stroopdata$stroop_effect)\n\n[1] 23.9597\n\n\nOr we can specify the dataframe, use the %&gt;% symbol, and call cov() inside summarise():\n\nstroopdata %&gt;%\n  summarise(\n    mean_age = mean(age),\n    mean_stroop = mean(stroop_effect),\n    cov_agestroop = cov(age, stroop_effect)\n  )\n\n# A tibble: 1 × 3\n  mean_age mean_stroop cov_agestroop\n     &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n1     42.8        2.40          24.0"
  },
  {
    "objectID": "rd1_04.html#categorical-and-categorical",
    "href": "rd1_04.html#categorical-and-categorical",
    "title": "Visualising and describing relationships",
    "section": "\n4 Categorical and Categorical",
    "text": "4 Categorical and Categorical\nWhat if we are interested in the relationship between two variables that are both categorical?\nAs a quick example, let’s read in a dataset containing information on passengers from the Titanic. We can see from the first few rows of the dataset that there are quite a few categorical variables here:\n\ntitanic &lt;- read_csv(\"https://uoepsy.github.io/data/titanic.csv\")\nhead(titanic)\n\n# A tibble: 6 × 5\n   ...1 class     age    sex   survived\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   \n1     1 1st class adults man   yes     \n2     2 1st class adults man   yes     \n3     3 1st class adults man   yes     \n4     4 1st class adults man   yes     \n5     5 1st class adults man   yes     \n6     6 1st class adults man   yes     \n\n\nRecall that we summarised one categorical variable using a frequency table:\n\ntitanic %&gt;%\n  count(survived)\n\n# A tibble: 2 × 2\n  survived     n\n  &lt;chr&gt;    &lt;int&gt;\n1 no         817\n2 yes        499\n\n\nWe can also achieve this using the table() function:\n\n#thes two lines of code do exactly the same thing!\ntable(titanic$survived)\ntitanic %&gt;% select(survived) %&gt;% table()\n\n\n\n\n no yes \n817 499 \n\n\nContingency Tables\nLet’s suppose we are interested in how the Class of passengers’ tickets (1st Class, 2nd Class, 3rd Class) can be used to understand their survival.\nWe can create two-way table, where we have each variable on either dimension of the table:\n\n#Either this:\ntable(titanic$class, titanic$survived)\n# or this:\ntitanic %&gt;% select(class, survived) %&gt;% table()\n\n\n\n           \n             no yes\n  1st class 122 203\n  2nd class 167 118\n  3rd class 528 178\n\n\nAnd we can pass this to prop.table() to turn these into proportions.\nWe can turn them into:\n\nproportions of the total:\n\n\ntitanic %&gt;% \n    select(class, survived) %&gt;% \n    table() %&gt;%\n    prop.table()\n\n           survived\nclass               no        yes\n  1st class 0.09270517 0.15425532\n  2nd class 0.12689970 0.08966565\n  3rd class 0.40121581 0.13525836\n\n\n\nproportions of each row:\n\n\ntitanic %&gt;% \n    select(class, survived) %&gt;% \n    table() %&gt;%\n    prop.table(margin = 1)\n\n           survived\nclass              no       yes\n  1st class 0.3753846 0.6246154\n  2nd class 0.5859649 0.4140351\n  3rd class 0.7478754 0.2521246\n\n\n\nproportions of each column:\n\n\ntitanic %&gt;% \n    select(class, survived) %&gt;% \n    table() %&gt;%\n    prop.table(margin = 2)\n\n           survived\nclass              no       yes\n  1st class 0.1493268 0.4068136\n  2nd class 0.2044064 0.2364729\n  3rd class 0.6462668 0.3567134\n\n\nMosaic Plots\nThe equivalent way to visualise a contingency table is in the form of a mosaic plot.\n\ntitanic %&gt;% \n  select(class, survived) %&gt;% \n  table() %&gt;%\n  plot()\n\n\n\n\nYou can think of the prop.table(margin = ) as scaling the areas of one of the variables to be equal:\n\ntitanic %&gt;% \n  select(class, survived) %&gt;% \n  table() %&gt;%\n  prop.table(margin = 1)\n\n           survived\nclass              no       yes\n  1st class 0.3753846 0.6246154\n  2nd class 0.5859649 0.4140351\n  3rd class 0.7478754 0.2521246\n\n\nIn the table above, each row (representing each level of the “Class” variable) sums to 1. The equivalent plot would make each of level of the “Class” variable as the same area:\n\ntitanic %&gt;% \n  select(class, survived) %&gt;% \n  table() %&gt;%\n  prop.table(margin = 1) %&gt;%\n  plot()"
  },
  {
    "objectID": "rd1_04.html#glossary",
    "href": "rd1_04.html#glossary",
    "title": "Visualising and describing relationships",
    "section": "\n5 Glossary",
    "text": "5 Glossary\n\n\nExplanatory variable: A variable used to understand or predict values of an outcome variable.\n\nOutcome variable: A variable which we are aiming to understand or predict via some explanatory variable(s).\n\nScatterplot: A plot in which the values of two variables are plotted along the two axes, the pattern of the resulting points revealing any relationship which is present.\n\nCovariance: A measure of the extent to which two variables vary together. \n\n\ncov() To calculate the covariance between two variables.\n\ngroup_by() To apply a grouping in a dataframe for each level of a given variable. Grouped dataframes will retain their grouping, so that if we use summarise() it will provide a summary calculation for each group.\n\ngeom_point() To add points/dots to a ggplot.\n\nfacet_wrap() To split a ggplot into multiple plots (facets) for each level of a given variable."
  },
  {
    "objectID": "rd1_02.html",
    "href": "rd1_02.html",
    "title": "Categorical data",
    "section": "",
    "text": "Before we get started on the statistics, we’re going to briefly introduce a crucial bit of R code. We have seen already seen a few examples of R code such as:\n\n# show the dimensions of the data\ndim(somedata)\n\n# show a summary of the data\nsummary(somedata)\n\n# factorise and show the \"somevariable\" variable in the \"somedata\" dataframe \nas.factor(somedata$somevariable)\n\nAnd we can actually wrap functions inside functions:\n\n# factorise the \"somevariable\" variable in the \"somedata\" dataframe, \n# then show a summary of it\nsummary(as.factor(somedata$somevariable))\n\nR evaluates code from the inside-out!\nYou can end up with functions inside functions inside functions …\n\n# Don't worry about what all these functions do, \n# it's just an example -\nround(mean(log(cumsum(diff(1:10)))))\n\n[1] 1\n\n\nWe can write in a different style, however, and this may help to keep code tidy and easily readable - we can write sequentially:\n\nNotice that what we are doing is using a new symbol: %&gt;%\nThis symbol takes the output of whatever is on it’s left-hand side, and uses it as an input for whatever is on the right-hand side. The %&gt;% symbol gets called a “pipe”.\nLet’s see it in action with the starwars2 dataset. The data contains information on various characteristics of characters from Star Wars. Before we can use the pipe operator, %&gt;%, we need to load the tidyverse packages, because that is where %&gt;% is found.\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\nstarwars2 &lt;- read_csv(\"https://uoepsy.github.io/data/starwars2.csv\")\nstarwars2 %&gt;%\n    head()\n\n# A tibble: 6 × 6\n  name           height hair_color  eye_color homeworld species\n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  \n1 Luke Skywalker    172 blond       blue      Tatooine  Human  \n2 C-3PO             167 &lt;NA&gt;        yellow    Tatooine  Human  \n3 R2-D2              96 &lt;NA&gt;        red       Naboo     Droid  \n4 Darth Vader       202 none        yellow    Tatooine  Human  \n5 Leia Organa       150 brown       brown     Alderaan  Human  \n6 Owen Lars         178 brown, grey blue      Tatooine  Human  \n\nstarwars2 %&gt;%\n    summary()\n\n     name               height       hair_color         eye_color        \n Length:75          Min.   : 79.0   Length:75          Length:75         \n Class :character   1st Qu.:167.5   Class :character   Class :character  \n Mode  :character   Median :180.0   Mode  :character   Mode  :character  \n                    Mean   :176.1                                        \n                    3rd Qu.:191.0                                        \n                    Max.   :264.0                                        \n  homeworld           species         \n Length:75          Length:75         \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nWe can now write code that requires reading it from the inside-out:\n\nsummary(as.factor(starwars2$homeworld))\n\nor which requires reading it from left to right:\n\nstarwars2$homeworld %&gt;%\n    as.factor() %&gt;%\n    summary()\n\n      Alderaan    Aleen Minor         Bespin     Bestine IV Cato Neimoidia \n             3              1              1              1              1 \n         Cerea       Champala      Chandrila   Concord Dawn       Corellia \n             1              1              1              1              2 \n     Coruscant       Dathomir          Dorin          Endor         Eriadu \n             3              1              1              1              1 \n      Geonosis    Glee Anselm     Haruun Kal        Iktotch       Iridonia \n             1              1              1              1              1 \n         Kalee         Kamino       Kashyyyk      Malastare         Mirial \n             1              3              2              1              2 \n      Mon Cala     Muunilinst          Naboo      Nal Hutta           Ojom \n             1              1              8              1              1 \n       Quermia          Rodia         Ryloth        Serenno          Shili \n             1              1              2              1              1 \n         Skako        Socorro    Springfield        Stewjon        Sullust \n             1              1              2              1              1 \n      Tatooine       Toydaria      Trandosha        Troiken           Tund \n            10              1              1              1              1 \n        Utapau        Vulpter          Zolan \n             1              1              1 \n\n\nAnd that long line of code from above:\n\n# again, don't worry about all these functions, \n# just notice the difference in the two styles.\nround(mean(log(cumsum(diff(1:10)))))\n\nbecomes:\n\n1:10 %&gt;%\n    diff() %&gt;%\n    cumsum() %&gt;%\n    log() %&gt;%\n    mean() %&gt;%\n    round()\n\nWe’re going to use this way of writing a lot throughout the course, and it pairs really well with a group of functions in the tidyverse packages, which were designed to be used in conjunction with %&gt;%."
  },
  {
    "objectID": "rd1_02.html#a-different-style-of-r-code",
    "href": "rd1_02.html#a-different-style-of-r-code",
    "title": "Categorical data",
    "section": "",
    "text": "Before we get started on the statistics, we’re going to briefly introduce a crucial bit of R code. We have seen already seen a few examples of R code such as:\n\n# show the dimensions of the data\ndim(somedata)\n\n# show a summary of the data\nsummary(somedata)\n\n# factorise and show the \"somevariable\" variable in the \"somedata\" dataframe \nas.factor(somedata$somevariable)\n\nAnd we can actually wrap functions inside functions:\n\n# factorise the \"somevariable\" variable in the \"somedata\" dataframe, \n# then show a summary of it\nsummary(as.factor(somedata$somevariable))\n\nR evaluates code from the inside-out!\nYou can end up with functions inside functions inside functions …\n\n# Don't worry about what all these functions do, \n# it's just an example -\nround(mean(log(cumsum(diff(1:10)))))\n\n[1] 1\n\n\nWe can write in a different style, however, and this may help to keep code tidy and easily readable - we can write sequentially:\n\nNotice that what we are doing is using a new symbol: %&gt;%\nThis symbol takes the output of whatever is on it’s left-hand side, and uses it as an input for whatever is on the right-hand side. The %&gt;% symbol gets called a “pipe”.\nLet’s see it in action with the starwars2 dataset. The data contains information on various characteristics of characters from Star Wars. Before we can use the pipe operator, %&gt;%, we need to load the tidyverse packages, because that is where %&gt;% is found.\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\nstarwars2 &lt;- read_csv(\"https://uoepsy.github.io/data/starwars2.csv\")\nstarwars2 %&gt;%\n    head()\n\n# A tibble: 6 × 6\n  name           height hair_color  eye_color homeworld species\n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  \n1 Luke Skywalker    172 blond       blue      Tatooine  Human  \n2 C-3PO             167 &lt;NA&gt;        yellow    Tatooine  Human  \n3 R2-D2              96 &lt;NA&gt;        red       Naboo     Droid  \n4 Darth Vader       202 none        yellow    Tatooine  Human  \n5 Leia Organa       150 brown       brown     Alderaan  Human  \n6 Owen Lars         178 brown, grey blue      Tatooine  Human  \n\nstarwars2 %&gt;%\n    summary()\n\n     name               height       hair_color         eye_color        \n Length:75          Min.   : 79.0   Length:75          Length:75         \n Class :character   1st Qu.:167.5   Class :character   Class :character  \n Mode  :character   Median :180.0   Mode  :character   Mode  :character  \n                    Mean   :176.1                                        \n                    3rd Qu.:191.0                                        \n                    Max.   :264.0                                        \n  homeworld           species         \n Length:75          Length:75         \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nWe can now write code that requires reading it from the inside-out:\n\nsummary(as.factor(starwars2$homeworld))\n\nor which requires reading it from left to right:\n\nstarwars2$homeworld %&gt;%\n    as.factor() %&gt;%\n    summary()\n\n      Alderaan    Aleen Minor         Bespin     Bestine IV Cato Neimoidia \n             3              1              1              1              1 \n         Cerea       Champala      Chandrila   Concord Dawn       Corellia \n             1              1              1              1              2 \n     Coruscant       Dathomir          Dorin          Endor         Eriadu \n             3              1              1              1              1 \n      Geonosis    Glee Anselm     Haruun Kal        Iktotch       Iridonia \n             1              1              1              1              1 \n         Kalee         Kamino       Kashyyyk      Malastare         Mirial \n             1              3              2              1              2 \n      Mon Cala     Muunilinst          Naboo      Nal Hutta           Ojom \n             1              1              8              1              1 \n       Quermia          Rodia         Ryloth        Serenno          Shili \n             1              1              2              1              1 \n         Skako        Socorro    Springfield        Stewjon        Sullust \n             1              1              2              1              1 \n      Tatooine       Toydaria      Trandosha        Troiken           Tund \n            10              1              1              1              1 \n        Utapau        Vulpter          Zolan \n             1              1              1 \n\n\nAnd that long line of code from above:\n\n# again, don't worry about all these functions, \n# just notice the difference in the two styles.\nround(mean(log(cumsum(diff(1:10)))))\n\nbecomes:\n\n1:10 %&gt;%\n    diff() %&gt;%\n    cumsum() %&gt;%\n    log() %&gt;%\n    mean() %&gt;%\n    round()\n\nWe’re going to use this way of writing a lot throughout the course, and it pairs really well with a group of functions in the tidyverse packages, which were designed to be used in conjunction with %&gt;%."
  },
  {
    "objectID": "rd1_02.html#data-exploration",
    "href": "rd1_02.html#data-exploration",
    "title": "Categorical data",
    "section": "\n2 Data Exploration",
    "text": "2 Data Exploration\nOnce we have collected some data, one of the first things we want to do is explore it - and we can do this through describing (or summarising) and visualising variables.\nWe are already familiar with the function summary(), which provides high-level information about our data, showing us things such as the minimum and maximum and mean of continuous variables, or the numbers of entries falling into each possible response level for a categorical variable:\n\nsummary(starwars2)\n\n     name               height       hair_color         eye_color        \n Length:75          Min.   : 79.0   Length:75          Length:75         \n Class :character   1st Qu.:167.5   Class :character   Class :character  \n Mode  :character   Median :180.0   Mode  :character   Mode  :character  \n                    Mean   :176.1                                        \n                    3rd Qu.:191.0                                        \n                    Max.   :264.0                                        \n  homeworld           species         \n Length:75          Length:75         \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nWhat we are doing here is providing numeric descriptions of the distributions of values in each variable.\n\n\n\n\n\n\nDistribution\nThe distribution of a variable shows how often different values occur. We’re going to focus on describing and visualising distributions of categorical data.\nThe graph showing the distribution of a variable shows us where the values are centred, how the values vary, and gives some information about where a typical value might fall. It can also alert you to the presence of outliers (unexpected observations)."
  },
  {
    "objectID": "rd1_02.html#unordered-categorical-nominal-data",
    "href": "rd1_02.html#unordered-categorical-nominal-data",
    "title": "Categorical data",
    "section": "\n3 Unordered Categorical (Nominal) Data",
    "text": "3 Unordered Categorical (Nominal) Data\nFor variables with a discrete number of response options, we can easily measure “how often” values occur in terms of their frequency.\n\n\n\n\n\n\nFrequency distribution\nA frequency distribution is an overview of all distinct values in some variable and the number of times they occur.\n\n\n\nSupposing that we have surveyed the people working in a psychology department and asked them what sub-discipline of psychological research they most strongly identify as working within (If you would like to work along with the reading, the data is available at https://uoepsy.github.io/data/psych_survey.csv).\n\n\nVariable Name\nDescription\n\n\n\nparticipant\nSubject identifier\n\n\narea\nRespondent’s sub-discpline of psychology\n\n\n\n First, we read our data in to R and store it in an object called “psych_disciplines”:\n\npsych_disciplines &lt;- read_csv(\"https://uoepsy.github.io/data/psych_survey.csv\")\npsych_disciplines\n\n# A tibble: 74 × 2\n   participant   area                  \n   &lt;chr&gt;         &lt;chr&gt;                 \n 1 respondent_1  Differential          \n 2 respondent_2  Social                \n 3 respondent_3  Differential          \n 4 respondent_4  Social                \n 5 respondent_5  Differential          \n 6 respondent_6  Differential          \n 7 respondent_7  Language              \n 8 respondent_8  Language              \n 9 respondent_9  Cognitive Neuroscience\n10 respondent_10 Language              \n# ℹ 64 more rows\n\n\nWe can get the frequencies of different response levels of the discipline variable by using the following code:\n\n# start with the psych_disciplines dataframe \n# %&gt;%\n# count() the values in the \"area\" variable \npsych_disciplines %&gt;%\n    count(area)\n\n# A tibble: 5 × 2\n  area                       n\n  &lt;chr&gt;                  &lt;int&gt;\n1 Cognitive Neuroscience    24\n2 Developmental             10\n3 Differential              20\n4 Language                   9\n5 Social                    11\n\n\n\n\n\n\n\n\nExtra detail on how this works\n\n\n\n\n\nIn the code above, R knows to look for the area variable inside the psych_disciplines data because we used %&gt;% to “pipe” in the psych_disciplines dataframe.\nWe could have also done:\n\n# count(data, variable)\ncount(psych_disciplines, area)\n\n# A tibble: 5 × 2\n  area                       n\n  &lt;chr&gt;                  &lt;int&gt;\n1 Cognitive Neuroscience    24\n2 Developmental             10\n3 Differential              20\n4 Language                   9\n5 Social                    11\n\n\nBut this would not work:\n\ncount(area)\n\n\nError in group_vars(x) : object ‘area’ not found\n\n\n\n\nFrequency table\nTo describe a distribution like this, we can simply provide the frequency table.\nLet’s store it as an object in R:\n\n# make a new object called \"freq_table\", and assign it:\n# the counts of values of \"area\" variable in \n# the psych_discipline dataframe.\nfreq_table &lt;- \n    psych_disciplines %&gt;%\n    count(area)\n\n# show the object called \"freq_table\"\nfreq_table\n\n# A tibble: 5 × 2\n  area                       n\n  &lt;chr&gt;                  &lt;int&gt;\n1 Cognitive Neuroscience    24\n2 Developmental             10\n3 Differential              20\n4 Language                   9\n5 Social                    11\n\n\nFor a report, we might want to make it a little more easily readable:\nCentral tendency\nOften, we might want to summarise data into a single summary value, reflecting the point at (or around) which most of the values tend to cluster. This is known as a measure of central tendency. For numeric data, we can use measures such as the mean, which you will likely have heard of. For nominal data (unordered categorical data), however, our only option is to use the mode.\n\n\n\n\n\n\nMode\nThe most frequent value (the value that occurs the greatest number of times).\n\n\n\nIn our case, the mode is the “Cognitive Neuroscience” category.\nRelative frequencies\nWe might alternatively want to show the percentage of respondents in each category, rather than the raw frequencies.\nThe percentages show the relative frequency distribution\n\n\n\n\n\n\nRelative frequency distribution\nA relative frequency distribution shows the proportion of times each value occurs\n(contrast this with the frequency distribution which shows the number of times).\nRelative frequencies can be written as fractions, percents, or decimals.\n\n\n\nIn the object “freq_table”, we have a variable called n, which is the frequencies (the number in each category).\nThe total of this column is equal to the total number of respondents:\n\n# sum all the values in the \"n\" variable in the \"freq_table\" object\nsum(freq_table$n)\n\n[1] 74\n\n\nAnd therefore, each value in freq_table$n, divided by the total, is equal to the proportion in each category:\n( Tip: Proportions are percentages/100. So 0.4 is another way of expressing 40%)\n\n# take the values in the \"n\" variable from the \"freq_table\" object, \n# and divide by the sum of all the values in the \"n\" variable in \"freq_table\"\nfreq_table$n/sum(freq_table$n)\n\n[1] 0.3243243 0.1351351 0.2702703 0.1216216 0.1486486\n\n\nWe can then simply add the proportions as a new column to our table of frequencies by assigning the values we just calculated to a new variable:\n\n# the variable \"prop\" in the \"freq_table\" object is now assigned \n# the values we calculated above (the proportions)\nfreq_table$prop &lt;- freq_table$n/sum(freq_table$n)\n\n# print the \"freq_table\" object\nfreq_table\n\n# A tibble: 5 × 3\n  area                       n  prop\n  &lt;chr&gt;                  &lt;int&gt; &lt;dbl&gt;\n1 Cognitive Neuroscience    24 0.324\n2 Developmental             10 0.135\n3 Differential              20 0.270\n4 Language                   9 0.122\n5 Social                    11 0.149\n\n\n However, we can also do this within a sequence of pipes (%&gt;%). To do so, we use a new function called mutate().\n\n\n\n\n\n\nmutate\n\n\n\nThe mutate() function is used to add or modify variables to data.\n\n# take the data\n# %&gt;%\n# mutate it, such that there is a variable called \"newvariable\", which\n# has the values of a variable called \"oldvariable\" multiplied by two.\ndata %&gt;%\n  mutate(\n    newvariable = oldvariable * 2\n  )\n\nNote: Inside mutate(), we don’t have to keep using the dollar sign $, as we have already told it what data to look for variables in.\nTo ensure that our additions/modifications of variables are stored in R’s environment (rather than simply printed out), we need to reassign the name of our dataframe:\n\ndata &lt;- \n  data %&gt;%\n  mutate(\n    newvariable = oldvariable * 2\n  )\n\n\n\nWe can actually add this step to our earlier code:\n\n# make a new object called \"freq_table\", and assign it:\n# the counts of values of \"area\" variable in \n# the psych_discipline dataframe.\n# from there, 'mutate' such that there is a variable called \"prop\" which\n# has the values of the \"n\" variable divided by the sum of the \"n\" variable.\nfreq_table &lt;- \n  psych_disciplines %&gt;%\n  count(area) %&gt;%\n  mutate(\n    prop = n/sum(n)\n  )\n\n# show the object called \"freq_table\"\nfreq_table\n\n# A tibble: 5 × 3\n  area                       n  prop\n  &lt;chr&gt;                  &lt;int&gt; &lt;dbl&gt;\n1 Cognitive Neuroscience    24 0.324\n2 Developmental             10 0.135\n3 Differential              20 0.270\n4 Language                   9 0.122\n5 Social                    11 0.149\n\n\nVisualising\n\n“By visualizing information, we turn it into a landscape that you can explore with your eyes. A sort of information map. And when you’re lost in information, an information map is kind of useful.”_ David McCandless\n\nWe’re going to now make our first steps into the world of data visualisation. R is an incredibly capable language for creating visualisations of almost any kind. It is used by many media companies (e.g., the BBC), and has the capability of producing 3d visualisations, animations, interactive graphs, and more.\nWe are going to use the most popular R package for visualisation, ggplot2. This is actually part of the tidyverse, so if we have an Rmarkdown document and have loaded the tidyverse packages at the start (by using library(tidyverse)), then ggplot2 will be loaded too).\nRecall our frequency distribution table:\n\n# show the object called \"freq_table\"\nfreq_table\n\n# A tibble: 5 × 3\n  area                       n  prop\n  &lt;chr&gt;                  &lt;int&gt; &lt;dbl&gt;\n1 Cognitive Neuroscience    24 0.324\n2 Developmental             10 0.135\n3 Differential              20 0.270\n4 Language                   9 0.122\n5 Social                    11 0.149\n\n\nWe can plot these values as a bar chart:\n\nggplot(data = freq_table, aes(x = area, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\n\n\n\nggplot components\nNote the key components of the ggplot code.\n\n\ndata = where we provide the name of the dataframe.\n\naes = where we provide the aesthetics. These are things which we map from the data to the graph. For instance, the x-axis, or if we wanted to colour the columns/bars according to some aspect of the data.\n\nThen we add (using +) some geometry. These are the shapes (in our case, the columns/bars), which will be put in the correct place according to what we specified in aes().\n\n\n+ geom_col() Adds columns to the plot.\n\n\n\n\n\n\n\n\n\n\nOptional - Different aes() and geoms, and labels\n\n\n\n\n\nUse these as reference for when you want to make changes to the plots you create.\nAdditionall, remember that google is your friend - there are endless forums with people asking how to do something in ggplot, and you can just copy and paste bits of code to add to your plots!\n\nFill the geoms:\n\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()\n\n\n\n\n\nChange the axis labels:\n\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")\n\n\n\n\n\nChange the geom:\n(Note that using geom_col had the y axis starting at 0, but geom_point starts just below the lowest value.\n\n\n# note that we also need to change \"fill = area\" to \"col = area\". \nggplot(data = freq_table, aes(x = area, y = n, col = area)) +\n    geom_point()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")\n\n\n\n\n\nChange the limits of the axes:\n\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")+\n    ylim(0,50)\n\n\n\n\n\nRemove (or reposition) the legend:\n\n\n# setting legend.position as \"bottom\" would put it at the bottom!\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")+\n    ylim(0,50)+\n    theme(legend.position = \"none\") \n\n\n\n\n\nChanging the theme:\n\n\n# there are many predefine themes, including: \n# theme_bw(), theme_classic(), theme_light()\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")+\n    ylim(0,50)+\n    theme_minimal()"
  },
  {
    "objectID": "rd1_02.html#ordered-categorical-ordinal-data",
    "href": "rd1_02.html#ordered-categorical-ordinal-data",
    "title": "Categorical data",
    "section": "\n4 Ordered Categorical (Ordinal) Data",
    "text": "4 Ordered Categorical (Ordinal) Data\nRecall that ordinal data is categorical data which has a natural ordering of the possible responses. One of the most common examples of ordinal data which you will encounter in psychology is the Likert Scale. You will probably have come across these before, perhaps when completing online surveys or questionnaires.\n\n\n\n\n\n\nLikert Scale\nA five or seven point scale on which an individual express how much they agree or disagree with a particular statement.\n\n\n\nWith Likert data, there is a set of discrete response options (it is categorical data). The response options can be ranked, making it ordered categorical ( strongly disagree &lt; disagree &lt; neither &lt; agree &lt; strongly agree ). Importantly, the distance between responses is not measurable.\nFrequency table\nLet’s suppose that as well as collecting information on the sub-discipline of psychology they identified with, we also asked our respondents to rate their level of happiness from 1 to 5, as well as their job satisfaction from 1 to 5.\n\n\nVariable Name\nDescription\n\n\n\nparticipant\nSubject identifier\n\n\nhappiness\nRespondent’s level of happiness from 1 to 5\n\n\njob_sat\nRespondent’s level of job satisfaction from 1 to 5\n\n\n\n\npsych_survey &lt;- read_csv(\"https://uoepsy.github.io/data/psych_survey2.csv\")\npsych_survey\n\n# A tibble: 74 × 3\n   participant   happiness job_sat\n   &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n 1 respondent_1          3       3\n 2 respondent_2          3       4\n 3 respondent_3          2       5\n 4 respondent_4          4       5\n 5 respondent_5          3       5\n 6 respondent_6          4       4\n 7 respondent_7          4       2\n 8 respondent_8          5       5\n 9 respondent_9          1       5\n10 respondent_10         3       4\n# ℹ 64 more rows\n\n\nFor these questions (variables happiness and job_sat), we could do the same thing as we did above for unordered categorical data, and summarise this into frequencies:\n\n# take the \"psych_survey\" dataframe %&gt;%\n# count() the values in the \"happiness\" variable \npsych_survey %&gt;%\n    count(happiness)\n\n# A tibble: 5 × 2\n  happiness     n\n      &lt;dbl&gt; &lt;int&gt;\n1         1     6\n2         2    13\n3         3    27\n4         4    21\n5         5     7\n\n# take the \"psych_survey\" dataframe %&gt;%\n# count() the values in the \"job_sat\" variable \npsych_survey %&gt;%\n    count(job_sat)\n\n# A tibble: 5 × 2\n  job_sat     n\n    &lt;dbl&gt; &lt;int&gt;\n1       1     3\n2       2     6\n3       3    11\n4       4    16\n5       5    38\n\n\nCentral tendency\nWe could again use the Mode - the most common value - to summarise this data. However, because the responses are ordered, it can be more useful to think about the percentages of respondents in and below/above each category. For instance, we might want to talk about asking which category has 50% of the observations in a lower category, and 50% of the observations in a higher category. This is mid-point is known as the Median.\n\n\n\n\n\n\nMedian\nThe value for which 50% of observations a lower and 50% are higher. It is the mid-point of a list of ordered values.\nTo find the median:\n\nrank order the values\nfind the middle value:\n\nIf there are \\(n\\) values, find the value at position \\(\\frac{n+1}{2}\\).\n\nIf \\(n\\) is even, \\(\\frac{n+1}{2}\\) will not be a whole number.\nFor instance, if \\(n = 20\\), you are looking for the \\(\\frac{n+1}{2} = \\frac{20+1}{2} = 10.5^{th}\\) value.\n\nWhen calculating the median for ordinal data, if the \\(\\frac{n}{2}^{th}\\) and \\(\\frac{n+1}{2}^{th}\\) values are different, report both.\nWhen calculating the median for numeric data, report the midpoint of the \\(\\frac{n}{2}^{th}\\) and \\(\\frac{n+1}{2}^{th}\\) values.\n\n\n\n\n\n\n\n\nYou can tell R explicitly that a variable is of a certain type using functions such as as.factor(), as.numeric(), and so on.\nYou may notice that we haven’t done this yet with the data we have been working with in so far today:\n\n# inside the \"psych_survey\" dataframe, take ($) the \"happiness\" variable,\n# and tell me what type/class it is\nclass(psych_survey$happiness)\n\n[1] \"numeric\"\n\n\nThis is because there are some benefits to letting R think your data is numeric, even when it is not. It means we can use functions such as median() to quickly find the median:\n\n# inside the \"psych_survey\" dataframe, take ($) the \"happiness\" variable,\n# and find the median\nmedian(psych_survey$happiness)\n\n[1] 3\n\n\n\n\n\n\n\n\nCaution!\n\n\n\nWhile we can make R treat this data is numeric, it is important to remember that it is actually measured on an ordinal scale.\nFor example, if the median falls between levels, R will tell us that the median is the mid-point:\n\n# for the values 2,1,2,3,4,5, \n# find the median\nmedian(c(2,1,2,3,5,4))\n\n[1] 2.5\n\n\nBut because our data is ordinal, then we know that 2.5 is not a valid response.\n\n\nWe can also use functions such as min() and max() to find the minimum and maximum values:\n\n# inside the \"psych_survey\" dataframe, take ($) the \"happiness\" variable,\n# and find the minimum value\nmin(psych_survey$happiness)\n\n[1] 1\n\n# and find the maximum value\nmax(psych_survey$happiness)\n\n[1] 5\n\n\nCumulative percentages, Quartiles\nIn calculating the median, we are going beyond talking about the relative frequencies (i.e., the percentage in each category), to talking about the cumulative percentage.\n\n\n\n\n\n\nCumulative percentage\nCumulative percentages are another way of expressing a frequency distribution.\nThey are the successive addition of percentages in each category. For example, the cumulative percentage for the 3rd category is the percentage of respondents in the 1st, 2nd and 3rd category:\n\n\n\n\n\n\n\n\n\n\n\nCategory\nFrequency count (n)\nRelative frequency (%)\nCumulative frequency\nCumulative percentage\n\n\n\nResponse 1\n10\n13.33\n10\n13.33\n\n\nResponse 2\n10\n13.33\n20\n26.67\n\n\nResponse 3\n20\n26.67\n40\n53.33\n\n\nResponse 4\n25\n33.33\n65\n86.67\n\n\nResponse 5\n10\n13.33\n75\n100.00\n\n\n\n\n\n\n\n\nWe saw before how we can calculate the proportions/percentages in each category:\n( Note: We multiply by 100 here to turn the proportion into a percentage)\n\n# take the \"psych_survey\" dataframe %&gt;%\n# count() the values in the \"happiness\" variable (creates an \"n\" column), and\n# from there, 'mutate' such that there is a variable called \"percent\" which\n# has the values of the \"n\" variable divided by the sum of the \"n\" variable.\npsych_survey %&gt;%\n  count(happiness) %&gt;%\n  mutate(\n    percent = n/sum(n)*100\n  )\n\n# A tibble: 5 × 3\n  happiness     n percent\n      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1         1     6    8.11\n2         2    13   17.6 \n3         3    27   36.5 \n4         4    21   28.4 \n5         5     7    9.46\n\n\nWe can add another variable, and make it the cumulative percentage, by using the cumsum() function.\n\n# take the \"psych_survey\" dataframe %&gt;%\n# count() the values in the \"happiness\" variable (creates an \"n\" column), and\n# from there, 'mutate' such that there is a variable called \"percent\" which\n# has the values of the \"n\" variable divided by the sum of the \"n\" variable,\n# and also make a variable called \"cumulative_percent\" which is the \n# successive addition of the values in the \"percent\" variable\npsych_survey %&gt;% \n  count(happiness) %&gt;% \n  mutate(\n    percent = n/sum(n)*100,\n    cumulative_percent = cumsum(percent)\n  )\n\n# A tibble: 5 × 4\n  happiness     n percent cumulative_percent\n      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;              &lt;dbl&gt;\n1         1     6    8.11               8.11\n2         2    13   17.6               25.7 \n3         3    27   36.5               62.2 \n4         4    21   28.4               90.5 \n5         5     7    9.46             100   \n\n\n\n\n\n\n\n\nThink about it\n\n\n\n\n\nThink about why this will not work:\n\npsych_survey %&gt;% \n  count(happiness) %&gt;% \n  mutate(\n    cumulative_percent = cumsum(percent),\n    percent = n/sum(n)*100\n  )\n\n\nError: object ‘percent’ not found\n\nAnswer: Inside the mutate() function, we are trying to assign the cumulative_percent variable based on the values of the percent variable. But in the code above, percent gets defined after cumulative_percent, and so it will not work. Hence the error message (“percent not found”).\n\n\n\nWhile the median splits the data in two (50% either side), you will often see data being split into four equal blocks.\nThe points which divide the four blocks are known as quartiles.\n\n\n\n\n\n\nQuartiles\nQuartiles are the points in rank-ordered data below which falls 25%, 50%, and 75% of the data.\n\nThe first quartile is the first category for which the cumulative percentage is \\(\\geq 25\\%\\).\n\nThe median is the first category for which the cumulative percentage is \\(\\geq 50\\%\\).\n\nThe third quartile is the first category for which the cumulative percentage is \\(\\geq 75\\%\\).\n\n\n\n\nBy looking at the quartiles, it gives us an idea of how spread out the data is.\nAs an example, if we had 10 categories A, B, C, D, E, F, G, H, I, J, and we knew that:\n\n\n\\(Q_1\\) (the \\(1^{st}\\) quartile) = G,\n\n\\(Q_2\\) (the \\(2^{nd}\\) quartile, the median) = H,\n\n\\(Q_3\\) (the \\(3^{rd}\\) quartile) = H,\n\nThis tells us that the first 25% of the data falls in one of the categories from A to G (quite a large range), the second 25% falls in categories G and H (a small range), and the third 25% of the data falls entirely in category H.\nSo a lot of the data is between G and H, with the data being more sparse in the lower and higher categories.\n\n\n\n\n\n\nLooking ahead to numeric data\nWe will talk about quartiles in numeric data too, where we commonly use the difference between the first and third quartile as a measure of how spread out the data are. This gets known as the inter-quartile range (IQR).\n\n\n\nVisualising\nWe can visualise ordered categorical data in the same way we did for unordered.\nFirst we save our frequencies/percentages as a new object:\n\nfreq_table2 &lt;- psych_survey %&gt;%\n  count(happiness) %&gt;%\n  mutate(\n    percent = n/sum(n)*100\n  )\n\nThen we give that object to our ggplot code, with the appropriate aes() mappings:\n\n# make a ggplot with the object \"freq_table2\". \n# on the x axis put the possible values in the \"happiness\" variable,\n# on the y axis put the possible values in the \"n\" variable.\n# add columns for each entry in the data. \nggplot(data = freq_table2, aes(x = happiness, y = percent)) + \n  geom_col()"
  },
  {
    "objectID": "rd1_02.html#glossary",
    "href": "rd1_02.html#glossary",
    "title": "Categorical data",
    "section": "\n5 Glossary",
    "text": "5 Glossary\n\n\ndistribution: How often different possible values in a variable occur.\n\nfrequency: Number of occurrences (count) in a given response value.\n\nrelative frequency: Percentage/proportion of occurrences in a given response value.\n\ncumulative percentage: Percentage of occurrences in or below a given reponse value (requires ordered data).\n\nmode: Most common value.\n\nmedian: Middle value. \n\n\n%&gt;% Takes the output of whatever is on the LHS and gives it as the input of whatever is on the RHS.\n\ncount() Counts the number of occurrences of each unique value in a variable.\n\nmutate() Used to add variables to the dataframe, or modify existing variables.\n\nmin() Returns the minimum value of a variable.\n\nmax() Returns the maximum value of a variable.\n\nmedian() Returns the median value of a variable.\n\nggplot() Creates a plot. Takes data= and a set of mappings aes() from the data to properties of the plot (e.g., x/y axes, colours).\n\ngeom_col() Adds columns to a ggplot."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is the homepage of DAPR1 labs. Please use the sidebar to navigate to the lab materials for a specific week of teaching.\nIf you are looking for the lecture materials, please go to the course page on Blackboard Learn.\n\n\n\n Back to top"
  },
  {
    "objectID": "2_04_htci.html",
    "href": "2_04_htci.html",
    "title": "Hypothesis testing and confidence intervals",
    "section": "",
    "text": "Next week: submission of Formative Report C (PDF file only)\n\n\n\n\n\n\nYou are required to submit a PDF file by 12 noon on Friday the 17th of February 2023 via Learn. One person needs to submit on behalf of your group.\nNo extensions allowed. As this is group-based work, no extensions are possible.\n\nThe report should be at most 6 pages long. At the end of the report, you are allowed two appendices which don’t count towards the page limit.\n\nAppendix A will contain any tables or figures which you cannot fit in the page limit (no text allowed).\nAppendix B will contain the code to reproduce the report results.\n\n\nExcluding Appendix B, the report should only include text, figures or tables. It should not include any reference to R code or functions, but be written for a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\nThe report title should be “Formative Report C (Group 0.A)”. Replace Group 0.A to be your group name.\n\nIn the author section of the PDF file write the exam number of each person within the group. The exam number starts with the letter B and can be found on your student ID card.\n\nFor example: B000001, B000002, B000003, B000004\n\n\nYou will receive formative feedback on your submission during Flexible Learning Week. Please keep going to the labs that week in order to receive feedback."
  },
  {
    "objectID": "2_04_htci.html#tasks",
    "href": "2_04_htci.html#tasks",
    "title": "Hypothesis testing and confidence intervals",
    "section": "\n1 Tasks",
    "text": "1 Tasks\nThe data dataset-ipeds-2012-subset2, available at https://uoepsy.github.io/data/dataset-ipeds-2012-subset2.csv, is a subset of data derived from the Integrated Postsecondary Education Data System (IPEDS) at the National Center for Education Statistics, 2012. The data were collected for a random sample from all colleges and universities in the United States in that year. The variables include:\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\ntype\nCollege Type:−1 = Not reported;\n1 = Public;\n2 = Private for-profit;\n3 = Private not-for-profit (no religious affiliation);\n4 = Private not-for-profit (religious affiliation)\n\n\n\nregion\nRegion:0 = US Service schools;\n1 = New England;\n2 = Mid East;\n3 = Great Lakes;\n4 = Plains;\n5 = Southeast;\n6 = Southwest;\n7 = Rocky Mountains;\n8 = Far West;\n9 = Outlying areas\n\n\n\ngradrate\nGraduation Rate – All\n\n\ngradratem\nGraduation Rate – Men\n\n\ngradratew\nGraduation Rate – Women\n\n\n\n\n\nIn formative report C, you will investigate the mean graduation rate for female students at colleges and universities in the United States. Specifically, you are asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task C4.\nC1) Read the data into R, describe the variable of interest both visually and numerically, and provide a 95% CI for the mean graduation rate of female students at colleges and universities in the United States.\nC2) At the 5% significance level and using the p-value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\nC3) At the 5% significance level and using the critical value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\n\n\n\n\n\n\nThis week’s task\n\n\n\nC4) Tidy up your report so far, making sure to have 3 sections: introduction, analysis and discussion. After those, you can have Appendix A (additional figures or tables) and Appendix B (the R code used).\n\n\nC5) Compute and report the effect size, check if the assumptions underlying the t-test are violated."
  },
  {
    "objectID": "2_04_htci.html#c4-sub-tasks",
    "href": "2_04_htci.html#c4-sub-tasks",
    "title": "Hypothesis testing and confidence intervals",
    "section": "\n2 C4 sub-tasks",
    "text": "2 C4 sub-tasks\nIn this section you will find some guided sub-steps you may want to consider to complete task C4.\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\n\nEnsure that your report has 3 sections:\n\nIntroduction - where you provide a brief description of the data, variables and their type, and the research questions you are going to address.\nAnalysis - where you show and describe your results. Please note that no R code or output should be visible, but only figures and tables.\nDiscussion - where you summarise your key results in a few take-home messages that answer the research questions.\n\n\nStructure your Rmd file as follows:\n\n---\ntitle: \"Formative report C (Group 0.A)\"\nauthor: \"B000000, B000001, B00002, B00003, B000004\"\ndate: \"Write the date here\"\noutput: bookdown::pdf_document2\ntoc: false\n---\n\n\nThis is the metadata block. It includes the:\n\ndocument title\nauthor name\ndate (to leave empty, use an empty string \"\")\nthe output type\n\nThe output type could be html_document, pdf_document, etc.\nWe use bookdown::pdf_document2 so that we can reference figures, which pdf_document doesn’t let you do.\nThe code bookdown::pdf_document2 simply means to use the pdf_document2 type from the bookdown package.\nThe code toc: false hides the table of contents.\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)\n```\n\n\nThis is the setup chunk and should always be included in your Rmd document. It sets the global options for all code chunks that will follow.\n\nIf echo=TRUE, the R code in chunks is displayed. If FALSE, not.\nIf message=TRUE, information messages are displayed. If FALSE, not.\nIf warning=TRUE, warning messages are printed. If FALSE, not.\n\nIf you want to change the setting in a specific code chunk, you can do so via:\n```{r, echo=FALSE}\n# A code chunk\n```\n\n```{r, include=FALSE}\n# week 1 code below\nlibrary(tidyverse)\n\n# week 2 code below\npltEye &lt;- ggplot(starwars, aes(x = eye_color)) + \n    geom_bar()\n\n# week 3 code below\n\n# week 4 code below\n\n# week 5 code below\n```\n\n\nThis code chunks contains your rough work from each week. Give names to plots and tables, so that you can reference those later on. The option include=FALSE hides both code and output.\nTo run each line of code while you are working, put your cursor on the line and press Control + Enter on Windows or Command + Enter on a macOS.\n## Introduction\n\nWrite here an introduction to the data, the variables, and anything worth of \nnotice in the data.\n\n\n## Analysis\n\nPresent here your tables, plots, and results. In the code chunk below, you do \nnot need to put the chunk option `echo=FALSE` as you set this option globally \nin the setup chunk. \n\n```{r}\npltEye\n```\n\nIf you didn't set it globally, you would need to put it in the chunk options:\n\n```{r, echo=FALSE}\npltEye\n```\n\nMore text...\n\n\n## Discussion\n\nWrite up your take home messages here...\n\n\nThis contains your actual textual reporting, as well as tables and figures. To show in place a plot previously created, just include the plot name in a code chunk with the option echo = FALSE to hide the code but display the output.\n## Appendix A - Additional tables and figures\n\nInsert here any additional tables or figures that you could not fit in the \npage limit.\n## Appendix B - R code\n\n```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}\n```\n\n\nCopy and paste in your report this last code chunk as it is here. This special code chunk will copy here all the previous R code chunks that you have created and automatically populate Appendix B for you.\nNote\nThe appendices do not count towards the 6-page limit."
  },
  {
    "objectID": "2_04_htci.html#worked-example",
    "href": "2_04_htci.html#worked-example",
    "title": "Hypothesis testing and confidence intervals",
    "section": "\n3 Worked Example",
    "text": "3 Worked Example\nThe R code is visible here for instructional purposes only, but it should not be visible in a PDF report. It should only appear as part of the appendix.\n\n\n\n\n\n\nR code\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(kableExtra)\n\npass_scores &lt;- read_csv(\"https://uoepsy.github.io/data/pass_scores.csv\")\ndim(pass_scores)\nhead(pass_scores)\nglimpse(pass_scores)\n\nsummary(pass_scores)\n\nplt_hist &lt;- ggplot(pass_scores, aes(x = PASS)) + \n    geom_histogram(color = 'white') +\n    labs(x = \"PASS scores\", title = \"(a) Histogram\")\n\nplt_box &lt;- ggplot(pass_scores, aes(x = PASS)) + \n    geom_boxplot() +\n    labs(x = \"PASS scores\", title = \"(b) Boxplot\")\n\nplt_hist / plt_box\nstats &lt;- pass_scores %&gt;%\n    summarise(n = n(),\n              Min = min(PASS),\n              Max = max(PASS),\n              M = mean(PASS),\n              SD = sd(PASS))\n\nkbl(stats, booktabs = TRUE, digits = 2, \n    caption = \"Descriptive statistics for PASS scores\")\n\n# Confidence interval\nxbar &lt;- stats$M\ns &lt;- stats$SD\nn &lt;- stats$n\nse &lt;- s / sqrt(n)\ntstar &lt;- qt(c(0.025, 0.975), df = n - 1)\n\nxbar + tstar * se\n\n# observed t-statistic\ntobs &lt;- (xbar - 33) / se\ntobs\n\n# p-value method\npvalue &lt;- 2 * pt(abs(tobs), df = n - 1, lower.tail = FALSE)\npvalue\n\n# critical value method\ntstar\ntobs\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\nA random sample of 20 students from the University of Edinburgh completed a questionnaire measuring their total endorsement of procrastination. The data, available from https://uoepsy.github.io/data/pass_scores.csv, were used to estimate the average procrastination score of all Edinburgh University students, as well as testing whether the mean procrastination score differed from the Solomon & Rothblum reported average of 33 at the 5% significance level. The recorded variables include a subject identifier (sid, categorical), the school each belongs to (school, categorical), and the total score on the Procrastination Assessment Scale for Students (PASS, numeric). The data do not include any impossible values for the PASS scores, as they were all within the possible range of 0 – 90. To answer the questions of interest, in the following we will only focus on the total PASS score variable.\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\n\nThe distribution of PASS scores, as shown in Figure 1(a), is roughly bell shaped and does not have any impossible values. The outlier (40) depicted in the boxplot shown in Figure 1(b) is well within the range of plausible values for the PASS scale (0–90) and as such was not removed for the analysis.\n\n\n\n\nFigure 1: Distribution of PASS scores for a sample of Edinburgh University students\n\n\n\n\n\n\n\nTable 1: Descriptive statistics for PASS scores\n\nn\nMin\nMax\nM\nSD\n\n\n20\n24\n40\n30.7\n3.31\n\n\n\n\n\n\n\nTable 1 displays summary statistics for the PASS scores in the sample of Edinburgh University students. From the sample data we obtain an average procrastination score of \\(M = 30.7\\), 95% CI [29.15, 32.25]. Hence, we are 95% confident that a Edinburgh University student will have a procrastination score between 29.15 and 32.25, which is between 0.75 and 3.85 lower than the average score of 33 reported by Solomon & Rothblum.\n\nLet \\(\\mu\\) denote the mean PASS score of all Edinburgh University students. At the 5% significance level, we performed a one sample t-test of \\(H_0 : \\mu = 33\\) against \\(H_1 : \\mu \\neq 33\\). The sample data provide very strong evidence against the null hypothesis and in favour of the alternative one that the mean procrastination score of Edinburgh University students is significantly different from the Solomon & Rothblum reported average of 33: \\(t(19) = -3.11, p = .006\\), two-sided.\n\n\n\n\n\n\n\n\n\nDiscussion\n\n\n\n\n\nData including the Procrastination Assessment Scale for Students (PASS) scores for a random sample of 20 students at Edinburgh University we used to estimate the average procrastination score for a student of that university. In addition, the data were used to test whether there is a significant difference between that average score and the Solomon & Rothblum reported average of 33.\nWe are 95% confident that a Edinburgh University student will have a procrastination score between 29.15 and 32.25. Furthermore, at the 5% significance level, the data provide very strong evidence that the mean procrastination score of Edinburgh University students is different from 33. The confidence interval, reported above, indicates that a Edinburgh University student tends to have a mean procrastination score between 0.75 and 3.85 lower than the Solomon & Rothblum reported average of 33.\n\n\n\nWhat is missing from this instructional example:\n\nAppendix A\nAppendix B"
  },
  {
    "objectID": "2_04_htci.html#footnotes",
    "href": "2_04_htci.html#footnotes",
    "title": "Hypothesis testing and confidence intervals",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: Ask last week’s driver for the Rmd file, they should share it with the group via email or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎"
  },
  {
    "objectID": "2_02_ht_pvalues.html",
    "href": "2_02_ht_pvalues.html",
    "title": "Hypothesis testing: p-values",
    "section": "",
    "text": "The data dataset-ipeds-2012-subset2, available at https://uoepsy.github.io/data/dataset-ipeds-2012-subset2.csv, is a subset of data derived from the Integrated Postsecondary Education Data System (IPEDS) at the National Center for Education Statistics, 2012. The data were collected for a random sample from all colleges and universities in the United States in that year. The variables include:\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\ntype\nCollege Type:−1 = Not reported;\n1 = Public;\n2 = Private for-profit;\n3 = Private not-for-profit (no religious affiliation);\n4 = Private not-for-profit (religious affiliation)\n\n\n\nregion\nRegion:0 = US Service schools;\n1 = New England;\n2 = Mid East;\n3 = Great Lakes;\n4 = Plains;\n5 = Southeast;\n6 = Southwest;\n7 = Rocky Mountains;\n8 = Far West;\n9 = Outlying areas\n\n\n\ngradrate\nGraduation Rate – All\n\n\ngradratem\nGraduation Rate – Men\n\n\ngradratew\nGraduation Rate – Women\n\n\n\n\n\nIn formative report C, you will investigate the mean graduation rate for female students at colleges and universities in the United States. Specifically, you are asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task C2.\nC1) Read the data into R, describe the variable of interest both visually and numerically, and provide a 95% CI for the mean graduation rate of female students at colleges and universities in the United States.\n\n\n\n\n\n\nThis week’s task\n\n\n\nC2) At the 5% significance level and using the p-value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\n\n\nC3) At the 5% significance level and using the critical value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\nC4) Tidy up your report so far, making sure to have 3 sections: introduction, analysis and discussion.\nC5) Compute and report the effect size, check if the assumptions underlying the t-test are violated."
  },
  {
    "objectID": "2_02_ht_pvalues.html#tasks",
    "href": "2_02_ht_pvalues.html#tasks",
    "title": "Hypothesis testing: p-values",
    "section": "",
    "text": "The data dataset-ipeds-2012-subset2, available at https://uoepsy.github.io/data/dataset-ipeds-2012-subset2.csv, is a subset of data derived from the Integrated Postsecondary Education Data System (IPEDS) at the National Center for Education Statistics, 2012. The data were collected for a random sample from all colleges and universities in the United States in that year. The variables include:\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\ntype\nCollege Type:−1 = Not reported;\n1 = Public;\n2 = Private for-profit;\n3 = Private not-for-profit (no religious affiliation);\n4 = Private not-for-profit (religious affiliation)\n\n\n\nregion\nRegion:0 = US Service schools;\n1 = New England;\n2 = Mid East;\n3 = Great Lakes;\n4 = Plains;\n5 = Southeast;\n6 = Southwest;\n7 = Rocky Mountains;\n8 = Far West;\n9 = Outlying areas\n\n\n\ngradrate\nGraduation Rate – All\n\n\ngradratem\nGraduation Rate – Men\n\n\ngradratew\nGraduation Rate – Women\n\n\n\n\n\nIn formative report C, you will investigate the mean graduation rate for female students at colleges and universities in the United States. Specifically, you are asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task C2.\nC1) Read the data into R, describe the variable of interest both visually and numerically, and provide a 95% CI for the mean graduation rate of female students at colleges and universities in the United States.\n\n\n\n\n\n\nThis week’s task\n\n\n\nC2) At the 5% significance level and using the p-value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\n\n\nC3) At the 5% significance level and using the critical value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\nC4) Tidy up your report so far, making sure to have 3 sections: introduction, analysis and discussion.\nC5) Compute and report the effect size, check if the assumptions underlying the t-test are violated."
  },
  {
    "objectID": "2_02_ht_pvalues.html#c2-sub-tasks",
    "href": "2_02_ht_pvalues.html#c2-sub-tasks",
    "title": "Hypothesis testing: p-values",
    "section": "\n2 C2 sub-tasks",
    "text": "2 C2 sub-tasks\nIn this section you will find some guided sub-steps you may want to consider to complete task C2.\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\nState the null and alternative hypotheses.2\n\n\n\nCompute the value of the t-statistic from the sample mean graduation rate of female students.3\n\n\n\nIdentify the null distribution.4\nCompute the p-value for the test.5\nUsing a 5% significance level, i.e. \\(\\alpha = 0.05\\) make a decision on whether or not to reject the null hypothesis.6\nProvide a write up of your results in the context of the research question.\nUpdate the report introduction to also include information about the second question being investigated, i.e. whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent."
  },
  {
    "objectID": "2_02_ht_pvalues.html#worked-example",
    "href": "2_02_ht_pvalues.html#worked-example",
    "title": "Hypothesis testing: p-values",
    "section": "\n3 Worked Example",
    "text": "3 Worked Example\nThe Procrastination Assessment Scale for Students (PASS) was designed to assess how individuals approach decision situations, specifically the tendency of individuals to postpone decisions (Solomon & Rothblum, 1984).\nThe PASS assesses the prevalence of procrastination in six areas: writing a paper; studying for an exam; keeping up with reading; administrative tasks; attending meetings; and performing general tasks. For a measure of total endorsement of procrastination, responses to 18 questions (each measured on a 1-5 scale) are summed together, providing a single score for each participant (range 0 to 90). The mean score from Solomon & Rothblum, 1984 was 33.\n\nResearch question:\nDoes the mean procrastination score of Edinburgh University students differ from the Solomon & Rothblum average of 33?\n\nTo answer this question, we will use data collected for a random sample of students from the University of Edinburgh: https://uoepsy.github.io/data/pass_scores.csv\n\n\n\n\nVariable Name\nDescription\n\n\n\nsid\nSubject identifier\n\n\nschool\nSchool each subject belonged to\n\n\nPASS\nTotal endorsement of procrastination score\n\n\n\n\n\nNecessary packages:\n\n\n\ntidyverse for using read_csv(), using summarise() and ggplot().\npatchwork for arranging plots side by side or underneath\nkableExtra for creating user-friendly tables\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(kableExtra)\n\nRead the data into R:\n\npass_scores &lt;- read_csv(\"https://uoepsy.github.io/data/pass_scores.csv\")\ndim(pass_scores)\n\n[1] 20  3\n\n\nTo inspect the data:\n\n\nhead()\nglimpse()\nsummary()\n\n\n\n\nhead(pass_scores)\n\n# A tibble: 6 × 3\n  sid   school       PASS\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 s_1   GeoSciences    31\n2 s_2   ECA            24\n3 s_3   LAW            32\n4 s_4   ECA            40\n5 s_5   LAW            28\n6 s_6   SSPS           31\n\n\n\n\n\nglimpse(pass_scores)\n\nRows: 20\nColumns: 3\n$ sid    &lt;chr&gt; \"s_1\", \"s_2\", \"s_3\", \"s_4\", \"s_5\", \"s_6\", \"s_7\", \"s_8\", \"s_9\", …\n$ school &lt;chr&gt; \"GeoSciences\", \"ECA\", \"LAW\", \"ECA\", \"LAW\", \"SSPS\", \"PPLS\", \"SLL…\n$ PASS   &lt;dbl&gt; 31, 24, 32, 40, 28, 31, 30, 28, 32, 29, 28, 33, 35, 33, 30, 31,…\n\n\n\n\n\nsummary(pass_scores)\n\n     sid               school               PASS      \n Length:20          Length:20          Min.   :24.00  \n Class :character   Class :character   1st Qu.:28.75  \n Mode  :character   Mode  :character   Median :31.00  \n                                       Mean   :30.70  \n                                       3rd Qu.:32.00  \n                                       Max.   :40.00  \n\n\n\n\n\nVisualise the distribution of PASS scores:\n\n\nNote\nThe boxplot highlights an outlier (40). However, this value is well within the plausible range of the scale (0 – 90), hence it is of no concern and the point can be kept for the analysis.\n\nplt_hist &lt;- ggplot(pass_scores, aes(x = PASS)) + \n    geom_histogram(color = 'white')\n\nplt_box &lt;- ggplot(pass_scores, aes(x = PASS)) + \n    geom_boxplot()\n\nplt_hist / plt_box\n\n\n\n\nDescriptive statistics:\n\nstats &lt;- pass_scores %&gt;%\n    summarise(n = n(),\n              Min = min(PASS),\n              Max = max(PASS),\n              M = mean(PASS),\n              SD = sd(PASS))\n\n\nstats %&gt;%\n    kbl(booktabs = TRUE, digits = 2, \n        caption = \"Descriptive statistics for PASS scores\")\n\n\n\n\nDescriptive statistics for PASS scores\n\nn\nMin\nMax\nM\nSD\n\n\n20\n24\n40\n30.7\n3.31\n\n\n\n\nStep 1: Identify the null and alternative hypotheses.\nFirst we need to write the null and alternative hypothesis, which take the form \\(H_0 : \\mu = \\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\). From the research question, we identify the hypothesised value \\(\\mu_0\\) to be 33, hence:\n\n\nThese are written as:\n$$H_{0}: \\mu = 33$$\n$$H_{1}: \\mu \\neq 33$$\nIn H_{0} and H_{1} the 0 and 1 within curly braces are written as subscripts. The curly braces delimit what goes in the subscript. The symbol \\mu denotes the greek letter “mu” that stands for the population mean (a parameter). The symbol \\neq means not equal.\n\\[H_0: \\mu = 33\\] \\[H_1: \\mu \\neq 33\\]\nStep 2: Compute the t-statistic\nNext, we compute the t-statistics, which compares the difference between the sample and hypothesised mean (\\(\\bar{x} - \\mu_0\\)) to the variation due to random sampling (\\(SE_{\\bar{x}}\\)).\nTo test the hypothesis, we need to compute the t-statistic,\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{SE_{\\bar{x}}} \\qquad \\text{where} \\qquad SE_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\n\\]\n\n# Sample mean\nxbar &lt;- stats$M\n\n# Standard error\ns &lt;- stats$SD\nn &lt;- stats$n\nse &lt;- s / sqrt(n)\n\n# Observed t-statistic\ntobs &lt;- (xbar - 33) / se\ntobs\n\n[1] -3.107272\n\n\nStep 3: Identify the null distribution, i.e. the distribution of the t-statistic assuming the null to be true.\nAs the sample size is \\(n =\\) 20, if the null hypothesis is true the t-statistic will follow a t(19) distribution.\nStep 4: Compute the p-value\nAs the alternative hypothesis is two-sided (or two-tailed), we can compute the p-value as twice the area to the right of abs(tobs).\n\n2 * pt(abs(tobs), df = n - 1, lower.tail = FALSE)\n\n[1] 0.005800318\n\n\nStep 5: Make a decision\nUsing the p-value method, we compare the p-value with the significance level (\\(\\alpha = 0.05\\) in this case). As 0.005800318 &lt; 0.05, we reject \\(H_0\\). Please note this is just an explanation and not how you would write up the result!\nWe can update the example introduction to add the new question investigated:\n\n\n\n\n\n\nExample introduction\n\n\n\nA random sample of 20 students from the University of Edinburgh completed a questionnaire measuring their total endorsement of procrastination. The data, available from https://uoepsy.github.io/data/pass_scores.csv, were used to estimate the average procrastination score of all Edinburgh University students, and whether the mean procrastination score differed from the Solomon & Rothblum reported average of 33 at the 5% significance level. The recorded variables included a subject identifier (sid), the school of each subject (school), and the total score on the Procrastination Assessment Scale for Students (PASS). The data do not include any impossible values for the PASS scores, as they were all within the possible range of 0 – 90. To answer the question of interest, in the following we will only focus on the total PASS score variable.\n\n\nAnd this is a potential way to report the t-test results:\n\n\n\n\n\n\nExample hypothesis test write-up\n\n\n\nAt the 5% significance level, the sample data provide very strong evidence against the null hypothesis and in favour of the alternative one that the mean procrastination score of Edinburgh University students is different from the Solomon & Rothblum reported average of 33: \\(t(19) = -3.11, p = .006\\), two-sided."
  },
  {
    "objectID": "2_02_ht_pvalues.html#student-glossary",
    "href": "2_02_ht_pvalues.html#student-glossary",
    "title": "Hypothesis testing: p-values",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\ngeom_histogram\n?\n\n\ngeom_boxplot\n?\n\n\nsummarise\n?\n\n\nn()\n?\n\n\nmean\n?\n\n\nsd\n?\n\n\nabs\n?\n\n\npt\n?"
  },
  {
    "objectID": "2_02_ht_pvalues.html#footnotes",
    "href": "2_02_ht_pvalues.html#footnotes",
    "title": "Hypothesis testing: p-values",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: Ask last week’s driver for the Rmd file, they should share it with the group via email or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nHint: Identify the hypothesised value of the mean, \\(\\mu_0\\), and replace that value in the following equations:\n\\[H_{0} : \\mu = \\mu_{0}\\] \\[H_{1} : \\mu \\neq \\mu_{0}\\]\nThe equations can be written with the following code, where the text within curly braces after an underscore is rendered as a subscript:\n$$ H_{0} : \\mu = \\mu_{0} $$\n$$ H_{1} : \\mu \\neq \\mu_{0} $$\n↩︎\n\n\nHint: Use the following formula for the t-statistic:\n\\[t = \\frac{\\bar{x} - \\mu_0}{SE_{\\bar{x}}} \\qquad \\text{where} \\qquad SE_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\]\nIn the above:\n\n\n\\(\\bar{x}\\) is the sample mean\n\n\\(\\mu_0\\) is the hypothesised value for the population mean\n\n\\(s\\) is the sample standard deviation\n\n\\(n\\) is the sample size\n\n↩︎\n\nHint: the t-statistic follows a \\(t(n-1)\\) distribution where \\(n-1\\) is the degrees of freedom.\nThe question asks you: what are the degrees of freedom to use in the t distribution?↩︎\nHint: the pt() function will be useful↩︎\nHint: compare the p-value with the significance level.\nIf p-value \\(\\leq \\alpha\\), reject. Otherwise, do not reject.↩︎"
  },
  {
    "objectID": "1_11_sampling.html",
    "href": "1_11_sampling.html",
    "title": "Sampling",
    "section": "",
    "text": "Formative report B - Structure\n\n\n\n\n\n\nThe submitted report should be a PDF file of 6 pages at most.\n\nIt should include 3 sections:\n\nIntroduction - Write here an introduction to the data, the variables, and anything worth of notice in the data.\nAnalysis - Present here your tables, plots, and results. Make sure to interpret what you present.\nDiscussion - Write up your take home messages here…\n\n\n\nAfter those, you can add after two Appendices which will not count towards the 6-page limit:\n\nAppendix A - where you can put any additional tables or plot that you may not be able to fit in the page limit. However, please ensure these plots are related to the interpretation provided.\nAppendix B - where you will collate all the R code in a chunk with the settings echo=TRUE, results='hide', fig.show='hide'.\n\n\nExcluding the Appendix, the report should not include any reference to R code or functions, but be written for a generic reader who is only assumed to have a basic statistical understanding without any R knowledge.\n\n\n\n\n\n\n\n\n\n\nFormative report B - Submission Instructions\n\n\n\n\n\n\nYour group must submit one PDF file for formative report B by 12 noon on Friday 2nd December 2022.\n\nTo submit go to the course Learn page, click “Assessment” from the left-hand side menu, then click “Report submission”, and then “Submit Formative Report B here (PDF file only)”.\n\nOnly one person per group is required to submit on behalf of the entire group. However, to ensure that everyone in the group can see the feedback, please double check that you self-registered in the group on Learn (see group name on the desk).\n\n\nAs mentioned on the Course Information page, there will be no extensions allowed for group-based reports.\nYou will receive formative feedback on your submission during the labs of week 12. Please attend your lab next week!\n\n\n\n\n\n\n\n\n\n\nFormative report B - Data\n\n\n\n\n\nHollywood Movies. At the link https://uoepsy.github.io/data/hollywood_movies_subset.csv you will find data on Hollywood movies released between 2012 and 2018 from the top 5 lead studios and top 10 genres. The following variables were recorded:\n\n\nMovie: Title of the movie\n\nLeadStudio: Primary U.S. distributor of the movie\n\nRottenTomatoes: Rotten Tomatoes rating (critics)\n\nAudienceScore: Audience rating (via Rotten Tomatoes)\n\nGenre: One of Action Adventure, Black Comedy, Comedy, Concert, Documentary, Drama, Horror, Musical, Romantic Comedy, Thriller, or Western\n\nTheatersOpenWeek: Number of screens for opening weekend\n\nOpeningWeekend: Opening weekend gross (in millions)\n\nBOAvgOpenWeekend: Average box office income per theater, opening weekend\n\nBudget: Production budget (in millions)\n\nDomesticGross: Gross income for domestic (U.S.) viewers (in millions)\n\nWorldGross: Gross income for all viewers (in millions)\n\nForeignGross: Gross income for foreign viewers (in millions)\n\nProfitability: WorldGross as a percentage of Budget\n\nOpenProfit: Percentage of budget recovered on opening weekend\n\nYear: Year the movie was released\n\nIQ1-IQ50: IQ score of each of 50 audience raters (every movie had different raters)\n\nSnacks: How many of the 50 audience raters brought snacks\n\nPrivateTransport: How many of the 50 audience raters reached the cinema via private transportation"
  },
  {
    "objectID": "1_11_sampling.html#formative-report-b",
    "href": "1_11_sampling.html#formative-report-b",
    "title": "Sampling",
    "section": "",
    "text": "Formative report B - Structure\n\n\n\n\n\n\nThe submitted report should be a PDF file of 6 pages at most.\n\nIt should include 3 sections:\n\nIntroduction - Write here an introduction to the data, the variables, and anything worth of notice in the data.\nAnalysis - Present here your tables, plots, and results. Make sure to interpret what you present.\nDiscussion - Write up your take home messages here…\n\n\n\nAfter those, you can add after two Appendices which will not count towards the 6-page limit:\n\nAppendix A - where you can put any additional tables or plot that you may not be able to fit in the page limit. However, please ensure these plots are related to the interpretation provided.\nAppendix B - where you will collate all the R code in a chunk with the settings echo=TRUE, results='hide', fig.show='hide'.\n\n\nExcluding the Appendix, the report should not include any reference to R code or functions, but be written for a generic reader who is only assumed to have a basic statistical understanding without any R knowledge.\n\n\n\n\n\n\n\n\n\n\nFormative report B - Submission Instructions\n\n\n\n\n\n\nYour group must submit one PDF file for formative report B by 12 noon on Friday 2nd December 2022.\n\nTo submit go to the course Learn page, click “Assessment” from the left-hand side menu, then click “Report submission”, and then “Submit Formative Report B here (PDF file only)”.\n\nOnly one person per group is required to submit on behalf of the entire group. However, to ensure that everyone in the group can see the feedback, please double check that you self-registered in the group on Learn (see group name on the desk).\n\n\nAs mentioned on the Course Information page, there will be no extensions allowed for group-based reports.\nYou will receive formative feedback on your submission during the labs of week 12. Please attend your lab next week!\n\n\n\n\n\n\n\n\n\n\nFormative report B - Data\n\n\n\n\n\nHollywood Movies. At the link https://uoepsy.github.io/data/hollywood_movies_subset.csv you will find data on Hollywood movies released between 2012 and 2018 from the top 5 lead studios and top 10 genres. The following variables were recorded:\n\n\nMovie: Title of the movie\n\nLeadStudio: Primary U.S. distributor of the movie\n\nRottenTomatoes: Rotten Tomatoes rating (critics)\n\nAudienceScore: Audience rating (via Rotten Tomatoes)\n\nGenre: One of Action Adventure, Black Comedy, Comedy, Concert, Documentary, Drama, Horror, Musical, Romantic Comedy, Thriller, or Western\n\nTheatersOpenWeek: Number of screens for opening weekend\n\nOpeningWeekend: Opening weekend gross (in millions)\n\nBOAvgOpenWeekend: Average box office income per theater, opening weekend\n\nBudget: Production budget (in millions)\n\nDomesticGross: Gross income for domestic (U.S.) viewers (in millions)\n\nWorldGross: Gross income for all viewers (in millions)\n\nForeignGross: Gross income for foreign viewers (in millions)\n\nProfitability: WorldGross as a percentage of Budget\n\nOpenProfit: Percentage of budget recovered on opening weekend\n\nYear: Year the movie was released\n\nIQ1-IQ50: IQ score of each of 50 audience raters (every movie had different raters)\n\nSnacks: How many of the 50 audience raters brought snacks\n\nPrivateTransport: How many of the 50 audience raters reached the cinema via private transportation"
  },
  {
    "objectID": "1_11_sampling.html#tasks",
    "href": "1_11_sampling.html#tasks",
    "title": "Sampling",
    "section": "\n2 Tasks",
    "text": "2 Tasks\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task B5.\nB1) Create and summarise categorical variables, before calculating probabilities.\nB2) Investigate if events are independent, and compute probabilities.\nB3) Computing and plotting probabilities with a binomial distribution. B4) Computing and plotting probabilities with a normal distribution.\n\n\n\n\n\n\nThis week’s task\n\n\n\nB5) Plot standard error of the mean, and finish the report write-up (i.e., knit to PDF, and submit the PDF for formative feedback)."
  },
  {
    "objectID": "1_11_sampling.html#b5-sub-tasks",
    "href": "1_11_sampling.html#b5-sub-tasks",
    "title": "Sampling",
    "section": "\n3 B5 sub-tasks",
    "text": "3 B5 sub-tasks\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\nIn this section you will find some guided sub-steps you may want to consider to complete task B5.\n\nContext\nAn investor has gotten in touch and has asked you to consult on what Lead Studio they should invest in, based on audience scores of the top three movie genres. They want to be presented with two options:\nA. High risk option: Highest average audience score, irrespectively of the standard error\nB. Low risk option: Small spread in audience scores, but with highest average audience score possible among those\n\n\nReopen last week’s Rmd file, and continue building on last week’s work. Make sure you are still using the movies dataset filtered to only include the top 3 genres.1\n\n\n\nVisualise the distribution of the audience scores and note if there are any unusual observations (e.g., outliers).2\n\n\n\nTo advise the investor on the average audience score, calculate the sample mean and standard error.3\n\n\n\\[\nSE_{\\bar x} = \\frac{s}{\\sqrt{n}}\n\\]\n\nCalculate the estimated means, along with a measure of their variability, across Lead Studios, and interpret your output4.\n\n\nIs the variation of means equal to or less than the variability of the sample data?5\n\n\n\nVisualise the average audience scores and SEs - how do these vary between the Lead Studios6?\n\n\nBased on what you have reported above, what high and low risk options will you present to the investor? Justify your answer.\nIn the analysis section of your report, write up a summary of what you have reported above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions.\nOrganise the Rmd file to have the following structure (click ‘Report B Structure’ below to expand)\n\n\n\n\n\n\n\nReport B Structure\n\n\n\n\n\n---\ntitle: \"Formative report B\"\nauthor: \"Group ?.?\"\ndate: \"Write the date here\"\noutput: bookdown::pdf_document2\ntoc: false\n---\n\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)\n```\n\n\n```{r, include=FALSE}\n# week 7 code below\nlibrary(tidyverse)\n\n# week 8 code below\npltEye &lt;- ggplot(starwars, aes(x = eye_color)) + \n    geom_bar()\n\n# week 9 code below\n\n# week 10 code below\n\n# week 11 code below\n```\n\n## Introduction\n\nWrite here an introduction to the data, the variables, and anything worth of notice in the data.\n\n\n## Analysis\n\nPresent here your tables, plots, and results. In the code chunk below, you do \nnot need to put the chunk option `echo=FALSE` as you set this option globally \nin the setup chunk. \n\n```{r}\npltEye\n```\n\nIf you didn't set it globally, you would need to put it in the chunk options:\n\n```{r, echo=FALSE}\npltEye\n```\n\nMore text...\n\n\n## Discussion\n\nWrite up your take home messages here...\n\n## Appendix A - Additional tables and figures\n\nInsert here any additional tables or figures that you could not fit in the \n6-page limit.\n## Appendix B - R code\n\n```{r, echo=TRUE, results='hide', fig.show='hide'}\n# copy and paste here all your R code\n\n# week 1 code below\nlibrary(tidyverse)\n\n# week 2 code below\npltEye &lt;- ggplot(starwars, aes(x = eye_color)) + \n    geom_bar()\n\n# week 3 code below\n\n# week 4 code below\n```\n\n\nKnit the document to PDF\n\nSubmit the PDF file on Learn:\n\nGo to the Learn page of the course\nClick Assessments on the left-hand side menu\nClick Report submission\nClick Formative Report B\nFollow the instructions\n\n\n\n\n\n\n\n\n\nReferencing figures\n\n\n\n\n\nFirst, you need to pick a unique label for the code chunk that displays the figure, in this case short-label but you should use a more descriptive name.\n```{r short-label, fig.cap = \"Figure caption\"}\npltEye &lt;- ggplot(starwars, aes(x = eye_color)) + \n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\npltEye\n```\n\n\n\n\nFigure caption\n\n\n\nTo reference a figure, for example the one above, you would \nwrite see Figure \\@ref(fig:short-label).\nwhich, when you Knit to PDF, becomes:\nTo reference a figure, for example the one above, you would write see Figure 1.\n\n\n\n\n\n\n\n\n\nReferencing tables\n\n\n\n\n\nFirst, you need to pick a unique label for the code chunk that displays the table, in this case tbl-short-label but you should use a more descriptive name.\n```{r tbl-short-label, echo=FALSE}\nlibrary(kableExtra)\ntblEye &lt;- starwars %&gt;%\n    count(eye_color) %&gt;%\n    kbl(booktabs = TRUE, caption = \"Short table caption\")\ntblEye\n```\n\n\n\nShort table caption\n\neye_color\nn\n\n\n\nblack\n10\n\n\nblue\n19\n\n\nblue-gray\n1\n\n\nbrown\n21\n\n\ndark\n1\n\n\ngold\n1\n\n\ngreen, yellow\n1\n\n\nhazel\n3\n\n\norange\n8\n\n\npink\n1\n\n\nred\n5\n\n\nred, blue\n1\n\n\nunknown\n3\n\n\nwhite\n1\n\n\nyellow\n11\n\n\n\n\n\nThe table is referenced as, see Table \\@ref(tab:tbl-short-label).\nWhich, when you knit to PDF, is displayed as:\nThe table is referenced as, see Table 1.\nFor details on styling PDF tables, see this link.\n\n\n\n\n\n\n\n\n\nReducing figure size\n\n\n\n\n\nYou could place multiple panels into a single figure using the functions | and / from the patchwork package.\nYou could adjust the figure height and width by playing with a few options for the numbers fig.height = ? and fig.width = ?, for example 5 and 4, or 12 and 8, and so on. Please note this is typically found by trial and error. Keep in mind, however, that the figure labels should still be legible in the plot you show.\n```{r, fig.height = 5, fig.width = 4}\n# your code to display the figure here\n```\n\n\n\n\n\n\n\n\n\nHiding R code or ouput\n\n\n\n\n\n\n\nHiding R code\nHiding R output\nHiding R code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\nTo hide both text output and figures, use:\n```{r, results='hide', fig.show='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```\n\n\n\n\n\n\n\n\n\nThis is the metadata block. It includes the:\n\ndocument title\nauthor name\ndate (to leave empty, use an empty string \"\")\nthe output type\n\nThe output type could be html_document, pdf_document, etc.\nWe use bookdown::pdf_document2 so that we can reference figures, which pdf_document doesn’t let you do.\nThe code bookdown::pdf_document2 simply means to use the pdf_document2 type from the bookdown package.\nThe code toc: false hides the table of contents.\n\nThis is the setup chunk and should always be included in your Rmd document. It sets the global options for all code chunks that will follow.\n\nIf echo=TRUE, the R code in chunks is displayed. If FALSE, not.\nIf message=TRUE, information messages are displayed. If FALSE, not.\nIf warning=TRUE, warning messages are printed. If FALSE, not.\n\nIf you want to change the setting in a specific code chunk, you can do so via:\n```{r, echo=FALSE}\n# A code chunk\n```\n\nThis code chunks contains your rough work from each week. Give names to plots and tables, so that you can reference those later on. The option include=FALSE hides both code and output.\nTo run each line of code while you are working, put your cursor on the line and press Control + Enter on Windows or Command + Enter on a macOS.\n\nThis contains your actual textual reporting, as well as tables and figures. To show in place a plot previously created, just include the plot name in a code chunk with the option echo = FALSE to hide the code but display the output.\n\nThis allows the marker to see the code you used to obtain your results. Please note that only the code should be visible in the appendix, no output.\nThe chunk options echo=TRUE, results='hide', fig.show='hide' ensure that the appendix code is visible (echo=TRUE), the output is hidden (results=‘hide’), and figures are hidden (fig.show=‘hide’).\nThe appendix does not count towards the 4-page limit."
  },
  {
    "objectID": "1_11_sampling.html#worked-example",
    "href": "1_11_sampling.html#worked-example",
    "title": "Sampling",
    "section": "\n4 Worked Example",
    "text": "4 Worked Example\nThe dataset available at https://uoepsy.github.io/data/RestaurantTips.csv was collected by the owner of a US bistro, and contains 157 observations on 7 variables.7\nThe bistro owners servers are concerned that some shifts are more profitable than others, and that their rota needs to be updated so that they all get the chance to maximise their tips. They have asked the owner to find out what days the highest percentage of tips are given, on average. They have also asked the owner to tell them the days on which variation in percentage tips is highest and lowest. We need to advise the bistro owner so that they can update their servers with the requested information.\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\n\nFirst we want to prepare our data, and check for any unusual or impossible values (e.g., outliers). One useful way to do this would be to plot our data:\n\n\nggplot(tips, aes(x = PctTip)) + \n    geom_histogram()\n\n\n\n\nWe can see one outlier (on the far right of the plot), where the percentage tip appears to be more than 2 x the total bill(!), so lets inspect that outlier:\n\ntips %&gt;%\n    filter(PctTip &gt; 100)\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  49.6    NA y           4 th    C         221\n\n\nWe can see that the ‘Tip’ column has an NA value, so perhaps the ‘PctTip’ value of 221 was a data input error? If so, we want to remove the outlier:\n\ntips &lt;- tips %&gt;%\n    filter(PctTip &lt;= 100)\n\n\nSince we are interested in looking at the percentage tips across weekdays, we may want to give our ‘Day’ variable better labels for levels:\n\n\ntips$Day &lt;- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"))\n\n\nIf we were asked to describe the shape of the distribution of the ‘PctTip’ variable, we could use either a histogram, a density plot, or a boxplot:\n\n\n\nHistogram\nDensity\nBoxplot\n\n\n\n\nggplot(tips, aes(x = PctTip)) + \n    geom_histogram(colour = 'white')\n\n\n\n\n\n\n\nggplot(tips, aes(x = PctTip)) + \n    geom_density()\n\n\n\n\n\n\n\nggplot(tips, aes(x = PctTip)) + \n    geom_boxplot()\n\n\n\n\n\n\n\nThe distribution of percentage tip is not exactly normal as it shows a slight skew to the right. This suggests that there were more individuals tipping well above the mean than below (i.e., more extremely high tips)\n\nNow that we have visualised our distribution, it would be useful to estimate the centre and spread of our data. In other words, calculate the sample mean and standard error of the mean.\n\n\n\n\n\n\n\nSample Statistics\n\n\n\n\n\n\nMean: the sample mean \\(\\bar{x}\\) (x-bar) is the mean computed from the sample data (sample statistic). As this is an estimate of the population mean \\(\\mu\\), it can also be denoted \\(\\hat{\\mu}\\) (mu-hat).\nStandard error: The standard error of the mean, denoted \\(SE\\) or \\(SE_{\\bar x}\\), is the standard deviation of its sampling distribution. The standard error is \\(SE_{\\bar x} = \\sigma / \\sqrt{n}\\) but, as we do not know the population standard deviation \\(\\sigma\\), we must estimate it with the sample standard deviation \\(s\\), leading to this formula for the standard error of the mean:\n\n\\[\nSE_{\\bar x} = \\frac{s}{\\sqrt{n}}\n\\]\n\n\n\nWe can calculate our sample statistics as follows:\n\nn_tips &lt;- nrow(tips)\nn_tips\n\n[1] 156\n\nxbar_tips &lt;- mean(tips$PctTip)\nxbar_tips\n\n[1] 16.59103\n\nse_tips &lt;- sd(tips$PctTip) / sqrt(n_tips)\nse_tips\n\n[1] 0.3511618\n\n\n\nWe can then check how our sample statistics vary across each day of the week:\n\n\nlibrary(kableExtra)\n\ntbl_tips &lt;- tips %&gt;%\n    group_by(Day) %&gt;% \n    summarise(n = n(),\n              xbar = mean(PctTip),\n              se = sd(PctTip) / sqrt(n)) %&gt;%\n    mutate(xbar = round(xbar, 2),\n           se = round(se, 2))\ntbl_tips %&gt;%\n    kbl(digits = 2, booktabs = TRUE)\n\n\n\n\n\nTable 1: Descriptive statistics of tips, as a percentage of the total bill, by day\n\nDay\nn\nxbar\nse\n\n\n\nMon\n20\n15.94\n0.72\n\n\nTue\n13\n18.02\n2.11\n\n\nWed\n62\n16.55\n0.44\n\n\nThu\n35\n16.75\n0.57\n\n\nFri\n26\n16.26\n1.21\n\n\n\n\n\n\n\n\n\n\nIf we were asked to interpret the sample statistics for each day, we could summarise as below:\n\nInterpreting \\(\\bar{x}\\) / \\(\\hat{\\mu}\\)\n\nOf the days of the week, Tuesday was when the highest average percentage tips were received, and Monday the lowest.\nApart from Tuesday (when the average percentage tip is likely to be above average), the other days of the week are very close to 15%.\n\n\nInterpreting \\(SE\\)\n\nThe percentage of tips varied most on Tuesdays on Fridays, where tips would either be very generous or measly.\n\n\n\n\nNext we want to visualise the association between days and percentage tip. We can either do this using ggplot() or do so manually based on our tibble we created above (tbl_tips):\n\n\n\nggplot()\nManually\n\n\n\n\nplt1 &lt;- ggplot(tips, aes(x = Day, y = PctTip)) + \n    stat_summary(fun.data = function(x) mean_se(x, 2)) +\n    ylim(0, 50)\nplt1\n\n\n\nFigure 1: % Tips by Day.\n\n\n\n\n\n\nplt2 &lt;- ggplot(tbl_tips) +\n    geom_pointrange(aes(x = Day, y = xbar,\n                        ymin = xbar - 2 * se,\n                        ymax = xbar + 2 * se)) + \n    ylim(0,50)\nplt2\n\n\n\nFigure 2: % Tips by Day.\n\n\n\n\n\n\nWe can then arrange our two plots side by side to see if they match, which they do:\n\nplt1 | plt2\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe know that the variability of the mean Percentage Tip across each day of the week should be equal to or less than the variability of the sample data. We can check that this is the case:\n\n\ntips %&gt;%\n    group_by(Day) %&gt;% \n    summarise(n = n(),\n              xbar = mean(PctTip),\n              SD = sd(PctTip),\n              SE = SD / sqrt(n)) %&gt;%\n    mutate(IsSESmaller = SE &lt; SD) \n\n# A tibble: 5 × 6\n  Day       n  xbar    SD    SE IsSESmaller\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;      \n1 Mon      20  15.9  3.20 0.716 TRUE       \n2 Tue      13  18.0  7.60 2.11  TRUE       \n3 Wed      62  16.6  3.43 0.436 TRUE       \n4 Thu      35  16.8  3.37 0.569 TRUE       \n5 Fri      26  16.3  6.17 1.21  TRUE       \n\n\nFor each entry in the ‘IsSESmaller’ column, we can see that it is true!\n\n\n\n\n\n\nExample writeup\n\n\n\nThe bistro servers are correct - percentage tips do vary by day (see Table 1). These differences are displayed in Figure 1.\nAs displayed above, Tuesdays are when servers received the highest average percentage tips, and Mondays were the lowest. In terms of variability, all days of the week (aside from Tuesday when the percentage of tip is more likely to be above average), are very close to the average tipping rate (15%)."
  },
  {
    "objectID": "1_11_sampling.html#student-glossary",
    "href": "1_11_sampling.html#student-glossary",
    "title": "Sampling",
    "section": "\n5 Student Glossary",
    "text": "5 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\nfilter\n?\n\n\nfactor\n?\n\n\ngroup_by()\n?\n\n\ngeom_histogram()\n?\n\n\ngeom_boxplot()\n?\n\n\ngeom_density()\n?\n\n\ngeom_line()\n?\n\n\nafter_stat()\n?"
  },
  {
    "objectID": "1_11_sampling.html#footnotes",
    "href": "1_11_sampling.html#footnotes",
    "title": "Sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: Ask last week’s driver for the Rmd file, they should share it with the group via email or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\nHint: It would be a good idea to visualise your data to try and identify any outliers in the data. You may want to consider one of these functions: geom_density(), geom_histogram(), geom_boxplot().↩︎\n\nHint: You can calculate the mean and standard error as:\n\nMean: the sample mean \\(\\bar{x}\\) (x-bar) is the mean computed from the sample data (sample statistic). As this is an estimate of the population mean \\(\\mu\\), it can also be denoted \\(\\hat{\\mu}\\) (mu-hat).\nStandard error: The standard error of the mean, denoted \\(SE\\) or \\(SE_{\\bar x}\\), is the standard deviation of its sampling distribution. The standard error is \\(SE_{\\bar x} = \\sigma / \\sqrt{n}\\) but, as we do not know the population standard deviation \\(\\sigma\\), we must estimate it with the sample standard deviation \\(s\\), leading to this formula for the standard error of the mean:\n\n↩︎\n\nHint: In order to have sample statistics for each Lead Studio, you will need to use the group_by() function before summarising the data with the sample mean and the standard error.↩︎\nHint: Compare the standard error to the standard deviation. Recall, the standard deviation tells you how much each data point varies around the mean, while the standard error of the mean tells you how much the means (from different random samples) vary with respect to their mean (the unknown population mean).↩︎\n\nHint: Some of these functions may be useful: stat_summary(), geom_pointrange().\nYou can also use the coord_flip() function to ‘flip’ your axes - it might make it easier to read your plot.↩︎\n\nData adapted from Lock et al. (2020).↩︎"
  },
  {
    "objectID": "1_09_discrete_dist.html",
    "href": "1_09_discrete_dist.html",
    "title": "Random Variables (Discrete)",
    "section": "",
    "text": "Instructions Recap - Formative Report B\n\n\n\n\n\n\nIn this block of the course (weeks 7-11), you should produce a PDF report using Rmarkdown for which you will receive formative feedback in week 12.\nThe report should not include any reference to R code or functions, but be written or a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\nYou will be required to submit a PDF file by 12 noon on Friday the 2nd of December 2022 via Learn. One person needs to submit on behalf of your group.\n\nThe report should be at most 6 pages long. At the end of the report, you are allowed two appendices which both don’t count towards the page limit.\n\nAppendix A will contain any tables or figures which you cannot fit in the page limit (no text allowed)\nAppendix B will contain the code to reproduce the report results (just like Formative Report A).\n\n\nNo extensions allowed. As this is group-based work, no extensions are possible."
  },
  {
    "objectID": "1_09_discrete_dist.html#tasks",
    "href": "1_09_discrete_dist.html#tasks",
    "title": "Random Variables (Discrete)",
    "section": "\n1 Tasks",
    "text": "1 Tasks\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task B3.\nB1) Create and summarise categorical variables, before calculating probabilities.\nB2) Investigate if events are independent, and compute probabilities.\n\n\n\n\n\n\nThis week’s task\n\n\n\nB3) Computing and plotting probabilities with a binomial distribution.\n\n\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Plot standard error of the mean, and finish the report write-up (i.e., knit to PDF, and submit the PDF for formative feedback)."
  },
  {
    "objectID": "1_09_discrete_dist.html#b3-sub-tasks",
    "href": "1_09_discrete_dist.html#b3-sub-tasks",
    "title": "Random Variables (Discrete)",
    "section": "\n2 B3 sub-tasks",
    "text": "2 B3 sub-tasks\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nFocus on completing all of the lab tasks, and leave non-essential things like changing colors for later.\nIf, after looking at the hint, you still have no clue on how to answer a question, check the worked example below!\n\n\n\nIn this section you will find some guided sub-steps you may want to consider to complete task B3.\nA new movie theatre is opening in Texas, and the management team are reviewing the requirements for snack stalls and their car parking capacity. They are interested in determining whether to have only one or multiple snack stalls selling popular food and drink items; and are considering whether to expand their car park size or introduce a bus stop on cinema grounds.\nA recent survey suggested that 49% of movie viewers always buy some form of snack item (i.e., popcorn, drinks, sweets) when watching a movie, and another suggested that 70% of movie viewers travel to the cinema via private transport.\nIn this lab, you will need to consider both the Snacks and Private Transport variables from the Hollywood movies dataset when answering the questions below.\n\nReopen last week’s Rmd file, and continue building on last week’s work. Make sure you are still using the movies dataset filtered to only include the top 3 genres.1\n\n\n\nConsider the Snacks and Private Transport variables, are they discrete or continuous?\nPlot separately the sample frequency distribution of both the Snacks and Private Transportvariables with barplots.2\n\n\nWhat kind of distribution do these follow? Estimate the parameters of your two distributions from the sample data.3\n\n\n\nPlot the fitted Binomial distribution on top of the sample frequency distribution for each variable. Is the Binomial distribution a good model for each variable?4\n\n\n\nWhat is the probability that exactly half of the audience for each movie (i.e., 25 viewers) bought snacks? What is the probability that exactly half traveled via private transport?5\n\n\n\nWhat is the probability that more than half of the audience for each movie (i.e., &gt;25 viewers) bought snacks? What is the probability that less than half of the audience for each movie (i.e., &lt;25 viewers) traveled via private transport?6\n\n\n\nBased on the probabilities you have reported above, do you think that the new movie theatre should (1) invest in multiple snack stations, or just one; and (2) increase car parking capacity or add a bus stop? Justify your answer.\nAre your two estimates consistent with the survey-reported values?7\n\n\nIn the analysis section of your report, write up a summary of what you have reported above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions."
  },
  {
    "objectID": "1_09_discrete_dist.html#worked-example",
    "href": "1_09_discrete_dist.html#worked-example",
    "title": "Random Variables (Discrete)",
    "section": "\n3 Worked Example",
    "text": "3 Worked Example\nThe dataset available at https://uoepsy.github.io/data/RestaurantTips2.csv was collected by the owner of a US bistro, and contains 99 observations on 10 variables. It is a subset of the RestaurantTips.csv data presented in the past weeks, focusing only on parties of 2 people.8\nThe bistro owner is interested in coffee sales, and whether they should consider introducing a 2 for 1 coffee deal to entice customers to purchase one of their Christmas coffees. Another option they are considering is starting a loyalty scheme for customers to be rewarded for every coffee purchase. Your job is to advise them on which scheme they should run to benefit most customers.\nFor context, Americans drink a lot of coffee, but slightly less than Norwegians (89.4% of Norwegians drink at least one coffee per day!9). We are interested in estimating the probability that an adult American will drink coffee.\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\nHadCoffee\nNumber of guests in the group who had coffee\n\n\nIQ1\nScore on IQ test for guest 1\n\n\nIQ2\nScore on IQ test for guest 2\n\n\n\n\n\n\nlibrary(tidyverse)\ntips2 &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips2.csv\")\nhead(tips2)\n\n# A tibble: 6 × 10\n   Bill   Tip Credit Guests Day   Server PctTip HadCoffee   IQ1   IQ2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2         2    93   100\n2  32.0  5.01 y           2 f     A        15.7         2    96    98\n3  17.4  3.61 y           2 f     B        20.8         2    94    99\n4  15.4  3    n           2 f     B        19.5         2    99   108\n5  18.6  2.5  n           2 f     A        13.4         2   129   106\n6  21.6  3.44 n           2 f     B        16           2    82   118\n\n\n\nIf we were asked to describe what kind of variable HadCoffee is, and to comment on the kind of probability distribution it may follow, we could say:\n\nThe number of coffee-consuming guests out of parties of size 2 is a discrete random variable that could be modeled by a Binomial probability distribution.\n\nWe can plot the frequency distribution of the HadCoffee variable in the sample as following:\n\nCompute the frequency distribution and relative frequency:\n\nfreq_distr &lt;- tips2 %&gt;%\n    count(HadCoffee) %&gt;%\n    mutate(rel_freq = n / sum(n)) %&gt;%\n    rename(freq = n)    # rename(new_name = current_name)\n                        # this renames column 'n' to 'freq'\nfreq_distr\n\n# A tibble: 3 × 3\n  HadCoffee  freq rel_freq\n      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n1         0     9   0.0909\n2         1    29   0.293 \n3         2    61   0.616 \n\n\nPlot the frequency distribution using a barplot:\n\n\ngeom_col() is used to create a barplot when you have already computed the bar heights, i.e. the frequency table.\ngeom_bar() takes the original data, and does the counting for you.\n\nggplot(freq_distr, aes(x = HadCoffee, y = rel_freq)) +\n    geom_col() +\n    labs(x = \"Coffee-drinking customers per party\",\n         y = \"Relative frequency\")\n\n\n\n\n\n\n\n\n\n\nFitting a distribution\n\n\n\n\n\nFitting a Binomial distribution to data involves estimating the parameters of the distribution from the data. In other words, we want to find values for \\(n\\) and \\(p\\) from the variable HadCoffee in the our data.\n\n\n\n\n\nWe can now fit a Binomial distribution to the variable. To do so, we need to start by estimating the parameters of the Binomial distribution:\n\n\n\\(n\\), the number of trials (or size)\n\n\\(p\\), the probability of success\n\n\n\nFor this dataset, \\(n\\) represents the size of each party, i.e. \\(n = 2\\). The discrete variable HadCoffee represents how many guests had coffee, out of the 2 possible guests per party.\n\n\nIn a Binomial distribution, the number of trials (\\(n\\)) should not be confused with the sample size. The sample size would be the total number of parties in the dataset, i.e. 99. It’s just unfortunate that both use the same symbol \\(n\\), but which one is the correct one should be clear from the context.\nThe event “had coffee” represents our “success”, and \\(p\\) denotes the probability of success. In other words, \\(p\\) represents the probability of an individual having coffee.\n\n\n\n\n\n\nEstimating the probability of success\n\n\n\n\n\nThe expected value \\(E(X)\\) of a Binomial distribution (i.e., the mean) is \\(E(X) = n * p\\) where \\(n\\) = number of trials = size, and \\(p\\) is the probability of success. From this, we have that \\(p = E(X) / n\\).\nWe typically denote the estimated probability of success from the sample data with a hat on top, \\(\\hat{p}\\), written in text as $\\hat{p}$.\n\n\n\n\n# The mean of the discrete random variable\nEX &lt;- mean(tips2$HadCoffee)  \n\n# Each party size (n in the formula)\nsize &lt;- 2\n\n# Estimated probability of having coffee: E(X) / n\np_hat &lt;- EX / size\n\n#check value\np_hat\n\n[1] 0.7626263\n\n\n\nWe can then compare the sample frequency distribution to the Binomial distribution, and comment on whether the Binomial fit is good:\n\nWe can create a new tibble having two columns. The first has the possible values of the Binomial distribution (0, 1, or 2 guests ordering coffee out of the 2). The second column has the theoretical probabilities, for each of the possible values, predicted by the Binomial distribution with parameters \\(n = 2\\) and \\(p = 0.76\\).\n\nbinom_distr &lt;- tibble(\n    HadCoffee = 0:2,\n    binom_prob = dbinom(x = HadCoffee, size = 2, prob = p_hat)\n)\nbinom_distr\n\n# A tibble: 3 × 2\n  HadCoffee binom_prob\n      &lt;int&gt;      &lt;dbl&gt;\n1         0     0.0563\n2         1     0.362 \n3         2     0.582 \n\n\nWe can plot the sample frequency distribution as a bar plot and put on top the fitted Binomial probability distribution as dots (see Option 1), and even add segments to show the Binomial distribution (see Option 2).\n\n\noption 1\noption 2\n\n\n\n\nggplot() +\n    geom_col(data = freq_distr, aes(x = HadCoffee, y = rel_freq)) +\n    geom_point(data = binom_distr, \n               aes(x = HadCoffee, y = binom_prob),  # adds points\n               colour = 'red', size = 3)\n\n\n\n\n\n\n\nggplot() +\n    geom_col(data = freq_distr, aes(x = HadCoffee, y = rel_freq)) +\n    geom_point(data = binom_distr, \n               aes(x = HadCoffee, y = binom_prob),  # adds points\n               colour = 'red', size = 3) +\n    geom_segment(data = binom_distr, \n                 aes(x = HadCoffee, y = binom_prob, # adds line (optional) \n                     xend = HadCoffee, yend = 0),   # from (x,y) to (xend,yend)\n                 colour = 'red')\n\n\n\nFigure 1: Probability of guests drinking coffee.\n\n\n\n\n\n\nThe Binomial distribution seems to be a good fit for the sample distribution, as the probabilities tend to agree. Among all parties of two guests, the highest probability is that both guests had coffee, and no one having coffee has the lowest probability.\n\n\n\n\n\n\nProbability Mass Function\n\n\n\n\n\n\n\n\\(P(X = x)\\) = dbinom(x, size, prob)\n\n\nThe probability mass function computes \\(P(X = x)\\) for a Binomial distribution where number of trials is size and probability of success is prob.\n\n\n\n\nTo calculate the probability that 1 person in the party orders coffee, we can compute the following:\n\nThe probability P(X = 1) for a Binomial with \\(x = 1\\), \\(size = 2\\), and \\(p = 0.76\\) is:\n\ndbinom(1, size = 2, prob = p_hat)\n\n[1] 0.3620549\n\n\n\nTo calculate the probability that 2 people in the party order coffee, we can compute the following:\n\nThe probability P(X = 2) for a Binomial with \\(x = 2\\), \\(size = 2\\), and \\(p = 0.76\\) is:\n\ndbinom(2, size = 2, prob = p_hat)\n\n[1] 0.5815988\n\n\n\n\n\n\n\n\nCumulative Distribution Function\n\n\n\n\n\n\n\n\\(P(X \\leq q)\\) = pbinom(q, size, prob)\n\n\nThe cumulative distribution function \\(P(X \\leq q)\\) gives the probability of having less than or equal to \\(q\\) successes.\nFrom the total probability rule, the probability of a value being greater than \\(q\\) is computed as \\(P(X &gt; q) = 1 - P(X \\leq q)\\):\n# option 1: P(X &gt; q) = 1 - P(X &lt;= q)\n1 - pbinom(q, size, prob)\n# option 2: P(X &gt; q) directly\npbinom(q, size, prob, lower.tail = FALSE)\n\n\n\n\nTo calculate the probability that 1 person or less orders coffee, we can compute the following:\n\nWe do this with the pbinom(q, size, prob) function, with q = 1 to have \\(P(X \\leq 1) = P(X = 0) + P(X = 1)\\):\n\npbinom(1, size = 2, prob = p_hat)\n\n[1] 0.4184012\n\n\nYou can also see this by creating a new column in the binomial distribution which has the cumulative sums of the probabilities, i.e. the values P(X = 0), followed by P(X = 0) + P(X = 1), and finally by P(X = 0) + P(X = 1) + P(X = 2):\n\nbinom_distr %&gt;% \n    mutate(\n        cumul_prob = cumsum(binom_prob)\n    )\n\n# A tibble: 3 × 3\n  HadCoffee binom_prob cumul_prob\n      &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1         0     0.0563     0.0563\n2         1     0.362      0.418 \n3         2     0.582      1     \n\n\nAs you can see, P(X = 0) + P(X = 1) is 0.42 in the second row, which agrees with the result computed using pbinom above.\n\nTo calculate the probability that at least one person from the group orders coffee, we can compute one of the below four equivalent calucaltions:\n\n\n\n1 - P(X &lt;= 0)\nP(X &gt; 0)\nP(X = 1) + P(X = 2)\n1 - P(X = 0)\n\n\n\n\n1 - pbinom(0, size = 2, prob = p_hat)\n\n[1] 0.9436537\n\n\n\n\n\npbinom(0, size = 2, prob = p_hat, lower.tail = FALSE)\n\n[1] 0.9436537\n\n\n\n\n\n# sum up the probability of that 1 person orders coffee and the probability that both persons order coffee\n# P(1 guest has coffee) + P(2 guests has coffee)\ndbinom(1, size = 2, prob = p_hat) + dbinom(2, size = 2, prob = p_hat)\n\n[1] 0.9436537\n\n\n\n\n\n# 1 - P(X = 0)\n# As the total probability is 1, we can get it as 1 - P(0 guests have coffee). \n# This follows from the total probability rule\n1 - dbinom(0, size = 2, prob = p_hat)\n\n[1] 0.9436537\n\n\n\n\n\n\nTo check whether our estimated probability of drinking coffee consistent with the one reported by a recent YouGov survey, which reported that three quarters of adult Americans drink coffee, we can take a look at our estimated \\(\\hat{p}\\). We can also then comment on whether adult Americans are more or less likely to drink coffee than Norwegians?\n\n\n# YouGov reported probability\np_survey &lt;- 3/4\np_survey\n\n[1] 0.75\n\n\n\n# Our estimate\np_hat\n\n[1] 0.7626263\n\n\nThe estimated probability of drinking coffee, based on the data from a US bistro, is 0.76. This is relatively close to the YouGov reported result of 0.75. The small deviation may be due to sampling variability, due to having chosen a different sample of people from the one that were considered in this bistro.\nAccording to a recent survey, 89.4% of Norwegians drink at least one coffee per day10 As such, while adult Americans drink lots of coffee, an adult American is less likely to drink coffee than a Norwegian.\n\n\n\n\n\n\nExample writeup\n\n\n\nFigure 1 displays the distribution of coffee-drinking customers in two-party tables, with a Binomial fit superimposed as red dots. Both party guests are more likely to both drink coffee (0.58) than not (0.06). The probability of only one guest drinking coffee is 0.36. As such, the probability of at least one guest drinking coffee is 0.94. The owner of the bistro should not consider running a 2 for 1 offer on Christmas coffees, as they would lose out on income. It is most likely that groups of two customers will already buy two coffees when they visit the bistro (0.58). Instead, the owner might want to offer a loyalty scheme to reward customers for purchasing coffees, regardless of the quantity, as this will also reward parties of two where only one person purchases a coffee."
  },
  {
    "objectID": "1_09_discrete_dist.html#student-glossary",
    "href": "1_09_discrete_dist.html#student-glossary",
    "title": "Random Variables (Discrete)",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\nrename()\n?\n\n\ndbinom()\n?\n\n\npbinom()\n?\n\n\ngeom_col()\n?\n\n\ngeom_point()\n?\n\n\ngeom_segment()\n?\n\n\ncumsum()\n?"
  },
  {
    "objectID": "1_09_discrete_dist.html#footnotes",
    "href": "1_09_discrete_dist.html#footnotes",
    "title": "Random Variables (Discrete)",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: Ask last week’s driver for the Rmd file, they should share it with the group via email or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nHint:\nFirst compute the frequency distribution of each variable, using count() and mutate().\nNext, plot each frequency distribution as a barplot. Since you already have the bar heights, use geom_col() to create the bars.\nWarning: The function geom_bar(), instead, does the counting for you and uses the data set instead of the frequency table.↩︎\n\n\nHint: A Binomial distribution has two parameters, the number of trials or size (\\(n\\)) and the probability of success (\\(p\\)).\nTo estimate the size, use the information from the study design.\nTo estimate the probability of success use the following fact. The expected value \\(E(X)\\) of a Binomial distribution (i.e., the mean) is \\(E(X) = n * p\\) where \\(n\\) = number of trials = size, and \\(p\\) is the probability of success. From this, we have that \\(p = E(X) / n\\). In your movies sample data, the number of trials (size) is 50, as you have the number of people that bought snacks or travelled via private transport out of the 50 movie raters.↩︎\n\n\nFirst, create a new tibble with two columns: (1) the possible values of the Binomial distribution, and (2) the probabilities of each outcome.\nNext, take the barplots previously created and use geom_point(data = ?, aes(x = ?, y = ?)) to add the Binomial probabilities on top of each barplot.\nIf the plot looks weird, are your bar heights probabilities or counts? The Binomial probabilities are, as the words says, probabilities so make sure you plot two relatable measures.↩︎\n\n\nHint: Here we need to use the function dbinom(), as we are interested in the probability of getting one specific result.\nThe function dbinom() will require you to state three arguments: (1) x - the value(s) you want to compute the probability for, (2) size - the number of trials, and (3) prob - the probability of success.\ndbinom(x = ?, size = ?, prob = ?)\n↩︎\n\n\nHint: Here we need to use the function pbinom(), as we are interested in the cumulative probability of getting a specific result.\nThe function pbinom() will require you to state four arguments: (1) q - the number of ‘successful’ outcomes, (2) size - the number of trials, (3) prob - the probability of success, and (4) lower.tail - specify which side of the probability distribution to test from, where TRUE if looking for ‘&lt;=’, or FALSE if ‘&gt;’.\npbinom(q = ?, size = ?, prob = ..., lower.tail=FALSE)\n↩︎\n\nHint: Compare your estimated probability of buying snacks to that reported in the above survey (which suggested that 49% of movie viewers always buy some form of snack item (i.e., popcorn, drinks, sweets) when watching a movie); and compare your estimated probability of travelling via private transportation to that reported in the above survey (which suggested that 70% of movie viewers viewers travel to the cinema via private transport).↩︎\nData adapted from Lock et al. (2020).↩︎\nTverdal, A., Hjellvik, V. & Selmer, R. Coffee intake and oral–oesophageal cancer: follow-up of 389,624 Norwegian men and women 40–45 years. Br J Cancer 105, 157–161 (2011).↩︎\nTverdal, A., Hjellvik, V. & Selmer, R. Coffee intake and oral–oesophageal cancer: follow-up of 389,624 Norwegian men and women 40–45 years. Br J Cancer 105, 157–161 (2011).↩︎"
  },
  {
    "objectID": "1_07_prob_theory.html",
    "href": "1_07_prob_theory.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Read the instructions in full - do not skip this section!\n\n\n\n\nRegister for your group on LEARN. Go to the course LEARN page, click Groups, click Labs_1_2_3, find your group and click Join.\nDownload the template Rmd file. Write your work in this file every week, and remember to save it often.\n\nFormative Report B spans the labs of the second block of DAPR1 teaching (weeks 7-11).\n\nReport due date: 12 noon on Friday the 1st December 2023.\nThe report should not include any reference to R code or functions, but be written or a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. The main part of the report should only show text, figures, and tables. Appendix B, which is compulsory, will show all the R code used - keep reading for details.\nThe submitted report must be a PDF file of max 6 sides of A4 paper.\nKeep the default settings in terms of Rmd knitting font and page margins.\n\nAt the end of the file, you will place the appendices and these will not count towards the page limit.\n\nYou can include an optional appendix for additional tables and figures which you can’t fit in the main part of the report;\nYou must include a compulsory appendix listing all of the R code used in the report. The template Rmd file include code that does it for you.\n\n\nNo extensions. As these are group-based submissions, no extensions will be given.\n\n\n\nInstall tinytex. Every single student, when logging into their personal RStudio Server account, must do this once. In order to generate a PDF file from RStudio, you must have a package called tinytex installed. This allows you to “Knit” your Rmd document (i.e. combine together text, code, and output) to produce a PDF file.\nType the following code in your console, and press Enter:\ninstall.packages(\"tinytex\")\nNext, type the following code in your console, press Enter, type Y, press Enter:\ntinytex::install_tinytex()\n\n\n\nThe following useful resources will help you with finalising the report formatting and knitting, prior to submission.\n\nIf you encounter errors when knitting the Rmd file, go through the following checklist to try finding the source of the errors.\n\nSuccessful knitting checklist\n\nCheck the following guide for reporting numbers and statistics in APA style (7th edition).\n\nNumbers and statistics guide, APA style 7th edition\n\n\n\nHide R code\nHide R output\nHide both R code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\nTo hide both text output and figures, use:\n```{r, results='hide', fig.show='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```"
  },
  {
    "objectID": "1_07_prob_theory.html#instructions",
    "href": "1_07_prob_theory.html#instructions",
    "title": "Probability Theory",
    "section": "",
    "text": "Read the instructions in full - do not skip this section!\n\n\n\n\nRegister for your group on LEARN. Go to the course LEARN page, click Groups, click Labs_1_2_3, find your group and click Join.\nDownload the template Rmd file. Write your work in this file every week, and remember to save it often.\n\nFormative Report B spans the labs of the second block of DAPR1 teaching (weeks 7-11).\n\nReport due date: 12 noon on Friday the 1st December 2023.\nThe report should not include any reference to R code or functions, but be written or a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. The main part of the report should only show text, figures, and tables. Appendix B, which is compulsory, will show all the R code used - keep reading for details.\nThe submitted report must be a PDF file of max 6 sides of A4 paper.\nKeep the default settings in terms of Rmd knitting font and page margins.\n\nAt the end of the file, you will place the appendices and these will not count towards the page limit.\n\nYou can include an optional appendix for additional tables and figures which you can’t fit in the main part of the report;\nYou must include a compulsory appendix listing all of the R code used in the report. The template Rmd file include code that does it for you.\n\n\nNo extensions. As these are group-based submissions, no extensions will be given.\n\n\n\nInstall tinytex. Every single student, when logging into their personal RStudio Server account, must do this once. In order to generate a PDF file from RStudio, you must have a package called tinytex installed. This allows you to “Knit” your Rmd document (i.e. combine together text, code, and output) to produce a PDF file.\nType the following code in your console, and press Enter:\ninstall.packages(\"tinytex\")\nNext, type the following code in your console, press Enter, type Y, press Enter:\ntinytex::install_tinytex()\n\n\n\nThe following useful resources will help you with finalising the report formatting and knitting, prior to submission.\n\nIf you encounter errors when knitting the Rmd file, go through the following checklist to try finding the source of the errors.\n\nSuccessful knitting checklist\n\nCheck the following guide for reporting numbers and statistics in APA style (7th edition).\n\nNumbers and statistics guide, APA style 7th edition\n\n\n\nHide R code\nHide R output\nHide both R code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\nTo hide both text output and figures, use:\n```{r, results='hide', fig.show='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```"
  },
  {
    "objectID": "1_07_prob_theory.html#formative-report-b",
    "href": "1_07_prob_theory.html#formative-report-b",
    "title": "Probability Theory",
    "section": "\n2 Formative report B",
    "text": "2 Formative report B\nIn weeks 7-11 of the course your group should produce a PDF report (using Rmarkdown) for which you will receive formative feedback in week 12.\n\n2.1 Data\nHollywood Movies. At the link https://uoepsy.github.io/data/hollywood_movies_subset.csv you will find data on Hollywood movies released between 2012 and 2018 from the top 5 lead studios and top 10 genres. The following variables were recorded:\n\n\nMovie: Title of the movie\n\nLeadStudio: Primary U.S. distributor of the movie\n\nRottenTomatoes: Rotten Tomatoes rating (critics)\n\nAudienceScore: Audience rating (via Rotten Tomatoes)\n\nGenre: One of Action Adventure, Black Comedy, Comedy, Concert, Documentary, Drama, Horror, Musical, Romantic Comedy, Thriller, or Western\n\nTheatersOpenWeek: Number of screens for opening weekend\n\nOpeningWeekend: Opening weekend gross (in millions)\n\nBOAvgOpenWeekend: Average box office income per theater, opening weekend\n\nBudget: Production budget (in millions)\n\nDomesticGross: Gross income for domestic (U.S.) viewers (in millions)\n\nWorldGross: Gross income for all viewers (in millions)\n\nForeignGross: Gross income for foreign viewers (in millions)\n\nProfitability: WorldGross as a percentage of Budget\n\nOpenProfit: Percentage of budget recovered on opening weekend\n\nYear: Year the movie was released\n\nIQ1-IQ50: IQ score of each of 50 audience raters (every movie had different raters)\n\nSnacks: How many of the 50 audience raters brought snacks\n\nPrivateTransport: How many of the 50 audience raters reached the cinema via private transportation\n\n2.2 Tasks\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find the guided sub-steps that you need to consider to complete this week’s task.\n\nB1) Create and summarise categorical variables, before calculating probabilities.\nB2) Investigate if events are independent, and compute probabilities.\nB3) Computing and plotting probabilities with a binomial distribution.\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Plot standard error of the mean, and finish the report write-up (i.e., knit to PDF, and submit the PDF for formative feedback).\n\n\n2.3 B1 sub-tasks\nThis week you will only focus on task B1. Below there are sub-steps you need to consider to complete task B1.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nChoose the top 3 most frequent movie genres, and filter your data to only include these rows.1\n\n\n\nCreate a variable called “Rating” where the Audience Score variable is recoded so that those scoring less than or equal to 50 are coded as “Bad” and those scoring over 50 (i.e., &gt;50) are “Good”.2\n\n\n\nEnsure Rating and Genre are coded as factors.3\n\n\n\nCreate a contingency table displaying how many ratings were good or bad for each of your chosen genres.4\n\n\n\nTransform the table of counts to a relative frequency table.5\n\n\n\nDo the numbers in the table satisfy the requirements of probabilities?6\n\n\n\nInstead of checking the probability requirements manually, add a “sum” column to your relative frequency table.7\n\n\n\nVisualise the relative frequency table as a mosaic plot, making sure to add a main title and clear axis titles.8\n\n\n\nDoes your plot have any NAs? If so, you can drop the missing values before re-plotting.9\n\n\n\nIn the introduction section of your report, write up a small introduction to the data.\nIn the analysis section of your report, write up a summary of what you have reported above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions."
  },
  {
    "objectID": "1_07_prob_theory.html#worked-example",
    "href": "1_07_prob_theory.html#worked-example",
    "title": "Probability Theory",
    "section": "\n3 Worked Example",
    "text": "3 Worked Example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;chr&gt; \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server &lt;chr&gt; \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\nWe can filter our data to only include rows of data from Servers A and B, and save this filtered data to a new dataset called “tips2”.\nBecause we want to include servers A and B, but not C, we can use the != (or does not equal) operator.\n\n\nSome common operators include:\n\n\nOperator\nDescription\n\n\n\n&lt;\nless than\n\n\n&gt;\nmore than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;=\nless than or equal to\n\n\n==\n(only) equal to\n\n\n!=\nnot equal to\n\n\n%in%\nis &lt;left&gt; a member of &lt;right&gt;?\n\n\n\n\ntips2 &lt;- tips %&gt;% \n    filter(Server != \"C\")\n\nConsider if, for example, we wanted to recode the Tip variable so that those tipping less than or equal to 15% were coded as “Below” average, and those tipping over 15% were coded as “Above” average (15% being used as below/above average tips cut-off in relation to usual US tipping rates). To do so, we could create a new variable called “Tip_Avg”.\n\ntips2 &lt;- tips2 %&gt;%\n    mutate(Tip_Avg = ifelse(PctTip &lt;= 15, 'Below', 'Above'))\n\ntable(tips2$Tip_Avg)\n\n\nAbove Below \n   80    45 \n\n\nNow that we have the variables we want, it would be a good point to make these both factors:\n\ntips2$Server &lt;- factor(tips2$Server)\ntips2$Tip_Avg &lt;- factor(tips2$Tip_Avg)\n\nTo visually represent the distribution of how many customers were served by each server and if they left a below or above average tip, we could Create a contingency table:\n\nfreq_tbl &lt;- table(tips2$Server, tips2$Tip_Avg)\nfreq_tbl\n\n   \n    Above Below\n  A    40    20\n  B    40    25\n\n\nWe could then transform the table of counts above to instead represent a relative frequency table:\n\nrel_freq_tbl &lt;- freq_tbl %&gt;%\n    prop.table()\nrel_freq_tbl\n\n   \n    Above Below\n  A  0.32  0.16\n  B  0.32  0.20\n\n\nBefore we interpret our results, we must ensure that the numbers above satisfy the requirements of probabilities. We can do this two ways:\n\n\nManually\nUsing addmargins()\n\n\n\nCheck that all values in the proportions table are greater than or equal 0, all values are less than or equal to 1, and they all sum to 1:\n\nall(rel_freq_tbl &gt;= 0)\n\n[1] TRUE\n\n\n\nall(rel_freq_tbl &lt;= 1)\n\n[1] TRUE\n\n\n\nsum(rel_freq_tbl)\n\n[1] 1\n\n\n\n\nInstead of checking manually, we can use the function addmargins() to check that the probabilities sum to 1:\n\nrel_freq_tbl &lt;- freq_tbl %&gt;%\n    prop.table() %&gt;%\n    addmargins()\nrel_freq_tbl\n\n     \n      Above Below  Sum\n  A    0.32  0.16 0.48\n  B    0.32  0.20 0.52\n  Sum  0.64  0.36 1.00\n\n\n\n\n\nIn order to visualise our results in a figure, we could use a mosaic plot:\n\nlibrary(ggmosaic)\nmos_plot &lt;- ggplot(tips2) +\n    geom_mosaic(aes(x = product(Server, Tip_Avg), fill = Server)) +  \n    labs(title = \"Association between Servers and Tips\", x = \"Tip\", y = \"Server\")\nmos_plot\n\n\n\nFigure 3: Association between Servers and Tips\n\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nMore customers tipped above (64%) than below (36%) average. Both Server A and Server B received an equal distribution of tips above average (32%), but server B had a higher proportion of tips below average (20%) in comparison to server A (16%). These associations are visually represented in Figure 3."
  },
  {
    "objectID": "1_07_prob_theory.html#student-glossary",
    "href": "1_07_prob_theory.html#student-glossary",
    "title": "Probability Theory",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\nfilter\n?\n\n\nmutate\n?\n\n\ndrop_na\n?\n\n\nfactor\n?\n\n\ntable\n?\n\n\nifelse\n?\n\n\nprop.table\n?\n\n\naddmargins\n?\n\n\nall\n?\n\n\n|\n?\n\n\ngeom_mosaic\n?"
  },
  {
    "objectID": "1_07_prob_theory.html#footnotes",
    "href": "1_07_prob_theory.html#footnotes",
    "title": "Probability Theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHint: use sort(table(DATA$COLUMN)) to find top 3 movie genres, and filter() to select the specific 3 genres.\nExample: For the starwars dataset, we can filter to include only the two most frequent species, Humans and Droids, via the following code:\n\n# sort the frequency table\nsort(table(starwars$species))\n\n# or, for decreasing order, you can use:\nsort(table(starwars$species), decreasing = TRUE)\n\n# keep only the rows where species is Human or Droid\n# in R, or is the vertical bar |\nstarwars2 &lt;- starwars %&gt;% \n    filter(species == \"Human\" | species ==  \"Droid\")\n\n↩︎\n\n\nHint: use ifelse() and mutate() functions to create a new ‘Rating’ variable capturing whether movies are good or bad.\nExample: For the starwars dataset, we can create a variable to capture whether characters were short (&lt;180cm) or tall (&gt;180cm) based on their recorded height using the following code:\n\nstarwars2 &lt;- starwars2 %&gt;%\n    mutate(size = ifelse(height &lt; 180, 's', 't'))\n\nIn the code above, mutate() changes to data to include a column named size, computed with the computation that follows the equal sign.\nThe ifelse function checks a condition (is height &lt; 180?) and uses the value 's' if the condition is TRUE, and 't' if the condition is FALSE.↩︎\n\n\nHint: use the factor() function.\nExample: For the starwars data, if I wanted to give better labes for my size variable, I could use the following code. Since the names are fine as is for species, I will just specify factor().\n\nstarwars2$size &lt;- factor(starwars2$size,\n                         levels = c(\"s\", \"t\"),\n                         labels = c(\"short\", \"tall\"))\n\nstarwars2$species &lt;- factor(starwars2$species)\n\n↩︎\n\n\nHint: use the table() function, passing to it the two columns that you want to be included in your contingency table.\nExample: For the starwars dataset, if I wanted a contingency table displaying how many humans and droids were short and tall, I could use the following code:\n\nswars_freq_tbl &lt;- table(starwars2$species, starwars2$size)\nswars_freq_tbl\n\n\n        short tall\n  Droid     4    1\n  Human    15   16\n\n\n↩︎\n\n\nHint: use the prop.table() function. You will want to pass your contingency table created above to this.\nExample: For the starwars dataset, if I wanted a relative frequency table, I could use the following code:\n\nswars_rel_freq_tbl &lt;- swars_freq_tbl %&gt;%\n    prop.table()\nswars_rel_freq_tbl\n\n\n             short       tall\n  Droid 0.11111111 0.02777778\n  Human 0.41666667 0.44444444\n\n\n↩︎\n\n\nHint: Recall the requirements of probabilities - are the proportions &gt;= 0 or &lt;= 1?; and that the values in the relative frequency table should sum to 1. The all() and sum() functions could be useful here.\nExample: For the starwars dataset, I could check these requirements using the following code:\n\nall(swars_rel_freq_tbl &gt;= 0)\n\n[1] TRUE\n\nall(swars_rel_freq_tbl &lt;= 1)\n\n[1] TRUE\n\nsum(swars_rel_freq_tbl)\n\n[1] 1\n\n\n↩︎\n\n\nHint: Use the addmargins() function.\nExample: For the starwars dataset, I could add this using the following code:\n\nswars_rel_freq_sum &lt;- swars_freq_tbl %&gt;%\n    prop.table() %&gt;%\n    addmargins()\nswars_rel_freq_sum\n\n\n             short       tall        Sum\n  Droid 0.11111111 0.02777778 0.13888889\n  Human 0.41666667 0.44444444 0.86111111\n  Sum   0.52777778 0.47222222 1.00000000\n\n\n↩︎\n\n\nHint: Make sure to load the ggmosaic() package so that you can specify geom_mosaic() when building your plot. To add a title, as well as x- and y-axis titles, specify labs(title = , x = , y = ).\nExample: For the starwars dataset, I create a mosaic plot using the following code:\n\nlibrary(ggmosaic)\nm_plot &lt;- ggplot(starwars2) +\n    geom_mosaic(aes(x = product(species, size), fill = species)) +\n    labs(title = \"Starwars Mosaic Plot Example Title\", \n         x = \"Size\", \n         y = \"Species\",\n         fill = \"Species\")\nm_plot\n\n\n\nFigure 1: Starwars Mosaic Plot Example Title\n\n\n\n↩︎\n\n\nHint: Here you could make sense of the drop_na() function.\nExample: For the starwars dataset, I do have NAs in my ‘size’ variable, so I need to remove these before re-plotting:\n\nstarwars2 &lt;- starwars2 %&gt;%\n    drop_na(size) %&gt;%\n    mutate(size = factor(size))\n\n\nlibrary(ggmosaic)\nm_plot &lt;- ggplot(starwars2) +\n    geom_mosaic(aes(x = product(species, size), fill = species)) +\n    labs(title = \"Starwars Mosaic Plot Example Title\", \n         x = \"Size\", \n         y = \"Species\",\n         fill = \"Species\")\nm_plot\n\n\n\nFigure 2: Starwars Mosaic Plot Example Title - No NAs\n\n\n\n↩︎"
  },
  {
    "objectID": "1_04_relationships.html",
    "href": "1_04_relationships.html",
    "title": "Relationships",
    "section": "",
    "text": "Instructions and data were released in week 1.\n\n\n\n\n\n\n\n\n\nNext week: Submission of Formative report A\nYour group must submit one PDF file for formative report A by 12 noon on Friday the 20th of October 2023 (next week).\nTo submit go to the course Learn page, click “Assessment”, then click “Submit Formative Report A (PDF file only)”.\nNo extensions. As mentioned in the Assessment Information page, no extensions are possible for group-based reports.\n\n\n\n\n\n\n\n\n\nTip: On the kbl() and kable() functions\nThe two functions are equivalent.\nHowever, if you provide a list of tables, we recommend using the kable() function as it allows you to also provide a caption without errors when knitting.\n\n\n\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choiceA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\nThis week you will only focus on task A4. Below there are some guided sub-steps you may want to consider to complete task A4.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\nChoose two variables and create a plot displaying their relationship.2\n\n\nSummarise the relationship with either a frequency table or descriptive statistics, depending on the type of the variables.3\n\n\n\nChoose a third variable of categorical type, and visualise how the relationship above varies across this third variable.4\n\n\n\nSummarise with a table how the relationship above varies across the third variable.5\n\nOrganise your report to have three sections:\n\n\nIntroduction: where you write a concise introduction to the data for a generic reader\n\nAnalysis: where you present your results, tables, and plots\n\nDiscussion: where you write take-home messages about the data and the insights you discovered\n\n\nKnit the report to PDF, making sure that only text, tables, and plots are visible. Hide the R code chunks so that no R code is visible.\n\n\n\n\n\n\n\nHiding R code and/or ouput\n\n\n\n\n\n\n\nHide code but show output\nShow code but hide output\nHide both code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```"
  },
  {
    "objectID": "1_04_relationships.html#formative-report-a",
    "href": "1_04_relationships.html#formative-report-a",
    "title": "Relationships",
    "section": "",
    "text": "Instructions and data were released in week 1.\n\n\n\n\n\n\n\n\n\nNext week: Submission of Formative report A\nYour group must submit one PDF file for formative report A by 12 noon on Friday the 20th of October 2023 (next week).\nTo submit go to the course Learn page, click “Assessment”, then click “Submit Formative Report A (PDF file only)”.\nNo extensions. As mentioned in the Assessment Information page, no extensions are possible for group-based reports.\n\n\n\n\n\n\n\n\n\nTip: On the kbl() and kable() functions\nThe two functions are equivalent.\nHowever, if you provide a list of tables, we recommend using the kable() function as it allows you to also provide a caption without errors when knitting.\n\n\n\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choiceA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\nThis week you will only focus on task A4. Below there are some guided sub-steps you may want to consider to complete task A4.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\nChoose two variables and create a plot displaying their relationship.2\n\n\nSummarise the relationship with either a frequency table or descriptive statistics, depending on the type of the variables.3\n\n\n\nChoose a third variable of categorical type, and visualise how the relationship above varies across this third variable.4\n\n\n\nSummarise with a table how the relationship above varies across the third variable.5\n\nOrganise your report to have three sections:\n\n\nIntroduction: where you write a concise introduction to the data for a generic reader\n\nAnalysis: where you present your results, tables, and plots\n\nDiscussion: where you write take-home messages about the data and the insights you discovered\n\n\nKnit the report to PDF, making sure that only text, tables, and plots are visible. Hide the R code chunks so that no R code is visible.\n\n\n\n\n\n\n\nHiding R code and/or ouput\n\n\n\n\n\n\n\nHide code but show output\nShow code but hide output\nHide both code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```"
  },
  {
    "objectID": "1_04_relationships.html#worked-example",
    "href": "1_04_relationships.html#worked-example",
    "title": "Relationships",
    "section": "\n2 Worked example",
    "text": "2 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\nWe can replace each factor level with a clearer label:\n\ntips$Day &lt;- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit &lt;- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server &lt;- factor(tips$Server)\n\n\nsummary(tips)\n\n      Bill            Tip         Credit        Guests             Day    \n Min.   : 1.66   Min.   : 0.250   No :106   Min.   :1.000   Monday   :20  \n 1st Qu.:15.19   1st Qu.: 2.075   Yes: 51   1st Qu.:2.000   Tuesday  :13  \n Median :20.22   Median : 3.340             Median :2.000   Wednesday:62  \n Mean   :22.73   Mean   : 3.807             Mean   :2.096   Thursday :36  \n 3rd Qu.:28.84   3rd Qu.: 5.000             3rd Qu.:2.000   Friday   :26  \n Max.   :70.51   Max.   :15.000             Max.   :7.000                 \n                 NA's   :1                                                \n Server     PctTip      \n A:60   Min.   :  6.70  \n B:65   1st Qu.: 14.30  \n C:32   Median : 16.20  \n        Mean   : 17.89  \n        3rd Qu.: 18.20  \n        Max.   :221.00  \n                        \n\n\nThe percentage of total bill has a maximum value of 221, which seems very strange. Someone is very unlikely to tip more than their bill total. In this case 221% of their bill value seems unlikely.\nLet’s inspect the row where PctTip is greater than 100:\n\ntips %&gt;% \n    filter(PctTip &gt; 100)\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;dbl&gt;\n1  49.6    NA Yes         4 Thursday C         221\n\n\nWith a bill of 49.6, the tip would be 109.62 dollars:\n\n49.6 * 221 / 100\n\n[1] 109.616\n\n\nFurthermore, we also notice that the tipping amount is not available (NA). The corresponding value in the percentage of total tip seems likely an inputting error, perhaps due to double typing the leading 2 when recording the data. We will set that value to not available (NA) with the following code:\n\ntips$PctTip[tips$PctTip &gt; 100] &lt;- NA\n\nConsider, for example, the relationship between bill and tip size. As these are two numerical variables, we visualise the relationship with a scatterplot:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\")\n\n\n\nFigure 1: Scatterplot displaying the relationship between bill and tip size\n\n\n\n\n\nThe code starts by setting up a blank canvas for plotting the dataset tip, and placing on the x axis the variable Bill and on the y axis the variable Tip:\nggplot(tips, aes(x = Bill, y = Tip))\nThe following line adds a geometric shape to the plot, in this case points:\ngeom_point()\nThe final line uses more informative labels for the reader, setting a label for the x and y axis respectively:\nlabs(x = \"Bill size (in US dollars)\", \n     y = \"Tip size (in US dollars)\")\nThe layers of the plot need to be added to each other with a + symbol at the end of each line, excluding the last one.\nWe can numerically summarise this relationship with the covariance between the two variables:\n\ncov(tips$Bill, tips$Tip)\n\n[1] NA\n\n\nThere are missing values, so the covariance cannot be computed if one or both of the values \\(X, Y\\) is missing.\nTo fix this, we use the option use = \"pairwise.complete.obs\" to tell R to only keep the complete pairs to compute the covariance, i.e. ignoring pairs where at least one number is NA:\n\nround(cov(tips$Bill, tips$Tip, use = \"pairwise.complete.obs\"), digits = 2)\n\n[1] 25.96\n\n\nTo investigate the relationship between bill and tip size for those who paid by credit card and those who didn’t we can create faceted scatterplots:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit)\n\n\n\n\n\n\nInstead of facet_wrap() you can use the facet_grid() function. This allows you to facet the rows by a variable, and the columns by another variable, or both:\n\n\nfacet_grid(rows ~ .)\n\n\nfacet_grid(. ~ cols)\n\nfacet_grid(rows ~ cols)\n\nTry replacing the last line of code with:\nfacet_grid(Server ~ Credit)\nWe can improve the labelling by using labeller = \"label_both\", which displays not only the group value as label, but both the variable and value:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit, labeller = \"label_both\")\n\n\n\nFigure 2: Relationship between bill and tip size by paying method\n\n\n\nSimilarly, you can compute grouped covariances via:\n\nlibrary(kableExtra)\ntips %&gt;%\n    group_by(Credit) %&gt;%\n    summarise(Cov = cov(Bill, Tip, use = \"pairwise.complete.obs\")) %&gt;% \n    kable(digits = 2, booktabs = TRUE,\n        caption = \"Relationship between bill and tip size by credit card usage\")\n\n\n\n\n\nTable 1: Relationship between bill and tip size by credit card usage\n\nCredit\nCov\n\n\n\nNo\n15.38\n\n\nYes\n37.68\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample write-up\n\n\n\nFigure 1 highlights a strong positive relationship between bill and tip size (in US dollars). The covariance between the two variables is 25.96 squared dollars. The relationship between bill and tip size is stronger for those who paid by credit card than those who did not, as highlighted by Figure 2 and Table 1, where the covariance between the two variables is 37.68 for those that used a credit card and 15.38 for those that did not."
  },
  {
    "objectID": "1_04_relationships.html#helpful-references-on-relationships-between-variables",
    "href": "1_04_relationships.html#helpful-references-on-relationships-between-variables",
    "title": "Relationships",
    "section": "\n3 Helpful references on relationships between variables",
    "text": "3 Helpful references on relationships between variables\nIn the following, Cat = Categorical and Num = Numerical.\n\n\n\n\n\n\nRelationships between two variables\n\n\n\n\n\n\n\nCat-Cat\nNum-Cat\nNum-Num\n\n\n\nVisualise with a mosaic plot:\n\nlibrary(ggmosaic)\nggplot(tips)+\n    geom_mosaic(aes(x = product(Credit, Server), fill=Credit))\n\n\n\n\nSummarise with a contingency table:\n\ntips %&gt;%\n    select(Credit, Server) %&gt;%\n    table() %&gt;%\n    kable(booktabs = TRUE)\n\n\n\n\n\n\nA\nB\nC\n\n\n\nNo\n39\n50\n17\n\n\nYes\n21\n15\n15\n\n\n\n\n\n\n\nBoxplot\n\nggplot(tips, aes(x = Credit, y = Tip)) +\n    geom_boxplot()\n\n\n\n\nor grouped histogram\n\nggplot(tips, aes(x = Tip)) +\n    geom_histogram(color='white') +\n    facet_wrap(~Credit)\n\n\n\n\nor coloured density plot\n\nggplot(tips, aes(x = Tip, colour = Credit)) +\n    geom_density()\n\n\n\n\nSummarise via a grouped table of descriptive statistics:\n\ntips %&gt;%\n    group_by(Credit) %&gt;%\n    summarise(N = n(),\n              M = mean(Tip),\n              SD = sd(Tip),\n              Med = median(Tip),\n              IQR = IQR(Tip)) %&gt;%\n    kable(digits = 2, booktabs = TRUE)\n\n\n\n\n\nCredit\nN\nM\nSD\nMed\nIQR\n\n\n\nNo\n106\n3.25\n1.93\n3.0\n2.00\n\n\nYes\n50\n4.99\n2.77\n4.1\n3.04\n\n\n\n\n\n\n\nVisualise with a scatterplot:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point()\n\n\n\n\nSummarise with the covariance:\n\ncov(tips$Bill, tips$Tip, use = \"pairwise.complete.obs\") %&gt;%\n    round(digits = 2)\n\n[1] 25.96\n\n\nThere’s no need to put just one number into a table, write it up in a sentence.\n\n\n\n\n\n\n\n\n\n\n\n\nRelationships between three variables\n\n\n\n\n\n\n\nCat-Cat-Cat\nCat-Num-Cat\nNum-Num-Cat\n\n\n\nVisalise with a faceted mosaic plot:\n\nggplot(tips)+\n    geom_mosaic(aes(x = product(Credit, Server), fill=Credit)) +\n    facet_wrap(~Day, scales = \"free\") # scales = \"free\" shows the x-axis on each plot\n\n\n\n\nSummarise with grouped frequency tables:\n\nmon &lt;- tips %&gt;%\n    filter(Day == \"Monday\") %&gt;%\n    select(Credit, Server) %&gt;%\n    table()\n\ntue &lt;- tips %&gt;%\n    filter(Day == \"Tuesday\") %&gt;%\n    select(Credit, Server) %&gt;%\n    table()\n\nwed &lt;- tips %&gt;%\n    filter(Day == \"Wednesday\") %&gt;%\n    select(Credit, Server) %&gt;%\n    table()\n\n\nthu &lt;- tips %&gt;%\n    filter(Day == \"Thursday\") %&gt;%\n    select(Credit, Server) %&gt;%\n    table()\n\nfri &lt;- tips %&gt;%\n    filter(Day == \"Friday\") %&gt;%\n    select(Credit, Server) %&gt;%\n    table()\n\n\nkable(list(mon, tue, wed, thu, fri), booktabs = TRUE,\n      caption = \"Frequency tables of Credit by Server for each day of the week (Mon to Fri)\")\n\n\n\n\nFrequency tables of Credit by Server for each day of the week (Mon to Fri)\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\nNo\n0\n14\n0\n\n\nYes\n0\n6\n0\n\n\n\n\n\n\nA\nB\nC\n\n\n\nNo\n3\n0\n2\n\n\nYes\n6\n0\n2\n\n\n\n\n\n\nA\nB\nC\n\n\n\nNo\n18\n16\n7\n\n\nYes\n10\n5\n6\n\n\n\n\n\n\nA\nB\nC\n\n\n\nNo\n10\n11\n3\n\n\nYes\n3\n3\n6\n\n\n\n\n\n\nA\nB\nC\n\n\n\nNo\n8\n9\n5\n\n\nYes\n2\n1\n1\n\n\n\n\n\n\n\n\n\nVisualise using a faceted boxplot:\n\nggplot(tips)+\n    geom_boxplot(aes(x = Credit, y = Tip)) + \n    facet_wrap(~Day)\n\n\n\n\nor faceted and coloured density plots:\n\nggplot(tips)+\n    geom_density(aes(x = Tip, color = Credit)) + \n    facet_wrap(~Day)\n\n\n\n\nSummarise with a table of descriptive statistics grouped by the categorical variables:\n\ntips %&gt;%\n    group_by(Credit, Day) %&gt;%\n    summarise(M = mean(Tip, na.rm=TRUE),\n              SD = sd(Tip, na.rm=TRUE)) %&gt;%\n    kable(digits = 2, booktabs = TRUE)\n\n\n\n\n\nCredit\nDay\nM\nSD\n\n\n\nNo\nMonday\n2.97\n1.36\n\n\nNo\nTuesday\n4.97\n3.86\n\n\nNo\nWednesday\n2.92\n1.64\n\n\nNo\nThursday\n3.75\n1.92\n\n\nNo\nFriday\n3.10\n2.07\n\n\nYes\nMonday\n5.02\n2.95\n\n\nYes\nTuesday\n3.90\n2.23\n\n\nYes\nWednesday\n5.43\n3.51\n\n\nYes\nThursday\n4.99\n1.76\n\n\nYes\nFriday\n4.84\n1.37\n\n\n\n\n\n\n\nScatterplot\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    facet_wrap(~Day)\n\n\n\n\nSummarise via a grouped table of descriptive statistics:\n\ntips %&gt;%\n    group_by(Day) %&gt;%\n    summarise(\n        Cov = cov(Bill, Tip, use = \"pairwise.complete.obs\")\n    ) %&gt;%\n    kable(digits = 2, booktabs = TRUE)\n\n\n\n\n\nDay\nCov\n\n\n\nMonday\n23.46\n\n\nTuesday\n37.05\n\n\nWednesday\n34.76\n\n\nThursday\n18.92\n\n\nFriday\n12.26"
  },
  {
    "objectID": "1_04_relationships.html#student-glossary",
    "href": "1_04_relationships.html#student-glossary",
    "title": "Relationships",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse and package\n\n\n\ngeom_histogram\n?\n\n\ngeom_density\n?\n\n\ngeom_boxplot\n?\n\n\ngeom_point\n?\n\n\ngeom_mosaic\n?\n\n\nfacet_wrap\n?\n\n\nfacet_grid\n?\n\n\ngroup_by\n?\n\n\nsummarise\n?\n\n\ncor\n?\n\n\nround\n?"
  },
  {
    "objectID": "1_04_relationships.html#footnotes",
    "href": "1_04_relationships.html#footnotes",
    "title": "Relationships",
    "section": "Footnotes",
    "text": "Footnotes\n\n Hint: ask last week’s driver for the Rmd file, they should share it with the group via email or Teams.\nTo download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nHint: Some possibilities are, among many others:\n\nCategorical-Categorical: geom_mosaic() from ggmosaic package, or geom_bar()\n\nCategorical-Numeric: geom_histogram(), geom_density(), or geom_boxplot()\n\nNumeric-Numeric: geom_point()\n\n\n↩︎\n\n\nHint:\nDepending on the type of the variables some of these functions may be useful: count(), table(), n(), mean(), sd(), cov()\nStop and think. If the result is NA, what could have caused that?\n\nFor some functions, this is solved by adding the argument na.rm = TRUE.\nFor cov() you need the argument use = \"pairwise.complete.obs\". This is because the covariance between a pair of variables \\(X, Y\\) cannot be computed if one or both the values in a product is NA. The argument above tells R to only use pairs with complete observations, i.e. no missing values.\n\n↩︎\n\nHint: the function facet_wrap() may be useful with a categorical variable.↩︎\n Hint: you may want to use functions such as count, table, summarise, n, mean, sd, cov, group_by↩︎"
  },
  {
    "objectID": "1_02_categorical_data.html",
    "href": "1_02_categorical_data.html",
    "title": "Categorical data",
    "section": "",
    "text": "Instructions and data were released in week 1.\n\n\n\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structureA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\nThis week you will only focus on task A2. Below there are some guided sub-steps you may want to consider to complete task A2.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\n\n\n\n\n\nSelecting a subset of columns\n\n\n\n\n\nConsider a table of toy data comprising a participant identifier (id: 1 to 5), the participant age, the course (A or B) they are enrolled into, and their height:\n\ntoy_data &lt;- tibble(\n    id = 1:5,\n    age = c(18, 20, 25, 22, 19),\n    course = c(\"A\", \"B\", \"A\", \"B\", \"A\"),\n    height = c(171, 180, 168, 193, 174)\n)\ntoy_data\n\n# A tibble: 5 × 4\n     id   age course height\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    18 A         171\n2     2    20 B         180\n3     3    25 A         168\n4     4    22 B         193\n5     5    19 A         174\n\n\nTo select columns to keep you can either (1) specify the range from:to, if the columns are sequential, or (2) list the columns one by one.\n\n\nRange from:to\nListing all columns\n\n\n\nIf the columns you want to keep are sequential, you can just specify the first and last by using numbers:\n\ntoy_data %&gt;%\n    select(1:3)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nor using their names:\n\ntoy_data %&gt;%\n    select(id:course)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nEither option keeps columns id up to course.\n\n\nIf the columns you want to keep are not in sequential order, you have to list all of the columns you want to keep. This can be tedious if you have many.\nYou can do so using numbers:\n\ntoy_data %&gt;%\n    select(1, 2, 3)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nOr column names:\n\ntoy_data %&gt;%\n    select(id, age, course)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\n\n\n\nHowever, if you check the data in toy_data, those didn’t change. The result of the above computation was only printed to the screen but not stored.\n\ntoy_data\n\n# A tibble: 5 × 4\n     id   age course height\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    18 A         171\n2     2    20 B         180\n3     3    25 A         168\n4     4    22 B         193\n5     5    19 A         174\n\n\nTo store it, we need to assign the result:\n\ntoy_data &lt;- toy_data %&gt;%\n    select(id:course)\n\n\ntoy_data\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nBy doing the above, we have overwritten the data stored in toy_data with the selected columns.\n\n\n\n\nOverwrite the data to only include the first 15 variables (i.e. columns).2\nCreate a plot displaying the frequency distribution of movie genres.3\n\n\nCreate a plot displaying the frequency distribution of the lead studios.4\nWould it make sense to create a plot of the frequency distribution of movie names?5\n\n\n\n\n\n\n\nTip\n\n\n\nBefore applying a function to your data, you should always ask yourself if what you are about to do is going to convey any insight about the data, compared to just looking at the data itself.\nThe goal of data analysis is to to go from a multitude of values to insights that provide actionable information from a quick glance.\n\n\n\nDescribe the distribution of movie genres. You may want to include both the frequency and the percentage frequency.6\n\n\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nConsider this code:\n\ntoy_data %&gt;%\n    count(course) %&gt;%\n    mutate(\n        perc = round(n / sum(n) * 100, 2)\n    )\n\n# A tibble: 2 × 3\n  course     n  perc\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 A          3    60\n2 B          2    40\n\n\nYou can change the names of the frequency from n to Freq and perc to Perc using:\n\ntoy_data %&gt;%\n    count(course, name = \"Freq\") %&gt;%\n    mutate(\n        Perc = round(Freq / sum(Freq) * 100, 2)\n    )\n\n# A tibble: 2 × 3\n  course  Freq  Perc\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 A          3    60\n2 B          2    40\n\n\nAn alternative to the above involves using the group_by(), summarise(), and n() functions from tidyverse. The following code creates a table of absolute frequencies (or counts):\n\ntoy_data %&gt;%\n    group_by(course) %&gt;%\n    summarise(\n        Freq = n()\n    )\n\n# A tibble: 2 × 2\n  course  Freq\n  &lt;chr&gt;  &lt;int&gt;\n1 A          3\n2 B          2\n\n\nIn the code above, we take toy_data and perform a grouped computation for each separate course (because of group_by). The computation says to summarise the data by creating a new column named Freq which stores the counts of the values in each group (n()).\nThe following code mutates the frequency table and adds a new column storing the percentages, which is given the name Perc:\n\ntoy_data %&gt;%\n    group_by(course) %&gt;%\n    summarise(\n        Freq = n()\n    ) %&gt;%\n    mutate(\n        Perc = round(Freq / sum(Freq) * 100, 2)\n    )\n\n# A tibble: 2 × 3\n  course  Freq  Perc\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 A          3    60\n2 B          2    40\n\n\nThe function round(&lt;values&gt;, 2) tells R to round the percentages to 2 decimal places.\n\n\n\n\nDescribe the distribution of lead studios. You may want to include both the frequency and the percentage frequency.7\nWhat is the most common genre and the most common lead studio?8\nFormat your frequency tables properly using the kbl() function from the kableExtra package.9"
  },
  {
    "objectID": "1_02_categorical_data.html#formative-report-a",
    "href": "1_02_categorical_data.html#formative-report-a",
    "title": "Categorical data",
    "section": "",
    "text": "Instructions and data were released in week 1.\n\n\n\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structureA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\nThis week you will only focus on task A2. Below there are some guided sub-steps you may want to consider to complete task A2.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\n\n\n\n\n\nSelecting a subset of columns\n\n\n\n\n\nConsider a table of toy data comprising a participant identifier (id: 1 to 5), the participant age, the course (A or B) they are enrolled into, and their height:\n\ntoy_data &lt;- tibble(\n    id = 1:5,\n    age = c(18, 20, 25, 22, 19),\n    course = c(\"A\", \"B\", \"A\", \"B\", \"A\"),\n    height = c(171, 180, 168, 193, 174)\n)\ntoy_data\n\n# A tibble: 5 × 4\n     id   age course height\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    18 A         171\n2     2    20 B         180\n3     3    25 A         168\n4     4    22 B         193\n5     5    19 A         174\n\n\nTo select columns to keep you can either (1) specify the range from:to, if the columns are sequential, or (2) list the columns one by one.\n\n\nRange from:to\nListing all columns\n\n\n\nIf the columns you want to keep are sequential, you can just specify the first and last by using numbers:\n\ntoy_data %&gt;%\n    select(1:3)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nor using their names:\n\ntoy_data %&gt;%\n    select(id:course)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nEither option keeps columns id up to course.\n\n\nIf the columns you want to keep are not in sequential order, you have to list all of the columns you want to keep. This can be tedious if you have many.\nYou can do so using numbers:\n\ntoy_data %&gt;%\n    select(1, 2, 3)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nOr column names:\n\ntoy_data %&gt;%\n    select(id, age, course)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\n\n\n\nHowever, if you check the data in toy_data, those didn’t change. The result of the above computation was only printed to the screen but not stored.\n\ntoy_data\n\n# A tibble: 5 × 4\n     id   age course height\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    18 A         171\n2     2    20 B         180\n3     3    25 A         168\n4     4    22 B         193\n5     5    19 A         174\n\n\nTo store it, we need to assign the result:\n\ntoy_data &lt;- toy_data %&gt;%\n    select(id:course)\n\n\ntoy_data\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nBy doing the above, we have overwritten the data stored in toy_data with the selected columns.\n\n\n\n\nOverwrite the data to only include the first 15 variables (i.e. columns).2\nCreate a plot displaying the frequency distribution of movie genres.3\n\n\nCreate a plot displaying the frequency distribution of the lead studios.4\nWould it make sense to create a plot of the frequency distribution of movie names?5\n\n\n\n\n\n\n\nTip\n\n\n\nBefore applying a function to your data, you should always ask yourself if what you are about to do is going to convey any insight about the data, compared to just looking at the data itself.\nThe goal of data analysis is to to go from a multitude of values to insights that provide actionable information from a quick glance.\n\n\n\nDescribe the distribution of movie genres. You may want to include both the frequency and the percentage frequency.6\n\n\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nConsider this code:\n\ntoy_data %&gt;%\n    count(course) %&gt;%\n    mutate(\n        perc = round(n / sum(n) * 100, 2)\n    )\n\n# A tibble: 2 × 3\n  course     n  perc\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 A          3    60\n2 B          2    40\n\n\nYou can change the names of the frequency from n to Freq and perc to Perc using:\n\ntoy_data %&gt;%\n    count(course, name = \"Freq\") %&gt;%\n    mutate(\n        Perc = round(Freq / sum(Freq) * 100, 2)\n    )\n\n# A tibble: 2 × 3\n  course  Freq  Perc\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 A          3    60\n2 B          2    40\n\n\nAn alternative to the above involves using the group_by(), summarise(), and n() functions from tidyverse. The following code creates a table of absolute frequencies (or counts):\n\ntoy_data %&gt;%\n    group_by(course) %&gt;%\n    summarise(\n        Freq = n()\n    )\n\n# A tibble: 2 × 2\n  course  Freq\n  &lt;chr&gt;  &lt;int&gt;\n1 A          3\n2 B          2\n\n\nIn the code above, we take toy_data and perform a grouped computation for each separate course (because of group_by). The computation says to summarise the data by creating a new column named Freq which stores the counts of the values in each group (n()).\nThe following code mutates the frequency table and adds a new column storing the percentages, which is given the name Perc:\n\ntoy_data %&gt;%\n    group_by(course) %&gt;%\n    summarise(\n        Freq = n()\n    ) %&gt;%\n    mutate(\n        Perc = round(Freq / sum(Freq) * 100, 2)\n    )\n\n# A tibble: 2 × 3\n  course  Freq  Perc\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 A          3    60\n2 B          2    40\n\n\nThe function round(&lt;values&gt;, 2) tells R to round the percentages to 2 decimal places.\n\n\n\n\nDescribe the distribution of lead studios. You may want to include both the frequency and the percentage frequency.7\nWhat is the most common genre and the most common lead studio?8\nFormat your frequency tables properly using the kbl() function from the kableExtra package.9"
  },
  {
    "objectID": "1_02_categorical_data.html#worked-example",
    "href": "1_02_categorical_data.html#worked-example",
    "title": "Categorical data",
    "section": "\n2 Worked example",
    "text": "2 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)\n\n\n\nWe load the tidyverse package as we will use the functions read_csv and glimpse from this package.\n\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\n\n\n\nread_csv is the function to read CSV (comma separated values) files. Once we have read the file, it is stored into an object called tips using the arrow (&lt;-).\n\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\n\n\nhead() shows the top 6 rows of data. Use the n = ... option to change the default behaviour, e.g. head(&lt;data&gt;, n = 10).\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;chr&gt; \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server &lt;chr&gt; \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\n\nglimpse is part of the tidyverse package and is used to check the type of each variable.\nWe can use better and more descriptive labels for the categorical variables:\n\ntips$Day &lt;- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\n\n\ntips$Day, i.e. the column Day within the data tips, is converted to a factor in R (the appropriate storage mode for categorical variables). Furthermore, it replaces the level “m” with the new label “Monday”, “t” with the new label “Tuesday”, and so on.\n\ntips$Credit &lt;- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\nWe don’t have better labels for Server (current values A , B, or C), so we will just convert it to a factor by keeping the current levels:\n\ntips$Server &lt;- factor(tips$Server)\n\nCheck the relabelled columns:\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;fct&gt; No, No, Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;fct&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Friday,…\n$ Server &lt;fct&gt; A, B, A, B, B, A, B, A, A, B, B, A, B, B, B, B, C, C, C, C, C, …\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\n\n\n\n\n\n\n\n\n\nLast week, we also saw that if someone tipped more than 100% of the bill size, it was likely a data input error and we decided to replace that value with NA (not available):\n\n\nThe mutate function takes as arguments:\n\ncolumn name\n\n=\n\nhow to compute that column\n\n\nThe syntax for ifelse is:\nifelse(test_condition, \n       value_if_true, \n       avalue_if_false)\n\n\ntips &lt;- tips %&gt;%\n    mutate(\n        PctTip = ifelse(PctTip &gt; 100, NA, PctTip)\n    )\n\nThis displays the frequency distribution of credit card payers:\n\nplt_credit &lt;- ggplot(tips, aes(x = Credit)) +\n    geom_bar() +\n    labs(x = \"Paid by credit card?\", y = \"Count\")\nplt_credit\n\n\n\n\nYou can even flip the coordinates, if you wish to, using the coord_flip() function:\n\nggplot(tips, aes(x = Credit)) +\n    geom_bar() +\n    labs(x = \"Paid by credit card?\") +\n    coord_flip()\n\n\n\n\nYou can use the patchwork package to place graphs side by side. Simply create an object for each graph, and concatenate the objects with | for horizontal concatenation and / for vertical concatenation of graphs. You can even combine this by using parentheses, e.g. (plot1 | plot2) / (plot3 | plot4) for 2 rows and 2 columns.\n\n\nRun install.packages(\"patchwork\") first in your R console\nWe can display the frequency distribution of all the categorical variables: Credit, Day, and Server:\n\n\n\n\n\n\nRotate x-axis labels\n\n\n\n\n\nTo rotate x-axis labels by 90 degrees, you can use this code:theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\nTo rotate the labels by 45 degrees, you can use: theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\nDon’t worry, no one remembers it. People always google “rotate x-axis labels ggplot” to find it.\n\n\n\n\nlibrary(patchwork)\n\nplt1 &lt;- ggplot(tips, aes(x = Credit)) +\n    geom_bar() +\n    labs(x = \"Paird by credit card?\", y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt2 &lt;- ggplot(tips, aes(x = Day)) +\n    geom_bar() +\n    labs(x = \"Day of week\", y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt3 &lt;- ggplot(tips, aes(x = Server)) +\n    geom_bar() +\n    labs(y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt1 | plt2 | plt3\n\n\n\n\n\n\n\n\n\n\nSorting the barplot\n\n\n\n\n\nIf wanted, you can sort the bars in order of frequency by using the fct_infreq() function.\nIn the last plot, plt3, this involves changing the first row from ggplot(tips, aes(x = Server)) to ggplot(tips, aes(x = fct_infreq(Server))).\nIn these plot I have preferred not to do so, as changing the order of levels may confuse the reader when the factors have easily understood ordering: credit (No/Yes), day (Mon,Tue,Wed,Thu,Fri), server (A,B,C)\n\nlibrary(patchwork)\n\nplt1 &lt;- ggplot(tips, aes(x = fct_infreq(Credit))) +\n    geom_bar() +\n    labs(x = \"Paird by credit card?\", y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt2 &lt;- ggplot(tips, aes(x = fct_infreq(Day))) +\n    geom_bar() +\n    labs(x = \"Day of week\", y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt3 &lt;- ggplot(tips, aes(x = fct_infreq(Server))) +\n    geom_bar() +\n    labs(x = \"Server\", y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt1 | plt2 | plt3\n\n\n\n\n\n\n\nA frequency table can be obtained using:\n\ntbl_credit &lt;- tips %&gt;%\n    count(Credit) %&gt;%\n    mutate(\n        perc = round((n / sum(n)) * 100, 2)\n    )\ntbl_credit\n\n# A tibble: 2 × 3\n  Credit     n  perc\n  &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;\n1 No       106  67.5\n2 Yes       51  32.5\n\n\n\ntbl_day &lt;- tips %&gt;%\n    count(Day) %&gt;%\n    mutate(\n        perc = round((n / sum(n)) * 100, 2)\n    )\ntbl_day\n\n# A tibble: 5 × 3\n  Day           n  perc\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Monday       20 12.7 \n2 Tuesday      13  8.28\n3 Wednesday    62 39.5 \n4 Thursday     36 22.9 \n5 Friday       26 16.6 \n\n\n\ntbl_server &lt;- tips %&gt;%\n    count(Server) %&gt;%\n    mutate(\n        perc = round((n / sum(n)) * 100, 2)\n    )\ntbl_server\n\n# A tibble: 3 × 3\n  Server     n  perc\n  &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;\n1 A         60  38.2\n2 B         65  41.4\n3 C         32  20.4\n\n\nYou can create nice tables with the kbl command from the kableExtra package.\n\n\nRun install.packages(\"kableExtra\") first in your R console\n\nlibrary(kableExtra)\n\nkbl(list(tbl_credit, tbl_day, tbl_server), booktabs = TRUE)\n\n\n\n\nFrequency tables of categorical variables\n\n\n\n\nPaid with a credit card\n\nCredit\nn\nperc\n\n\n\nNo\n106\n67.52\n\n\nYes\n51\n32.48\n\n\n\n\n\n\n\n\n\n\nDay of the week\n\nDay\nn\nperc\n\n\n\nMonday\n20\n12.74\n\n\nTuesday\n13\n8.28\n\n\nWednesday\n62\n39.49\n\n\nThursday\n36\n22.93\n\n\nFriday\n26\n16.56\n\n\n\n\n\n\n\n\n\n\n\n\nServer\n\nServer\nn\nperc\n\n\n\nA\n60\n38.22\n\n\nB\n65\n41.40\n\n\nC\n32\n20.38\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow do I arrange by descending frequency order?\n\n\n\n\n\nAdd arrange(desc(&lt;column_of_freq&gt;)). For example:\n\ntbl_day &lt;- tips %&gt;%\n    count(Day) %&gt;%\n    mutate(\n        perc = round((n / sum(n)) * 100, 2)\n    ) %&gt;%\n    arrange(desc(n))\ntbl_day\n\n# A tibble: 5 × 3\n  Day           n  perc\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Wednesday    62 39.5 \n2 Thursday     36 22.9 \n3 Friday       26 16.6 \n4 Monday       20 12.7 \n5 Tuesday      13  8.28\n\n\nIf you just did arrange(n), it would be in ascending order.\n\n\n\n\n\n\n\n\n\nHow do I rename the frequency and percent columns?\n\n\n\n\n\nYou can specify a different name for the column of counts by using name = \"new name\". If you don’t specify it, the default is n.\nYou can specify any valid name for the percentages inside of mutate.\nFor example:\n\ntbl_day &lt;- tips %&gt;%\n    count(Day, name = \"Freq\") %&gt;%\n    mutate(\n        Perc = round((Freq / sum(Freq)) * 100, 2)\n    ) %&gt;%\n    arrange(desc(Freq))\ntbl_day\n\n# A tibble: 5 × 3\n  Day        Freq  Perc\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Wednesday    62 39.5 \n2 Thursday     36 22.9 \n3 Friday       26 16.6 \n4 Monday       20 12.7 \n5 Tuesday      13  8.28\n\n\n\n\n\nFrom the univariate distribution (or marginal distribution) of each categorical variable we see that the most common payment method was not a credit card, and the most common day of the week to dine at that restaurant was Wednesday, followed by Thursday and Friday. Finally, most parties were waited on by server B.\n\n\nThe mode of a variable is the value that appears most often.\nThe term comes from the French expression “à la mode”, i.e. in fashion. If you think about it, something is considered to be in fashion if it’s worn very often.\n\n\n\n\n\n\nReferencing tables\n\n\n\n\n\nTo reference a table in text you first give the code chunk a unique label, e.g. tableLabel, and a caption to the table, e.g. “My table caption is this”\n```{r tableLabel}\ntbl_credit %&gt;%\n    kbl(digits = 2, booktabs = TRUE, caption = \"My table caption is this\")\n```\nThis creates\n\n\n\n\nTable 1: My table caption is this\n\nCredit\nn\nperc\n\n\n\nNo\n106\n67.52\n\n\nYes\n51\n32.48\n\n\n\n\n\n\n\n\nThen you reference it in text using \\@ref(tab:tableLabel). For example:\n\nTable \\@ref(tab:tableLabel) displays etc.\n\nWhich renders as:\nTable 1 displays etc."
  },
  {
    "objectID": "1_02_categorical_data.html#student-glossary",
    "href": "1_02_categorical_data.html#student-glossary",
    "title": "Categorical data",
    "section": "\n3 Student Glossary",
    "text": "3 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse and package\n\n\n\nfactor\n?\n\n\n%&gt;%\n?\n\n\ngeom_bar\n?\n\n\nlabs\n?\n\n\ncount\n?\n\n\nmutate\n?\n\n\nsum\n?\n\n\nround\n?\n\n\ncoord_flip\n?\n\n\nkbl\n?\n\n\narrange\n?\n\n\ndesc\n?"
  },
  {
    "objectID": "1_02_categorical_data.html#footnotes",
    "href": "1_02_categorical_data.html#footnotes",
    "title": "Categorical data",
    "section": "Footnotes",
    "text": "Footnotes\n\n Hint: ask last week’s driver for the Rmd file, they should share it with the group via email or Teams.↩︎\nHint: the select() function from tidyverse↩︎\n\nHint: we display categorical variables with barplots. Consider the geom_bar() function.\nExample: For the toy_data from above, the frequency distribution of course enrollment is:\n\nggplot(toy_data, aes(x = course)) +\n    geom_bar() +\n    labs(x = \"Enrollment per course\", y = \"Frequency\")\n\n\n\n\n↩︎\n\n Hint: similar to above, change the column to LeadStudio.↩︎\n Hint: what would be the height of each bar? Would adding such a plot to a report bring any insights and be useful to a decision maker? In the data, Movie, which stores the movie title, is what is known is statistics as the “identifier” or “ID”. This uniquely identifies each unit in the study. If your study involved several participants, your ID would be the unique participant identifier. It doesn’t make sense to plot the frequency distribution of an identifier variable as it will have vertical bars all of height equal 1.↩︎\n\nHint: We describe categorical variables with frequency distributions.\nConsider using the count function from tidyverse and mutate for adding percentages.\nExample:\n\ntoy_data %&gt;%\n    count(course) %&gt;%\n    mutate(\n        perc = round(n / sum(n) * 100, 2)\n    )\n\n# A tibble: 2 × 3\n  course     n  perc\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 A          3    60\n2 B          2    40\n\n\nAdvanced: count(course) is equivalent to group_by(course) %&gt;% summarise(n = n()). See the box below for more details.↩︎\n\n Hint: similar to above, but replacing Genre with LeadStudio↩︎\n Hint: What is the mode of Genre and LeadStudio? In other words, which category in each of those frequency distributions has the highest frequency? Tip: You may want to order the barplots and/or frequency tables in descending order. For barplots, the function fct_infreq() may help. For tables, the function arrange(desc(&lt;column_of_freq&gt;)) may help.↩︎\nHint: See the worked example below.↩︎"
  },
  {
    "objectID": "1_01_design_and_data.html",
    "href": "1_01_design_and_data.html",
    "title": "Research design & data",
    "section": "",
    "text": "At the start of each teaching block, you will be given a dataset that you will use throughout the labs of that block’s five weeks. By the end of each block your group should have produced a report that analyses the given dataset.\nThe reports are due:\n\nFormative Report A (Block 1): 12 noon, Friday 20th October 2023.\nFormative Report B (Block 2): 12 noon, Friday 1st December 2023.\nFormative Report C (Block 3): 12 noon, Friday 16th February 2024.\nAssessed Report (Block 4): 12 noon, Friday 29th March 2024.\n\n\nYou will be required to submit a PDF file, not the Rmd file used to create the PDF.\n\nNo extensions. As these are group-based submissions, no extensions will be given.\nYou will receive formative feedback on each of the formative reports the week after the report due date. This will be signposted via announcements.\n\n\nWork through the lab tasks in groups of up to 5 students.\nIn each group, each week one person is the driver and the rest are the navigators.\n\nThe driver is responsible for typing on the PC keyboard for that week.\nThe navigators are responsible for commenting on the strategy, code, and spotting typos or fixing errors.\n\nEach week the driver will rotate so that everyone experiences being a driver at least once.\n\n\n\nDriver: download the template Rmd file, upload it to RStudio server online, and start writing your work there. Don’t forget to save your file regularly via File -&gt; Save.\nNavigators: be alert and start providing suggestions and comments on the strategy and code.\n\n\n\n\n\n\n\nTemplate Rmd file\nClick here to download the template Rmd file\nComplete it in the following weeks, and follow instructions in week 5 on how to “knit” and submit the PDF file.\n\n\n\n\n\nEach submitted report must be a PDF file of max 6 sides of A4 paper.\nKeep the default settings in terms of Rmd knitting font and page margins.\nAt the end of the file, you will place the appendices and these will not count towards the page limit.\n\nYou can include an optional appendix for additional tables and figures which you can’t fit in the main part of the report;\nYou must include a compulsory appendix listing all of the R code used in the report. This is done automatically if you end your file with the following section, which is already included in the template Rmd file:\n\n\n\n# Appendix: R code\n\n```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}\n\n```\n\nThe lab is structured to provide various levels of support. When attending a lab, you should prioritise completing that week’s tasks. However, if you are unsure or stuck at any point, you should make use of all the available help:\n\nRaise your hand to get help from a tutor;\nHover your mouse on the superscript number to get a quick hint. The hints may sometimes show multiple equivalent ways of getting an answer - you just need one way;\nScroll down to the Worked Example section, where you can read through a worked example.\nEven if you don’t use the Worked Example to complete the tasks, ensure you review and study its content during your independent study time.\n\n\n\n\nA. Yes\nB. No\nIf B didn’t work\n\n\n\n\nLogin to EASE using your university UUN and password.\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and RStudio password.\n\n\n\nTry these steps first to register for RStudio server online:\n\nLog in to EASE using your university UUN and password.\nSet your RStudio password here, the username will be the same as your UUN (make sure you type your UUN correctly).\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and the password you set above in (2).\n\n\n\n\nPlease complete this form and wait for an email. Please note that this can take up to four working days.\n\nOnce you receive an email from us, please follow the following instructions:\n\nSet your here, the username will be the same as your UUN (make sure you type your UUN correctly).\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and the password you just set above.\n\n\n\n\n\n\n\nEvery single student, when logging into their personal RStudio Server account, must do the following at least once. In other words, everyone in each group has to do it at some point in their own RStudio when they are the driver.\nIn order to generate a PDF file from RStudio, you must have a package called tinytex installed. This allows you to “Knit” your Rmd document (i.e. combine together text, code, and output) to produce a PDF file. Copy and paste the following code in your console, and press Enter.\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()"
  },
  {
    "objectID": "1_01_design_and_data.html#instructions",
    "href": "1_01_design_and_data.html#instructions",
    "title": "Research design & data",
    "section": "",
    "text": "At the start of each teaching block, you will be given a dataset that you will use throughout the labs of that block’s five weeks. By the end of each block your group should have produced a report that analyses the given dataset.\nThe reports are due:\n\nFormative Report A (Block 1): 12 noon, Friday 20th October 2023.\nFormative Report B (Block 2): 12 noon, Friday 1st December 2023.\nFormative Report C (Block 3): 12 noon, Friday 16th February 2024.\nAssessed Report (Block 4): 12 noon, Friday 29th March 2024.\n\n\nYou will be required to submit a PDF file, not the Rmd file used to create the PDF.\n\nNo extensions. As these are group-based submissions, no extensions will be given.\nYou will receive formative feedback on each of the formative reports the week after the report due date. This will be signposted via announcements.\n\n\nWork through the lab tasks in groups of up to 5 students.\nIn each group, each week one person is the driver and the rest are the navigators.\n\nThe driver is responsible for typing on the PC keyboard for that week.\nThe navigators are responsible for commenting on the strategy, code, and spotting typos or fixing errors.\n\nEach week the driver will rotate so that everyone experiences being a driver at least once.\n\n\n\nDriver: download the template Rmd file, upload it to RStudio server online, and start writing your work there. Don’t forget to save your file regularly via File -&gt; Save.\nNavigators: be alert and start providing suggestions and comments on the strategy and code.\n\n\n\n\n\n\n\nTemplate Rmd file\nClick here to download the template Rmd file\nComplete it in the following weeks, and follow instructions in week 5 on how to “knit” and submit the PDF file.\n\n\n\n\n\nEach submitted report must be a PDF file of max 6 sides of A4 paper.\nKeep the default settings in terms of Rmd knitting font and page margins.\nAt the end of the file, you will place the appendices and these will not count towards the page limit.\n\nYou can include an optional appendix for additional tables and figures which you can’t fit in the main part of the report;\nYou must include a compulsory appendix listing all of the R code used in the report. This is done automatically if you end your file with the following section, which is already included in the template Rmd file:\n\n\n\n# Appendix: R code\n\n```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}\n\n```\n\nThe lab is structured to provide various levels of support. When attending a lab, you should prioritise completing that week’s tasks. However, if you are unsure or stuck at any point, you should make use of all the available help:\n\nRaise your hand to get help from a tutor;\nHover your mouse on the superscript number to get a quick hint. The hints may sometimes show multiple equivalent ways of getting an answer - you just need one way;\nScroll down to the Worked Example section, where you can read through a worked example.\nEven if you don’t use the Worked Example to complete the tasks, ensure you review and study its content during your independent study time.\n\n\n\n\nA. Yes\nB. No\nIf B didn’t work\n\n\n\n\nLogin to EASE using your university UUN and password.\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and RStudio password.\n\n\n\nTry these steps first to register for RStudio server online:\n\nLog in to EASE using your university UUN and password.\nSet your RStudio password here, the username will be the same as your UUN (make sure you type your UUN correctly).\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and the password you set above in (2).\n\n\n\n\nPlease complete this form and wait for an email. Please note that this can take up to four working days.\n\nOnce you receive an email from us, please follow the following instructions:\n\nSet your here, the username will be the same as your UUN (make sure you type your UUN correctly).\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and the password you just set above.\n\n\n\n\n\n\n\nEvery single student, when logging into their personal RStudio Server account, must do the following at least once. In other words, everyone in each group has to do it at some point in their own RStudio when they are the driver.\nIn order to generate a PDF file from RStudio, you must have a package called tinytex installed. This allows you to “Knit” your Rmd document (i.e. combine together text, code, and output) to produce a PDF file. Copy and paste the following code in your console, and press Enter.\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()"
  },
  {
    "objectID": "1_01_design_and_data.html#formatting-resources",
    "href": "1_01_design_and_data.html#formatting-resources",
    "title": "Research design & data",
    "section": "\n2 Formatting resources",
    "text": "2 Formatting resources\nThese will be useful in week 5 when finalising your report formatting prior to submission.\n\n2.1 APA style\nCheck the following guide for reporting numbers and statistics in APA style (7th edition).\n\nNumbers and statistics guide, APA style 7th edition\n\n2.2 Hiding code and/or output\n\n\nHide code but show output\nShow code but hide output\nHide both code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```\n\n\n\n\n2.3 Checklist for successful knitting\nIf you encounter errors when knitting the Rmd file, go through the following checklist to try finding the source of the errors.\n\nSuccessful knitting checklist"
  },
  {
    "objectID": "1_01_design_and_data.html#formative-report-a",
    "href": "1_01_design_and_data.html#formative-report-a",
    "title": "Research design & data",
    "section": "\n3 Formative Report A",
    "text": "3 Formative Report A\nIn the first five weeks of the course your group should produce a PDF report (using Rmarkdown) for which you will receive formative feedback in week 6.\nThe report should not include any reference to R code or functions, but be written or a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\n\n3.1 Data\nHollywood Movies. At the link https://uoepsy.github.io/data/hollywood_movies_subset.csv you will find data on Hollywood movies released between 2012 and 2018 from the top 5 lead studios and top 10 genres. The following variables were recorded:\n\n\n\n\nVariable\nDescription\n\n\n\nMovie\nTitle of the movie\n\n\nLeadStudio\nPrimary U.S. distributor of the movie\n\n\nRottenTomatoes\nRotten Tomatoes rating (critics)\n\n\nAudienceScore\nAudience rating (via Rotten Tomatoes)\n\n\nGenre\nOne of Action Adventure, Black Comedy, Comedy, Concert, Documentary, Drama, Horror, Musical, Romantic Comedy, Thriller, or Western\n\n\nTheatersOpenWeek\nNumber of screens for opening weekend\n\n\nOpeningWeekend\nOpening weekend gross (in millions)\n\n\nBOAvgOpenWeekend\nAverage box office income per theater, opening weekend\n\n\nBudget\nProduction budget (in millions)\n\n\nDomesticGross\nGross income for domestic (U.S.) viewers (in millions)\n\n\nWorldGross\nGross income for all viewers (in millions)\n\n\nForeignGross\nGross income for foreign viewers (in millions)\n\n\nProfitability\nWorldGross as a percentage of Budget\n\n\nOpenProfit\nPercentage of budget recovered on opening weekend\n\n\nYear\nYear the movie was released\n\n\nIQ1-IQ50 (ignore for Formative report A)\nIQ score of each of 50 audience raters\n\n\nSnacks (ignore for Formative report A)\nHow many of the 50 audience raters brought snacks\n\n\nPrivateTransport (ignore for Formative report A)\nHow many of the 50 audience raters reached the cinema via private transportation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor formative report A, please only focus on the variables Movie to Year, ignoring anything beyond that. In other words, do not analyse the variables IQ1 to PrivateTransport in the next five weeks of the course. We will use those later in the course.\n\n\n\n\n3.2 Tasks\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variables.\nA3) Display and describe six numerical variables of your choice.\nA4) Display and describe a relationship of interest between two or three variables of your choice.\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback.\n\n\n3.3 A1 sub-tasks\nThis week you will only focus on task A1. Below there are some guided sub-steps you may want to consider to complete task A1.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nRead the movie data into R, and give it a useful name. Inspect the data by looking at the data in RStudio. By viewing, we actually mean looking at the data either on the viewer or the console.1\nHow many observations are there?2\nHow many variables are there?3\n\n\n\n\n\n\n\n\n\nThink about it\n\n\n\n\n\n\nWhat does dim(DATA) return?\nWhat is the function of appending a [1] or [2]?\n\n\n\n\n\nWhat is the type of each variable?4\nWhat’s the minimum and maximum budget in the sample? What about the average Rotten Tomatoes rating?5\nDo you notice any issues when computing the minimum and maximum Budget and the average RottenTomatoes rating?6\nWhat is the range (i.e. minimum and maximum) of the variables in the data? What about the number of missing values for each variable?7\nWrite-up a description of the dataset for the reader. You don’t need to show the actual data in the report, but a description in words is sufficient for the reader."
  },
  {
    "objectID": "1_01_design_and_data.html#worked-example",
    "href": "1_01_design_and_data.html#worked-example",
    "title": "Research design & data",
    "section": "\n4 Worked example",
    "text": "4 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)\n\n\n\nWe load the tidyverse package as we will use the functions read_csv and glimpse from this package.\n\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\n\n\n\nread_csv is the function to read CSV (comma separated values) files. Once we have read the file, it is stored into an object called tips using the arrow (&lt;-).\n\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\n\n\nhead() shows by default the top 6 rows of the data. Use the n = ... option to change the default behaviour, e.g. head(&lt;data&gt;, n = 10).\n\ndim(tips)\n\n[1] 157   7\n\n\n\n\nThis returns the number of rows and columns in the data\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;chr&gt; \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server &lt;chr&gt; \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\n\nglimpse is part of the tidyverse package\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nAlternatives to glimpse are the data “structure” function:\n\nstr(tips)\n\nspc_tbl_ [157 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Bill  : num [1:157] 23.7 36.1 32 17.4 15.4 ...\n $ Tip   : num [1:157] 10 7 5.01 3.61 3 2.5 3.44 2.42 3 2 ...\n $ Credit: chr [1:157] \"n\" \"n\" \"y\" \"y\" ...\n $ Guests: num [1:157] 2 3 2 2 2 2 2 2 2 2 ...\n $ Day   : chr [1:157] \"f\" \"f\" \"f\" \"f\" ...\n $ Server: chr [1:157] \"A\" \"B\" \"A\" \"B\" ...\n $ PctTip: num [1:157] 42.2 19.4 15.7 20.8 19.5 13.4 16 12.4 12.7 10.7 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Bill = col_double(),\n  ..   Tip = col_double(),\n  ..   Credit = col_character(),\n  ..   Guests = col_double(),\n  ..   Day = col_character(),\n  ..   Server = col_character(),\n  ..   PctTip = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nor:\n\nsapply(tips, data.class)\n\n       Bill         Tip      Credit      Guests         Day      Server \n  \"numeric\"   \"numeric\" \"character\"   \"numeric\" \"character\" \"character\" \n     PctTip \n  \"numeric\" \n\n\n\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nA dataset containing records on 7 variables related to tipping was obtained from https://uoepsy.github.io/data/RestaurantTips.csv, and was provided by the owner of a bistro in the US interested in studying which factors affected the tipping behaviour of the bistro’s customers. The data contains measurements for a total of 157 parties on four numeric variables: size of the bill (in dollars), size of the tip, number of guests in the group, and tip as a percentage of the bill total. The data also includes three categorical variables indicating whether or not the party paid with a credit card, the day of the week, as well as a server-specific identifier.\n\n\n\nsummary(tips)\n\n      Bill            Tip            Credit              Guests     \n Min.   : 1.66   Min.   : 0.250   Length:157         Min.   :1.000  \n 1st Qu.:15.19   1st Qu.: 2.075   Class :character   1st Qu.:2.000  \n Median :20.22   Median : 3.340   Mode  :character   Median :2.000  \n Mean   :22.73   Mean   : 3.807                      Mean   :2.096  \n 3rd Qu.:28.84   3rd Qu.: 5.000                      3rd Qu.:2.000  \n Max.   :70.51   Max.   :15.000                      Max.   :7.000  \n                 NA's   :1                                          \n     Day               Server              PctTip      \n Length:157         Length:157         Min.   :  6.70  \n Class :character   Class :character   1st Qu.: 14.30  \n Mode  :character   Mode  :character   Median : 16.20  \n                                       Mean   : 17.89  \n                                       3rd Qu.: 18.20  \n                                       Max.   :221.00  \n                                                       \n\n\n\n\nsummary returns a quick summary of the data, i.e. a list of numerical summaries.\nYou probably won’t understand some parts of the output above, but we will learn more in the coming weeks, so don’t worry too much about it. For the moment, you should be able to understand the minimum, maximum, and the mean.\nCurrently, it is not showing very informative output for the categorical variables, also known as factors.\nWe can replace each factor level with a clearer label. The following code takes the column Day from the tips data and assigns a new label “Monday” to the level “m”, etc.\n\ntips$Day &lt;- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit &lt;- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server &lt;- factor(tips$Server)\n\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nUsing tidyverse, the function mutate is used to mutate a variable (column) in the data:\n\ntips &lt;- tips %&gt;%\n    mutate(\n        Day = factor(Day,\n                     levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                     labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")),\n        Credit = factor(Credit,\n                        levels = c(\"n\", \"y\"),\n                        labels = c(\"No\", \"Yes\")),\n        Server = factor(Server)\n    )\n\nThe functions %&gt;% and mutate are part of the tidyverse package. The former, %&gt;%, is called pipe.\nThe pipe works by taking what’s on the left and passing it to the operation on the right. For example, rounding to 2 decimal places the logarithm of the whole numbers from 1 to 10:\n\nround(log(1:10), digits = 2)\n\n [1] 0.00 0.69 1.10 1.39 1.61 1.79 1.95 2.08 2.20 2.30\n\n\nis equivalent to:\n\n1:10 %&gt;%\n    log() %&gt;%\n    round(digits = 2)\n\n [1] 0.00 0.69 1.10 1.39 1.61 1.79 1.95 2.08 2.20 2.30\n\n\n\n\n\nLet’s check the result of the changes to the variable types:\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;fct&gt; No, No, Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;fct&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Friday,…\n$ Server &lt;fct&gt; A, B, A, B, B, A, B, A, A, B, B, A, B, B, B, B, C, C, C, C, C, …\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\nsummary(tips)\n\n      Bill            Tip         Credit        Guests             Day    \n Min.   : 1.66   Min.   : 0.250   No :106   Min.   :1.000   Monday   :20  \n 1st Qu.:15.19   1st Qu.: 2.075   Yes: 51   1st Qu.:2.000   Tuesday  :13  \n Median :20.22   Median : 3.340             Median :2.000   Wednesday:62  \n Mean   :22.73   Mean   : 3.807             Mean   :2.096   Thursday :36  \n 3rd Qu.:28.84   3rd Qu.: 5.000             3rd Qu.:2.000   Friday   :26  \n Max.   :70.51   Max.   :15.000             Max.   :7.000                 \n                 NA's   :1                                                \n Server     PctTip      \n A:60   Min.   :  6.70  \n B:65   1st Qu.: 14.30  \n C:32   Median : 16.20  \n        Mean   : 17.89  \n        3rd Qu.: 18.20  \n        Max.   :221.00  \n                        \n\n\n\n\nAfter making categorical variables factors, summary shows the count of each category for the categorical variables.\nThe percentage of total bill has a maximum value of 221, which seems very strange. Someone is very unlikely to tip more than their bill total. In this case 221% of their bill value seems unlikely.\nLet’s inspect the row where PctTip is greater than 100:\n\ntips[tips$PctTip &gt; 100, ]\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;dbl&gt;\n1  49.6    NA Yes         4 Thursday C         221\n\n\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nAlternatively, using tidyverse, the function filter is used to only filter the rows that satisfy a condition:\n\ntips %&gt;% \n    filter(PctTip &gt; 100)\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;dbl&gt;\n1  49.6    NA Yes         4 Thursday C         221\n\n\n\n\n\nWith a bill of 49.6, the tip would be 109.62 dollars:\n\n49.6 * 221 / 100\n\n[1] 109.616\n\n\nFurthermore, we also notice that the tipping amount is not available (NA). The corresponding value in the percentage of total tip seems likely an inputting error, perhaps due to double typing the leading 2 when recording the data. We will set that value to not available (NA) with the following code:\n\ntips$PctTip[tips$PctTip &gt; 100] &lt;- NA\n\n\n\na &gt; b tests whether a is greater than b. a &lt; b tests whether a is smaller than b. a == b tests whether a is equal to b; notice the double equal sign! You can also use &gt;= or &lt;=\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nAlternatively you can use tidyverse:\n\ntips &lt;- tips %&gt;%\n    mutate(\n        PctTip = ifelse(PctTip &gt; 100, NA, PctTip)\n    )\n\nWhere the function ifelse selects a value depending on a condition to test: ifelse(test, value_if_true, value_if_false). In the case above, each value in the column PctTip is replaced by NA if Pct &gt; 100, and it is kept the same otherwise.\n\n\n\n\nsummary(tips)\n\n      Bill            Tip         Credit        Guests             Day    \n Min.   : 1.66   Min.   : 0.250   No :106   Min.   :1.000   Monday   :20  \n 1st Qu.:15.19   1st Qu.: 2.075   Yes: 51   1st Qu.:2.000   Tuesday  :13  \n Median :20.22   Median : 3.340             Median :2.000   Wednesday:62  \n Mean   :22.73   Mean   : 3.807             Mean   :2.096   Thursday :36  \n 3rd Qu.:28.84   3rd Qu.: 5.000             3rd Qu.:2.000   Friday   :26  \n Max.   :70.51   Max.   :15.000             Max.   :7.000                 \n                 NA's   :1                                                \n Server     PctTip     \n A:60   Min.   : 6.70  \n B:65   1st Qu.:14.30  \n C:32   Median :16.15  \n        Mean   :16.59  \n        3rd Qu.:18.05  \n        Max.   :42.20  \n        NA's   :1      \n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nThe average bill size was $22.73, and the average tip was $3.85, corresponding to roughly 17% of the total bill. Out of 157 parties, only 51 paid with a credit card. Most parties tended to be of around 2 people each, and people tended to go to that restaurant more often on Wednesday. Among the three servers, server C was the one that served the least number of parties. The data also included a missing tipping value, corresponding to a bill $49.59, and a data inputting error for the corresponding measure of the tip as a percentage of the total bill."
  },
  {
    "objectID": "1_01_design_and_data.html#student-glossary",
    "href": "1_01_design_and_data.html#student-glossary",
    "title": "Research design & data",
    "section": "\n5 Student Glossary",
    "text": "5 Student Glossary\nTo conclude the lab, create a glossary of R functions. You can do so by opening Microsoft Word, Excel, or OneNote and creating a table with two columns: one where you should write the name of an R function, and the other column where you should provide a brief description of what the function does.\nThis “do it yourself” glossary is an opportunity for you to revise what you have learned in today’s lab and write down a few take-home messages. You will find this glossary handy as a reference to keep next to you when you will be doing the assessed weekly quizzes.\nBelow you can find an example to get you started:\n\n\n\n\n\n\nFunction\nUse and package\n\n\n\nread_csv\nFor reading comma separated value files. Part of tidyverse package\n\n\nView\n?\n\n\nhead\n?\n\n\nnrow\n?\n\n\nncol\n?\n\n\ndim\n?\n\n\nglimpse\n?\n\n\nstr\n?\n\n\nsummary\n?\n\n\nfactor\n?"
  },
  {
    "objectID": "1_01_design_and_data.html#footnotes",
    "href": "1_01_design_and_data.html#footnotes",
    "title": "Research design & data",
    "section": "Footnotes",
    "text": "Footnotes\n\n Hint: To read the data use read_csv() from the tidyverse package.  To preview the data, use View(DATA) or head(DATA)↩︎\n Hint: nrow(DATA)  or dim(DATA)[1]↩︎\n Hint: ncol(DATA)  or dim(DATA)[2]↩︎\n Hint: glimpse(DATA) from tidyverse or str(DATA) or sapply(DATA, data.class)↩︎\n Hint: summary(DATA) or min(DATA$VARIABLE) and max(DATA$VARIABLE) Hint: mean(DATA$VARIABLE)↩︎\n For some movies, data on the budget or rotten tomatoes rating are not available (NA). These are also called missing values.\nIf you used the functions min(), max(), mean() you will get NA as a result. This is because if a value is missing, you cannot compute the mean of something you don’t know. For example, what is the mean of 5, 10, and NA? How would I compute (5 + 10 + NA) / 3? I don’t know, so it remains NA.\nYou can tell R to ignore the missing values by saying min(DATA$VARIABLE, na.rm = TRUE) and similarly for max and mean.\nInstead, summary() does this for you automatically and immediately tells you if a variable had any NAs and how many.↩︎\n Hint: summary(DATA) and nrow(DATA)↩︎"
  },
  {
    "objectID": "1_03_numeric_data.html",
    "href": "1_03_numeric_data.html",
    "title": "Numeric data",
    "section": "",
    "text": "Instructions and data were released in week 1.\n\n\n\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variablesA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\nThis week you will only focus on task A3. Below there are some guided sub-steps you may want to consider to complete task A3.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\n\n\n\n\n\nVisualising the distribution of numerical data\n\n\n\n\n\nWe display numeric variables with histograms, density plots, or boxplots. Respectively, these use the function geom_histogram(), geom_density(), or geom_boxplot() from ggplot2, which is a package automatically loaded when you load tidyverse via library(tidverse). For illustration purposes, we will use the starwars dataset from tidyverse, containing information on Starwars characters.\n\nlibrary(tidyverse)\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"The Empire Strikes Back\", \"Revenge of the Sith\", \"Return…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\n\nHistogram\nDensity plot\nBoxplot\n\n\n\nThe distribution of the character heights (cm) can be displayed with a histogram:\n\nggplot(starwars, aes(x = height)) +\n    geom_histogram(color = 'gray', fill = 'lightblue') +\n    labs(x = \"Character height (cm)\", y = \"Frequency\")\n\n\n\n\n\n\nThe distribution of the character heights (cm) can be displayed with a density plot:\n\nggplot(starwars, aes(x = height)) +\n    geom_density(color = 'dodgerblue') +\n    labs(x = \"Character height (cm)\")\n\n\n\n\n\n\nThe distribution of the character heights (cm) can be displayed with a box plot:\n\nggplot(starwars, aes(x = height)) +\n    geom_boxplot() +\n    labs(x = \"Character height (cm)\")\n\n\n\n\n\n\n\n\n\n\n\n\nCreate six plots, each displaying the distribution of:2\n\nProduction budgets\nAudience scores\nRotten Tomatoes ratings\nWorld gross income\nForeign gross income\nYear of movie release\n\n\n\n\nArrange the above plots as a single figure comprising 2 by 3 panels3\n\n\n\n\n\n\n\n\nCompute the mean and standard deviation of a variable\n\n\n\n\n\nConsider again the starwars dataset. The mean and SD of the height variable can be computed as:\n\nstarwars %&gt;%\n    summarise(M = mean(height, na.rm = TRUE),\n              SD = sd(height, na.rm = TRUE)) %&gt;%\n    round(digits = 2)\n\n# A tibble: 1 × 2\n      M    SD\n  &lt;dbl&gt; &lt;dbl&gt;\n1  174.  34.8\n\n\nTo make a nice table for the PDF document, you can use the kbl() function from the kableExtra package:\n\nlibrary(kableExtra)\n\nstarwars %&gt;%\n    summarise(M = mean(height, na.rm = TRUE),\n              SD = sd(height, na.rm = TRUE)) %&gt;%\n    kbl(booktabs = TRUE, digits = 2)\n\n\n\n\n\nM\nSD\n\n\n174.36\n34.77\n\n\n\n\n\n\n\n\nCreate a table of descriptive statistics that summarises the production budgets using the mean and standard deviation.\nIn the next step you will learn how to create a table for all variables at once, so this step can be excluded from your report, but it’s important to know how to manually compute it too.4\n\n\n\n\n\n\n\n\nTable of descriptive statistics\n\n\n\n\n\nUsing summarise for more than a couple of variables would make the job very tedious and long. There is a shortcut, which uses the describe function from the psych package.\nThe following code creates a table of descriptive statistics (via the describe function from the psych package) and ensures the table is in proper format by using the kbl function from the kableExtra package.\n\nlibrary(kableExtra) # for the kbl function\nlibrary(psych)      # for the describe function\n\nstarwars %&gt;%\n    select(height, mass) %&gt;%\n    describe() %&gt;%\n    kbl(digits = 2, booktabs = TRUE)   # kbl converts to a nice PDF table\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\nheight\n1\n81\n174.36\n34.77\n180\n178.17\n19.27\n66\n264\n198\n-1.03\n1.78\n3.86\n\n\nmass\n2\n59\n97.31\n169.46\n79\n75.44\n16.31\n15\n1358\n1343\n6.97\n48.93\n22.06\n\n\n\n\n\nTo only show the columns n, mean, sd, median you can use:\n\nlibrary(kableExtra) # for the kbl function\nlibrary(psych)      # for the describe function\n\nstarwars %&gt;%\n    select(height, mass) %&gt;%\n    describe() %&gt;%\n    select(n, mean, sd, median) %&gt;%\n    kbl(digits = 2, booktabs = TRUE)   # kbl converts to a nice PDF table\n\n\n\n\n\n\nn\nmean\nsd\nmedian\n\n\n\nheight\n81\n174.36\n34.77\n180\n\n\nmass\n59\n97.31\n169.46\n79\n\n\n\n\n\n\n\n\n\nCreate a table of descriptive statistics that summarises (using the mean and standard deviation) the six numeric variables which you plotted above. 5\n\n\n\nWrite up a summary of what you have reported in the plots and/or tables, using proper rounding to 2 decimal places and avoiding any reference to R code or functions.\n\n\n\n\n\n\n\nHiding R code or ouput\n\n\n\n\n\n\n\nHide code but show output\nShow code but hide output\nHide both code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```\n\n\n\n\n\n\n\nMake sure that all R code and output is not visible in the PDF report. The PDF report should only include text, tables, and plots."
  },
  {
    "objectID": "1_03_numeric_data.html#formative-report-a",
    "href": "1_03_numeric_data.html#formative-report-a",
    "title": "Numeric data",
    "section": "",
    "text": "Instructions and data were released in week 1.\n\n\n\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variablesA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\nThis week you will only focus on task A3. Below there are some guided sub-steps you may want to consider to complete task A3.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\n\n\n\n\n\nVisualising the distribution of numerical data\n\n\n\n\n\nWe display numeric variables with histograms, density plots, or boxplots. Respectively, these use the function geom_histogram(), geom_density(), or geom_boxplot() from ggplot2, which is a package automatically loaded when you load tidyverse via library(tidverse). For illustration purposes, we will use the starwars dataset from tidyverse, containing information on Starwars characters.\n\nlibrary(tidyverse)\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"The Empire Strikes Back\", \"Revenge of the Sith\", \"Return…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\n\nHistogram\nDensity plot\nBoxplot\n\n\n\nThe distribution of the character heights (cm) can be displayed with a histogram:\n\nggplot(starwars, aes(x = height)) +\n    geom_histogram(color = 'gray', fill = 'lightblue') +\n    labs(x = \"Character height (cm)\", y = \"Frequency\")\n\n\n\n\n\n\nThe distribution of the character heights (cm) can be displayed with a density plot:\n\nggplot(starwars, aes(x = height)) +\n    geom_density(color = 'dodgerblue') +\n    labs(x = \"Character height (cm)\")\n\n\n\n\n\n\nThe distribution of the character heights (cm) can be displayed with a box plot:\n\nggplot(starwars, aes(x = height)) +\n    geom_boxplot() +\n    labs(x = \"Character height (cm)\")\n\n\n\n\n\n\n\n\n\n\n\n\nCreate six plots, each displaying the distribution of:2\n\nProduction budgets\nAudience scores\nRotten Tomatoes ratings\nWorld gross income\nForeign gross income\nYear of movie release\n\n\n\n\nArrange the above plots as a single figure comprising 2 by 3 panels3\n\n\n\n\n\n\n\n\nCompute the mean and standard deviation of a variable\n\n\n\n\n\nConsider again the starwars dataset. The mean and SD of the height variable can be computed as:\n\nstarwars %&gt;%\n    summarise(M = mean(height, na.rm = TRUE),\n              SD = sd(height, na.rm = TRUE)) %&gt;%\n    round(digits = 2)\n\n# A tibble: 1 × 2\n      M    SD\n  &lt;dbl&gt; &lt;dbl&gt;\n1  174.  34.8\n\n\nTo make a nice table for the PDF document, you can use the kbl() function from the kableExtra package:\n\nlibrary(kableExtra)\n\nstarwars %&gt;%\n    summarise(M = mean(height, na.rm = TRUE),\n              SD = sd(height, na.rm = TRUE)) %&gt;%\n    kbl(booktabs = TRUE, digits = 2)\n\n\n\n\n\nM\nSD\n\n\n174.36\n34.77\n\n\n\n\n\n\n\n\nCreate a table of descriptive statistics that summarises the production budgets using the mean and standard deviation.\nIn the next step you will learn how to create a table for all variables at once, so this step can be excluded from your report, but it’s important to know how to manually compute it too.4\n\n\n\n\n\n\n\n\nTable of descriptive statistics\n\n\n\n\n\nUsing summarise for more than a couple of variables would make the job very tedious and long. There is a shortcut, which uses the describe function from the psych package.\nThe following code creates a table of descriptive statistics (via the describe function from the psych package) and ensures the table is in proper format by using the kbl function from the kableExtra package.\n\nlibrary(kableExtra) # for the kbl function\nlibrary(psych)      # for the describe function\n\nstarwars %&gt;%\n    select(height, mass) %&gt;%\n    describe() %&gt;%\n    kbl(digits = 2, booktabs = TRUE)   # kbl converts to a nice PDF table\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\nheight\n1\n81\n174.36\n34.77\n180\n178.17\n19.27\n66\n264\n198\n-1.03\n1.78\n3.86\n\n\nmass\n2\n59\n97.31\n169.46\n79\n75.44\n16.31\n15\n1358\n1343\n6.97\n48.93\n22.06\n\n\n\n\n\nTo only show the columns n, mean, sd, median you can use:\n\nlibrary(kableExtra) # for the kbl function\nlibrary(psych)      # for the describe function\n\nstarwars %&gt;%\n    select(height, mass) %&gt;%\n    describe() %&gt;%\n    select(n, mean, sd, median) %&gt;%\n    kbl(digits = 2, booktabs = TRUE)   # kbl converts to a nice PDF table\n\n\n\n\n\n\nn\nmean\nsd\nmedian\n\n\n\nheight\n81\n174.36\n34.77\n180\n\n\nmass\n59\n97.31\n169.46\n79\n\n\n\n\n\n\n\n\n\nCreate a table of descriptive statistics that summarises (using the mean and standard deviation) the six numeric variables which you plotted above. 5\n\n\n\nWrite up a summary of what you have reported in the plots and/or tables, using proper rounding to 2 decimal places and avoiding any reference to R code or functions.\n\n\n\n\n\n\n\nHiding R code or ouput\n\n\n\n\n\n\n\nHide code but show output\nShow code but hide output\nHide both code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```\n\n\n\n\n\n\n\nMake sure that all R code and output is not visible in the PDF report. The PDF report should only include text, tables, and plots."
  },
  {
    "objectID": "1_03_numeric_data.html#worked-example",
    "href": "1_03_numeric_data.html#worked-example",
    "title": "Numeric data",
    "section": "\n2 Worked example",
    "text": "2 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;chr&gt; \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server &lt;chr&gt; \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\ntips$Day &lt;- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit &lt;- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server &lt;- factor(tips$Server)\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;fct&gt; No, No, Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;fct&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Friday,…\n$ Server &lt;fct&gt; A, B, A, B, B, A, B, A, A, B, B, A, B, B, B, B, C, C, C, C, C, …\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\ntips &lt;- tips %&gt;%\n    mutate(\n        PctTip = ifelse(PctTip &gt; 100, NA, PctTip)\n    )\n\nWe can create a histogram of tips via:\n\nggplot(tips, aes(x = Tip)) + \n    geom_histogram(color = 'white') + \n    labs(x = \"Size of the tip (US dollars)\")\n\n\n\n\nWe can create a single figure with the distribution of all numeric variables by using the patchwork package:\n\n\nIf you save each plot into an object with a name, e.g. p1, p2, p3, p4, you can arrange the four plots into 2 by 2 panels as follows:\nlibrary(patchwork)\n(p1 | p2) / (p3 | p4)\nTo do 1 row of 4 plots:\np1 | p2 | p3 | p4\n\nlibrary(patchwork)\n\npltBill &lt;- ggplot(tips, aes(x = Bill)) +\n    geom_histogram(color = 'white', fill = 'lightblue') +\n    labs(x = \"Size of the bill (US dollars)\")\n\npltTip &lt;- ggplot(tips, aes(x = Tip)) +\n    geom_histogram(color = 'white', fill = 'lightblue') +\n    labs(x = \"Size of the tip (US dollars)\")\n\npltGuests &lt;- ggplot(tips, aes(x = Guests)) +\n    geom_bar(fill = 'lightblue') +\n    labs(x = \"Number of people in the group\")\n\npltPctTip &lt;- ggplot(tips, aes(x = PctTip)) +\n    geom_histogram(color = 'white', fill = 'lightblue') +\n    labs(x = \"Tip as a percentage of the bill\")\n\n(pltBill | pltTip) / (pltGuests | pltPctTip)\n\n\n\n\nTo summarise one numeric variable, you can use the summarise function from tidyverse, which takes the data and computes a numeric summary. The syntax is:\ndata %&gt;%\n    summarise(\n        write_the_column_name = computation\n    )\nThis computes the mean and SD of tip size (in US dollars), and calls the column storing the mean M, and the column storing the standard deviation SD:\n\ntips %&gt;%\n    summarise(\n        M = mean(Tip, na.rm = TRUE),\n        SD = sd(Tip, na.rm = TRUE)\n    ) %&gt;%\n    kbl(digits = 2, booktabs = TRUE)\n\n\n\n\n\nM\nSD\n\n\n3.81\n2.37\n\n\n\n\nTo summarise all of the numeric variables into a single table of descriptive statistics you can use the describe function from the psych package:\n\nlibrary(kableExtra)\nlibrary(psych)\n\ntips %&gt;%\n    select(Bill, Tip, Guests, PctTip) %&gt;%\n    describe() %&gt;%\n    kbl(booktabs = TRUE, digits = 2)\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\nBill\n1\n157\n22.73\n12.16\n20.22\n21.37\n11.03\n1.66\n70.51\n68.85\n1.22\n1.89\n0.97\n\n\nTip\n2\n156\n3.81\n2.37\n3.34\n3.50\n1.99\n0.25\n15.00\n14.75\n1.50\n3.13\n0.19\n\n\nGuests\n3\n157\n2.10\n0.93\n2.00\n1.98\n0.00\n1.00\n7.00\n6.00\n2.22\n7.81\n0.07\n\n\nPctTip\n4\n156\n16.59\n4.39\n16.15\n16.25\n2.74\n6.70\n42.20\n35.50\n2.50\n12.39\n0.35\n\n\n\n\n\nTo only keep the sample size, mean, SD, median, min, max, we use the select function from tidyverse:\n\nlibrary(kableExtra)\nlibrary(psych)\n\ntips %&gt;%\n    select(Bill, Tip, Guests, PctTip) %&gt;%\n    describe() %&gt;%\n    select(n, mean, sd, median, min, max) %&gt;%\n    kbl(booktabs = TRUE, digits = 2)\n\n\n\n\n\n\nn\nmean\nsd\nmedian\nmin\nmax\n\n\n\nBill\n157\n22.73\n12.16\n20.22\n1.66\n70.51\n\n\nTip\n156\n3.81\n2.37\n3.34\n0.25\n15.00\n\n\nGuests\n157\n2.10\n0.93\n2.00\n1.00\n7.00\n\n\nPctTip\n156\n16.59\n4.39\n16.15\n6.70\n42.20\n\n\n\n\n\n\n\n\n\n\n\nUse the appropriate summary for each variable type\n\n\n\nEnsure that you summarise variables correctly:\n\nFor categorical variables use frequency tables\nFor continuous variables use a table of descriptive statistics (mean, SD, Median, etc.)\n\nYou should not summarise categorical variables with the mean, SD, and this is why it’s important to use select() before describe() to only keep the variables that are continuous.\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nThe distributions of bill size, tip size, and group size are skewed to the right. The distribution of tips, as a percentage of the total bill, appears to be approximately bell shaped, with three outliers in the right tail of the distribution.\nThe average bill was about $22.73, with a SD of $12.16. The average tip was $3.81, with a SD of $2.37, corresponding to an average tip as a percentage of the total bill of $16.59, with a SD of $4.39. The average party size comprised 2 guests, with a SD of roughly 1 person."
  },
  {
    "objectID": "1_03_numeric_data.html#student-glossary",
    "href": "1_03_numeric_data.html#student-glossary",
    "title": "Numeric data",
    "section": "\n3 Student Glossary",
    "text": "3 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse and package\n\n\n\ngeom_histogram\n?\n\n\ngeom_density\n?\n\n\ngeom_boxplot\n?\n\n\npatchwork: | and /\n\n?\n\n\nsummarise\n?\n\n\nselect\n?\n\n\nkbl\n?\n\n\ndescribe\n?"
  },
  {
    "objectID": "1_03_numeric_data.html#footnotes",
    "href": "1_03_numeric_data.html#footnotes",
    "title": "Numeric data",
    "section": "Footnotes",
    "text": "Footnotes\n\n Hint: ask last week’s driver for the Rmd file, they should share it with the group via email or Teams.\nTo download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nHint: geom_histogram(), geom_density(), or geom_boxplot().\nStop and think. Do you notice anything strange in the distribution of one of the variables? Among the six variables, one is different from the other 5. Can you think of which one, and why?\nAnswer. The variable that is slightly different from the other numeric variables is Year. While Year is stored as a numeric variable, is it perhaps better visualised by geom_bar(). You could think of it as an ordinal variable rather than a continuous one.↩︎\n\n\nHint: the | and / functions from the patchwork package.\nFor example, if plt1, plt2, plt3, and plt4 are four plots, this creates a single figure of 2 by 2 panels:\n(plt1 | plt2) / (plt3 | plt4)↩︎\n\n Hint: the summarise() function from tidyverse, mean() and sd().  Hint: if you remember, there are NAs in the variable. What do you need to add to the mean() and sd() function to ensure they do not affect the computation?↩︎\n\nHint: the describe function from the psych package.\nStop and think. Think about the variable Year again. Does it make sense to compute the average year? If not, you may wish to exclude it from your table of descriptive statistics.↩︎"
  },
  {
    "objectID": "1_05_formative_report_a.html",
    "href": "1_05_formative_report_a.html",
    "title": "Formative report A",
    "section": "",
    "text": "Instructions and data were released in week 1."
  },
  {
    "objectID": "1_05_formative_report_a.html#tasks",
    "href": "1_05_formative_report_a.html#tasks",
    "title": "Formative report A",
    "section": "\n1 Tasks",
    "text": "1 Tasks\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choiceA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback"
  },
  {
    "objectID": "1_05_formative_report_a.html#a5-sub-tasks",
    "href": "1_05_formative_report_a.html#a5-sub-tasks",
    "title": "Formative report A",
    "section": "\n2 A5 sub-tasks",
    "text": "2 A5 sub-tasks\nThis week you will only focus on task A5. Below there are some guided sub-steps you may want to consider to complete task A5.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nEnsure all group members have joined the group on LEARN. If you have not done so yet, go to the course LEARN page, click “Groups” from the top menu bar, find the group having the same name as your table label, and click Join.\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\nOrganise the Rmd file to have the following structure:\n\n---\ntitle: \"Formative Report A (Group &lt;NUMBER&gt;.&lt;LETTER&gt;)\"\nauthor: \"&lt;insert exam numbers here, e.g. B001, B002, B003, B004, B005&gt;\"\ndate: \"&lt;insert date here&gt;\"\noutput: bookdown::pdf_document2\ntoc: false\n---\n\n\nThis is the metadata block. It includes the:\n\ndocument title\nauthor name\ndate (to leave empty, use an empty string \"\")\nthe output type\nwhether or not to display the Table of Contents (TOC)\n\nThe output type could be html_document, pdf_document, etc. We use bookdown::pdf_document2 so that we can reference figures, which pdf_document doesn’t let you do. The code bookdown::pdf_document2 simply means to use the pdf_document2 type from the bookdown package.\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)\n```\n\n\nThis is the setup chunk and should always be included in your Rmd document. It sets the global options for all code chunks that will follow.\nIn your knitted file:\n\nIf echo=FALSE, the R code in chunks is not displayed. If TRUE, it is.\nIf message=FALSE, information messages are not displayed. If TRUE, they are.\nIf warning=FALSE, warning messages are not printed. If TRUE, they are.\n\nIf you want to change the setting in a specific code chunk, you can do so via:\n```{r, echo=TRUE}\n# A code chunk\n```\n\n```{r, include=FALSE}\n# Week 1 code below\nlibrary(tidyverse)\n\n# Week 2 code below\npltEye &lt;- ggplot(starwars, aes(x = eye_color)) + \n    geom_bar()\n\n# Week 3 code below\n\n# week 4 code below\n```\n\n\nThis code chunks contains your rough work from each week. Give names to plots and tables, so that you can reference those later on. The option include=FALSE hides both code and output.\nTo run each line of code while you are working, put your cursor on the line and press Control + Enter on Windows or Command + Enter on a macOS.\n# Introduction\n\nWrite here a concise introduction to the data, the variables, and anything worth \nof notice in the dataset.\n\n\n# Analysis\n\nPresent here your tables, plots, and results. In the code chunk below, you do \nnot need to put the chunk option `echo=FALSE` as you set this option globally \nin the setup chunk. \n\n```{r}\npltEye\n```\n\nIf you didn't set it globally, you would need to put it in the chunk options:\n\n```{r, echo=FALSE}\npltEye\n```\n\nMore text...\n\n\n# Discussion\n\nWrite up your take home messages here...\n\n\nThis contains your actual textual reporting, as well as tables and figures. To show in place a plot previously created, just include the plot name in a code chunk with the option echo=FALSE to hide the code but display the output.\n# Appendix A: Additional figures and tables\n\nInclude here additional tables and figures, with captions, and properly \nreferenced. These should be used somewhere in the text, do not include tables or \nfigures which are not referenced anywhere in your writing.\n\n\nIf you don’t need Appendix A, because all your figures and tables fit in the page limit, you can delete it.\n# Appendix B: R code\n\nDo not edit the code chunk below, but remove this paragraph of text before \nsubmitting.\n\n```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}\n\n```\n\n\nThis special code chunk (do not edit it) takes all code above and places it here into Appendix B for you. As such, it allows the marker to see the code you used to obtain your results. Please note that only the code should be visible in the appendix, no output. Hence why the options are echo=TRUE (show the code), but eval=FALSE (do not evaluate/run the code).\nThe appendices do not count towards the 6-page limit.\n\nKnit the document to PDF\n\nSubmit the PDF file on Learn:\n\nGo to the Learn page of the course\nClick Assessments\nClick Submit Formative Report A (PDF file only)\nFollow the instructions\n\n\n\n\n\n\n\n\n\nReferencing figures\n\n\n\n\n\nFirst, you need to pick a unique label for the code chunk that displays the figure, in this case shortLabel but you should use a more descriptive name.\n```{r shortLabel, fig.cap = \"Figure caption\"}\npltEye &lt;- ggplot(starwars, aes(x = eye_color)) + \n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\npltEye\n```\n\n\n\n\nFigure caption\n\n\n\nTo reference a figure, for example the one above, you would \nwrite see Figure \\@ref(fig:shortLabel).\nwhich, when you Knit to PDF, becomes:\n\nTo reference a figure, for example the one above, you would write see Figure 1.\n\n\n\n\n\n\n\n\n\n\nReferencing tables\n\n\n\n\n\nFirst, you need to pick a unique label for the code chunk that displays the table, in this case anotherLabel but you should use a more descriptive name.\n```{r anotherLabel, echo=FALSE}\nlibrary(kableExtra)\ntblEye &lt;- starwars %&gt;%\n    count(eye_color) %&gt;%\n    kable(booktabs = TRUE, caption = \"Short table caption\")\ntblEye\n```\n\n\n\nShort table caption\n\neye_color\nn\n\n\n\nblack\n10\n\n\nblue\n19\n\n\nblue-gray\n1\n\n\nbrown\n21\n\n\ndark\n1\n\n\ngold\n1\n\n\ngreen, yellow\n1\n\n\nhazel\n3\n\n\norange\n8\n\n\npink\n1\n\n\nred\n5\n\n\nred, blue\n1\n\n\nunknown\n3\n\n\nwhite\n1\n\n\nyellow\n11\n\n\n\n\n\nThe table is referenced as, see Table \\@ref(tab:anotherLabel).\nWhich, when you knit to PDF, is displayed as:\n\nThe table is referenced as, see Table 1.\n\nFor details on styling PDF tables, see this link.\n\n\n\n\n\n\n\n\n\nReducing figure size\n\n\n\n\n\nYou could place multiple panels into a single figure using the functions | and / from the patchwork package.\nYou could adjust the figure height and width by playing with a few options for the numbers fig.height = ? and fig.width = ?, for example 5 and 4, or 12 and 8, and so on. Please note this is typically found by trial and error. Keep in mind, however, that the figure labels should still be legible in the plot you show.\n```{r, fig.height = 5, fig.width = 4}\n# your code to display the figure here\n```\n\n\n\n\n\n\n\n\n\nHiding R code or ouput\n\n\n\n\n\n\n\nHiding R code\nHiding R output\nHiding R code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\nTo hide both text output and figures, use:\n```{r, results='hide', fig.show='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```"
  },
  {
    "objectID": "1_05_formative_report_a.html#worked-example",
    "href": "1_05_formative_report_a.html#worked-example",
    "title": "Formative report A",
    "section": "\n3 Worked example",
    "text": "3 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\n\n\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\nWe can replace each factor level with a clearer label:\n\ntips$Day &lt;- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit &lt;- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server &lt;- factor(tips$Server)\n\nThe percentage of total bill has a maximum value of 221, which seems very strange. Someone is very unlikely to tip more than their bill total. In this case 221% of their bill value seems unlikely.\nLet’s inspect the row where PctTip is greater than 100:\n\ntips %&gt;% \n    filter(PctTip &gt; 100)\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;dbl&gt;\n1  49.6    NA Yes         4 Thursday C         221\n\n\n\n\nThis code means: take the data tips and then filter it to only keep the rows where PctTip is larger than 100. You can also provide a condition using other comparison operators such as ==, &gt;=, &lt;=, &gt;, &lt;, …\nWith a bill of 49.59, the tip would be 109.59 dollars:\n\n49.59 * 221 / 100\n\n[1] 109.5939\n\n\nFurthermore, we also notice that the tipping amount is not available (NA). The corresponding value in the percentage of total tip seems likely an inputting error, perhaps due to double typing the leading 2 when recording the data. We will set that value to not available (NA) with the following code:\n\ntips$PctTip[tips$PctTip &gt; 100] &lt;- NA\n\nConsider, for example, the relationship between bill and tip size. As these are two numerical variables, we visualise the relationship with a scatterplot:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\")\n\n\n\n\nWe can numerically summarise this relationship with the covariance between the two variables:\n\nround(cov(tips$Bill, tips$Tip, use = \"pairwise.complete.obs\"), digits = 2)\n\n[1] 25.96\n\n\nThe relationship looks roughly like a line. You can superimpose a “best-fit” line with the function geom_smooth(method = lm, se = FALSE). The argument method = lm tells to fit a line (in R this is called a linar model, lm), and se = FALSE tells R to not plot the uncertainty bands.\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    geom_smooth(method = lm, se = FALSE)\n\n\n\n\nYou will only learn how to find the functional relationship between two variables in the second-year course DAPR2, so for now I will give it to you:\n\\[\ny = -0.26 + 0.18 * x \\qquad \\text{where} \\qquad \\begin{cases}\nx = \\text{Bill} \\\\\ny = \\text{Tip}\n\\end{cases}\n\\]\nWhat is the predicted tip for a bill of 50 US dollars? Let’s do the computation:\n\n-0.26 + 0.18 * 50\n\n[1] 8.74\n\n\nFrom the plot above, a tip of 8.74 US dollars seems roughly right!\nLet’s find the tips for bills of size 20, 40, 60.\n\ntips_line_func &lt;- tibble(\n    bills = c(20, 40, 60),\n    tips  = -0.26 + 0.18 * bills\n)\ntips_line_func\n\n# A tibble: 3 × 2\n  bills  tips\n  &lt;dbl&gt; &lt;dbl&gt;\n1    20  3.34\n2    40  6.94\n3    60 10.5 \n\n\nTo investigate the relationship between bill and tip size for those who paid by credit card and those who didn’t we can create faceted scatterplots:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit, labeller = \"label_both\")\n\n\n\n\nYou can also fit a best-fit line by payment method:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit, labeller = \"label_both\") +\n    geom_smooth(method = lm, se = FALSE)\n\n\n\n\nTo extend the lins for the full range of the x-axis, you can use the option fullrange = TRUE:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit, labeller = \"label_both\") +\n    geom_smooth(method = lm, se = FALSE, fullrange = TRUE)\n\n\n\n\nAgain, you will not know how to find out the functional relationship between the variables within each group until the course DAPR2 in 2nd year, so I will give it to you.\nFor those who did not pay by credit card:\n\\[\ny = -0.17 + 0.18 * x \\qquad \\text{where} \\qquad \\begin{cases}\nx = \\text{Bill} \\\\\ny = \\text{Tip}\n\\end{cases}\n\\]\nFor those who paid by credit card:\n\\[\ny = -0.34 + 0.18 * x \\qquad \\text{where} \\qquad \\begin{cases}\nx = \\text{Bill} \\\\\ny = \\text{Tip}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "1_05_formative_report_a.html#student-glossary",
    "href": "1_05_formative_report_a.html#student-glossary",
    "title": "Formative report A",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse and package\n\n\n\ngeom_smooth\n?\n\n\ntibble\n?\n\n\nknitr::opts_chunk$set()\n?"
  },
  {
    "objectID": "1_05_formative_report_a.html#footnotes",
    "href": "1_05_formative_report_a.html#footnotes",
    "title": "Formative report A",
    "section": "Footnotes",
    "text": "Footnotes\n\n Hint: ask last week’s driver for the Rmd file, they should share it with the group via email or Teams.\nTo download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎"
  },
  {
    "objectID": "1_08_prob_rules.html",
    "href": "1_08_prob_rules.html",
    "title": "Probability Rules",
    "section": "",
    "text": "Instructions Recap - Formative Report B\n\n\n\n\n\n\nIn this block of the course (weeks 7-11), you should produce a PDF report using Rmarkdown for which you will receive formative feedback in week 12.\nThe report should not include any reference to R code or functions, but be written or a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\nYou will be required to submit a PDF file by 12 noon on Friday the 2nd of December 2022 via Learn. One person needs to submit on behalf of your group.\n\nThe report should be at most 6 pages long. At the end of the report, you are allowed two appendices which both don’t count towards the page limit.\n\nAppendix A will contain any tables or figures which you cannot fit in the page limit (no text allowed)\nAppendix B will contain the code to reproduce the report results (just like Formative Report A).\n\n\nNo extensions allowed. As this is group-based work, no extensions are possible."
  },
  {
    "objectID": "1_08_prob_rules.html#tasks",
    "href": "1_08_prob_rules.html#tasks",
    "title": "Probability Rules",
    "section": "\n1 Tasks",
    "text": "1 Tasks\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task B2.\nB1) Create and summarise categorical variables, before calculating probabilities.\n\n\n\n\n\n\nThis week’s task\n\n\n\nB2) Investigate if events are independent, and compute probabilities.\n\n\nB3) Computing and plotting probabilities with a binomial distribution.\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Plot standard error of the mean, and finish the report write-up (i.e., knit to PDF, and submit the PDF for formative feedback)."
  },
  {
    "objectID": "1_08_prob_rules.html#b2-sub-tasks",
    "href": "1_08_prob_rules.html#b2-sub-tasks",
    "title": "Probability Rules",
    "section": "\n2 B2 sub-tasks",
    "text": "2 B2 sub-tasks\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\nIn this section you will find some guided sub-steps you may want to consider to complete task B2.\n\nReopen last week’s Rmd file, as you will continue last week’s work (i.e., the top 3 most frequent movie genres that were rated as ‘good’ and ‘bad’), and build on it.1\n\n\n\nWhat’s the probability of a movie being rated as Good?2\n\n\n\nGiven that a viewer watched a Drama movie, what’s the probability of them giving a good rating?3\n\n\n\nGiven that a viewer watched an Action movie, what’s the probability of them giving a good rating?\nGiven that a viewer watched a Comedy movie, what’s the probability of them giving a good rating?\nGiven that a viewer gave a bad rating, what’s the probability of them having watched a non-drama movie? 4\n\n\n\n\n\n\n\nAdvanced material\n\n\n\n\n\nConsider three mutually exclusive events \\(A_1, A_2, A_3\\) and another event \\(B\\).\nWe have that:\n\\[\n\\begin{aligned}\nP\\big((\\sim A_3) | B\\big)\n&= P\\big( (A_1 \\cup A_2) | B \\big) \\\\\n&= \\frac{P\\big((A_1 \\cup A_2) \\cap B\\big)}{P(B)} \\\\\n&= \\frac{P\\big((A_1 \\cap B) \\cup (A_2 \\cap B)\\big)}{P(B)} \\\\\n&= \\frac{P(A_1 \\cap B) + P(A_2 \\cap B)}{P(B)}\n\\end{aligned}\n\\]\nSuppose \\(A_1 = Action\\), \\(A_2 = Comedy\\), \\(A_3 = Drama\\), and \\(B = Bad\\).\n\\[\n\\begin{aligned}\nP\\big((\\sim Drama) \\mid Bad\\big)\n&= P\\big( (Action \\cup Comedy) \\mid Bad \\big) \\\\\n&= \\frac{P\\big((Action \\cup Comedy) \\cap Bad\\big)}{P(Bad)} \\\\\n&= \\frac{P\\big((Action \\cap Bad) \\cup (Comedy \\cap Bad)\\big)}{P(Bad)} \\\\\n&= \\frac{P(Action \\cap Bad) + P(Comedy \\cap Bad)}{P(Bad)}\n\\end{aligned}\n\\]\n\n\n\n\nWhat’s the probability of a rater watching a Drama movie or rating a movie as Bad?5\n\n\n\nDo you think that a movie receiving a Good rating is independent of Genre?6\n\n\n\nBased on your analysis above, which movie Genre do you think lead studios should invest in for their next movie?7\n\n\n\nUsing a conditional mosaic plot, display the conditional distribution of movie genres being rated as either good or bad, making sure to add a main title and clear axis titles.8\n\n\n\nIn the analysis section of your report, write up a summary of what you have reported above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions. In particular, focus on whether events were independent, and which genre of movie lead studios should consider investing in based on audience ratings.9"
  },
  {
    "objectID": "1_08_prob_rules.html#worked-example",
    "href": "1_08_prob_rules.html#worked-example",
    "title": "Probability Rules",
    "section": "\n3 Worked Example",
    "text": "3 Worked Example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\nWorking with the “Tip_Avg” variable created last week, we can see our relative frequency table for all of our servers (A, B, and C) who were tipped either Above or Below the standard tipping rate in the US (i.e., 15%).\n\ntips2 &lt;- tips %&gt;%\n    mutate(Tip_Avg = ifelse(PctTip &lt;= 15, 'Below', 'Above'))\n\nrel_freq_tbl &lt;- table(tips2$Server, tips2$Tip_Avg) %&gt;%\n    prop.table() %&gt;%\n    addmargins()\nrel_freq_tbl\n\n     \n           Above      Below        Sum\n  A   0.25477707 0.12738854 0.38216561\n  B   0.25477707 0.15923567 0.41401274\n  C   0.12738854 0.07643312 0.20382166\n  Sum 0.63694268 0.36305732 1.00000000\n\n\n\nWhat’s the probability of a customer tipping above average?\n\n\n# P(above) = 0.25477707 + 0.25477707 + 0.12738854\n# P(above) = 0.63694268\n\n\n# indexing: table[row numbers, col numbers]\nrel_freq_tbl[4, 1]\n\n[1] 0.6369427\n\n# or indexing: table[row names, col names]\nrel_freq_tbl['Sum', 'Above']\n\n[1] 0.6369427\n\n\n\\(P(Above) ≈ 0.64\\)\n\nGiven that the server is A, what’s the probability of receiving an above average tip?\n\n\n#P(above | server A) = 0.25477707 / 0.38216561\n#P(above | server A) = 0.6666667\n\n\\(P(Above | Server A) ≈ 0.67\\)\n\nGiven that the server is B, what’s the probability of receiving an above average tip?\n\n\n#P(above | server B) = 0.25477707 / 0.41401274\n#P(above | server B) = 0.6153846\n\n\\(P(Above | Server B) ≈ 0.62\\)\n\nGiven that the server is C, what’s the probability of receiving an above average tip?\n\n\n#P(above | server C) = 0.12738854 / 0.20382166\n#P(above | server C) = 0.625\n\n\\(P(Above | Server C) ≈ 0.63\\)\n\nGiven that a server received a tip below average, what’s the probability of them being Server A or B?\n\n\n# P( (server A ∪ server B) | below ) = \n# (P( server A ∩ below ) + P( server B ∩ below )) / P(below) = \n# (0.12738854  + 0.15923567) / 0.36305732 =\n# 0.7894737 \n\n\\(P((Server A \\cup Server B) \\mid Below) ≈ 0.79\\)\n\nWhat’s the probability of the customer being served by server A or tipping below 15%?\n\n\n# P(server A) = 0.38216561\n# P(below) = 0.36305732\n# P(server A ∩ below) = 0.12738854\n\n# P(server A) + P(below) - P(server A ∩ below) = \n# (0.38216561 + 0.36305732) - 0.12738854 =\n# 0.6178344 ≈ 0.62\n\n\\(P(Server A \\cup Below) ≈ 0.62\\)\n\nIs tipping above average independent of the server?\n\nNo, the events seem to be dependent, but very weakly. The conditional probabilities of tipping above average for each server are different from P(above), even though to a small degree. In particular, the probability of tipping above average after service from Server A is higher.\n\nBased on your analysis above, which server do you think offers the best customer service based on their tips?\n\nServer A appears to offer the best service to their customers, based solely on their personal tips - they had a much higher probability of receiving an above average tip (67%) than a below average tip (33%).\n\nTo visualise our findings, we could use a conditional mosaic plot:\n\n\nlibrary(ggmosaic)\nmos_cond_plot &lt;- ggplot(tips2) +\n    geom_mosaic(aes(x = product(Tip_Avg), fill = Tip_Avg, conds = product(Server))) +  \n    labs(title = \"Conditional Association between Servers and Tips\", \n         x = \"Server\", \n         y = \"Tip Average\", \n         fill = \"Tip Average\")\nmos_cond_plot\n\n\n\nFigure 2: Conditional Association between Servers and Tips\n\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nIt was more likely for customers to tip above (64%) than below (36%) average. Though it was likely that all servers would receive an above average tip, tipping did not appear to be independent of server, based on conditional probabilities. Based on their personal tips, Server A appeared to offer the best service, where they were more likely to receive an above average tip (67%). Servers B and C were almost equally likely to receive above average tips (62% and 63% respectively). These associations are visually represented in Figure 2.\n\n\n\n\n\n\n\n\nAdvanced material: Three definitions of independence\n\n\n\n\n\nRecall the frequency table\n\nrel_freq_tbl\n\n     \n           Above      Below        Sum\n  A   0.25477707 0.12738854 0.38216561\n  B   0.25477707 0.15923567 0.41401274\n  C   0.12738854 0.07643312 0.20382166\n  Sum 0.63694268 0.36305732 1.00000000\n\n\nThe following three definitions of independence are equivalent. Two events \\(A\\) and \\(B\\) are independent if one of these holds:\n\n\\(P(A | B) = P(A)\\)\nor \\(P(B | A) = P(B)\\)\n\nor \\(P(A \\cap B) = P(A) P(B)\\)\n\n\nFor now, let’s focus on the third definition. To see if tipping above average is independent of the specific server, we can checks that condition separately for each server:\nA. Is \\(P(Server A \\cap Above)\\) equal to \\(P(Server A) P(Above)\\)?\n\n# 0.25477707 is P(Server A ∩ Above)\n0.38216561 * 0.63694268 # P(Server A) * P(Above)\n\n[1] 0.2434176\n\n\nB. Is \\(P(Server B \\cap Above)\\) equal to \\(P(Server B) P(Above)\\)?\n\n# 0.25477707 is P(Server B ∩ Above)\n0.41401274 * 0.63694268 # P(Server B) * P(Above)\n\n[1] 0.2637024\n\n\nC. Is \\(P(Server C \\cap Above)\\) equal to \\(P(Server C) P(Above)\\)?\n\n# 0.12738854 is P(Server C ∩ Above)\n0.20382166 * 0.63694268 # P(Server C) * P(Above)\n\n[1] 0.1298227\n\n\nFor server A and B, the values are close enough but not exactly equal. However, for server C, the values are identical up to the 2nd decimal place. This suggests the events are dependent, but to a small extent."
  },
  {
    "objectID": "1_08_prob_rules.html#student-glossary",
    "href": "1_08_prob_rules.html#student-glossary",
    "title": "Probability Rules",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\n|\n?\n\n\n/\n?\n\n\nconds\n?"
  },
  {
    "objectID": "1_08_prob_rules.html#footnotes",
    "href": "1_08_prob_rules.html#footnotes",
    "title": "Probability Rules",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: Ask last week’s driver for the Rmd file, they should share it with the group via email or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nHint: For the starwars example data, if we were to ask what is the probability of a species being short, i.e. P(short), we would calculate the following:\nP(Droid ∩ short) + P(Ewok ∩ short) + P(Human ∩ short) + P(Wookiee ∩ short) = 0.10256410 + 0.02564103 + 0.38461538 + 0.00000000 = 0.51282051 ≈ 0.51.\nAlternatively, we would use look in the “Sum” value under the “short” column of our table.\n\nswars_rel_freq_sum\n\n\n               short       tall        Sum\n  Droid   0.10256410 0.02564103 0.12820513\n  Ewok    0.02564103 0.00000000 0.02564103\n  Human   0.38461538 0.41025641 0.79487179\n  Wookiee 0.00000000 0.05128205 0.05128205\n  Sum     0.51282051 0.48717949 1.00000000\n\n# P(short) = 0.51282051 ≈ 0.51\n\n↩︎\n\n\nHint: For the starwars example data, if we were to ask what is the probability of being short for the Human species, i.e. P(short | human), we could calculate the following:\nP(short | human) = P(short ∩ human) / P(human) = (0.38461538 / 0.79487179) = 0.483871 ≈ 0.48.\n\nswars_rel_freq_sum\n\n\n               short       tall        Sum\n  Droid   0.10256410 0.02564103 0.12820513\n  Ewok    0.02564103 0.00000000 0.02564103\n  Human   0.38461538 0.41025641 0.79487179\n  Wookiee 0.00000000 0.05128205 0.05128205\n  Sum     0.51282051 0.48717949 1.00000000\n\n# P(short | human) = P(short ∩ human) / P(human) = \n# (0.38461538 / 0.79487179) = 0.483871 ≈ 0.48 \n\n↩︎\n\n\nHint: For the starwars example data, if we were to ask what is the probability of a character being not a human given they are short, we could calculate the following:\n\nswars_rel_freq_sum\n\n\n               short       tall        Sum\n  Droid   0.10256410 0.02564103 0.12820513\n  Ewok    0.02564103 0.00000000 0.02564103\n  Human   0.38461538 0.41025641 0.79487179\n  Wookiee 0.00000000 0.05128205 0.05128205\n  Sum     0.51282051 0.48717949 1.00000000\n\n# P((~human) | short) = \n# (P(droid ∩ short) + P(ewok ∩ short) + P(wookiee ∩ short)) / P(short) =\n# (0.10256410 + 0.02564103 + 0.00000000) / 0.51282051 = 0.25\n\n↩︎\n\n\nHint: If we were to ask what is the probability of a species being Droid or being tall, i.e. P(Droid ∪ tall), we would compute the probability as:\nP(Droid ∪ tall) = P(Droid) + P(tall) - P(Droid ∩ tall)\n\nswars_rel_freq_sum\n\n\n               short       tall        Sum\n  Droid   0.10256410 0.02564103 0.12820513\n  Ewok    0.02564103 0.00000000 0.02564103\n  Human   0.38461538 0.41025641 0.79487179\n  Wookiee 0.00000000 0.05128205 0.05128205\n  Sum     0.51282051 0.48717949 1.00000000\n\n# P(Droid) = 0.12820513\n# P(tall) = 0.48717949\n# P(Droid ∩ tall) = 0.02564103\n\n# P(Droid ∪ tall) = P(Droid) + P(tall) - P(Droid ∩ tall) = \n# 0.12820513 + 0.48717949 - 0.02564103 = 0.5897436 ≈ 0.59\n\n↩︎\n\n\nHint: Recall that we say that two events \\(A\\) and \\(B\\) are independent if knowing that one occurred doesn’t change the probability of the other occurring, i.e. \\(P(A | B) = P(A)\\).\nIn the starwars example, we would say that being short seem to be dependent on the species, given that the conditional probabilities are different from P(short). For example, the probability of being short for Ewoks and Droids is much higher than P(short):\n\n#P(short) ≈ 0.51\n#P(short | droid) ≈ 0.80\n#P(short | ewok) ≈ 1\n#P(short | human) ≈ 0.48\n#P(short | wookiee) ≈ 0\n\n↩︎\n\nHint: Here it would be useful to think about which movie genre offers the best audience experience - lead studios will likely want to invest in making movies in Genres that people enjoy watching!↩︎\n\nHint: Make sure to load the ggmosaic package so that you can specify geom_mosaic() when building your plot. To add a title, as well as x- and y-axis titles, specify labs(title = , x = , y = ). This week we will also need to specify the conds() argument.\nExample: For the starwars dataset, I create a mosaic plot using the following code, and specify conds() within my aes() argument:\n\nlibrary(ggmosaic)\nm_plot &lt;- ggplot(starwars2) +\n    geom_mosaic(aes(x = product(size), fill = size, conds = product(species))) +\n    labs(title = \"Starwars Conditional Mosaic Plot Example\", \n         x = \"Species\", \n         y = \"Size\",\n         fill = \"Size\")\nm_plot\n\n\n\nFigure 1: Starwars Mosaic Plot Example Title\n\n\n\n↩︎\n\n\nHint: You may want to consider using proper notation in your write-up (as you have seen in lectures). To do so, you can use $ name $. For example, if I wanted to specify the general addition rule, in the main Rmd file (i.e., not a code chunk) I could write $p(A \\cup B) = p(A) + p(B) - p(A \\cap B)$. This would render in my main file as:\n\\(p(A \\cup B) = p(A) + p(B) - p(A \\cap B)\\)\nTo get the \\(\\cup\\) symbol, use the command $A \\cup B$.\nTo get the \\(\\cap\\) symbol, use the command $A \\cap B$.↩︎"
  },
  {
    "objectID": "1_10_cont_dist.html",
    "href": "1_10_cont_dist.html",
    "title": "Random Variables (Continuous)",
    "section": "",
    "text": "Instructions Recap - Formative Report B\n\n\n\n\n\n\nIn this block of the course (weeks 7-11), you should produce a PDF report using Rmarkdown for which you will receive formative feedback in week 12.\nThe report should not include any reference to R code or functions, but be written or a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\nYou will be required to submit a PDF file by 12 noon on Friday the 2nd of December 2022 via Learn. One person needs to submit on behalf of your group.\n\nThe report should be at most 6 pages long. At the end of the report, you are allowed two appendices which both don’t count towards the page limit.\n\nAppendix A will contain any tables or figures which you cannot fit in the page limit (no text allowed)\nAppendix B will contain the code to reproduce the report results (just like Formative Report A).\n\n\nNo extensions allowed. As this is group-based work, no extensions are possible."
  },
  {
    "objectID": "1_10_cont_dist.html#tasks",
    "href": "1_10_cont_dist.html#tasks",
    "title": "Random Variables (Continuous)",
    "section": "\n1 Tasks",
    "text": "1 Tasks\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task B4.\nB1) Create and summarise categorical variables, before calculating probabilities.\nB2) Investigate if events are independent, and compute probabilities.\nB3) Computing and plotting probabilities with a binomial distribution.\n\n\n\n\n\n\nThis week’s task\n\n\n\nB4) Computing and plotting probabilities with a normal distribution.\n\n\nB5) Plot standard error of the mean, and finish the report write-up (i.e., knit to PDF, and submit the PDF for formative feedback)."
  },
  {
    "objectID": "1_10_cont_dist.html#b4-sub-tasks",
    "href": "1_10_cont_dist.html#b4-sub-tasks",
    "title": "Random Variables (Continuous)",
    "section": "\n2 B4 sub-tasks",
    "text": "2 B4 sub-tasks\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nFocus on completing all of the lab tasks, and leave non-essential things like changing colors for later.\nIf, after looking at the hint, you still have no clue on how to answer a question, check the worked example below!\n\n\n\nIn this section you will find some guided sub-steps you may want to consider to complete task B4.\nAs detailed last week, a new movie theatre is opening in Texas soon. The management team are thinking of innovative launch events, and they’d like to host a movie trivia night where viewers can compete to win free cinema tickets for the year! They want to make sure however that their questions aren’t too easy, or too difficult, so gave viewers an IQ test so that they could see where to pitch their questions.\nAccording to a recent survey, the average IQ in Texas is 97.4. If at least 50% of movie viewers have an IQ score above this value, but less than 60% do, the the management team won’t need to make any changes (i.e., make it easier or harder) to their trivia questions.\nIn this lab, you will need to consider the variables IQ1 to IQ50 (i.e. the IQ score of each of the 50 audience raters for each movie) from the Hollywood movies dataset when answering the questions below.\n\nReopen last week’s Rmd file, and continue building on last week’s work. Make sure you are still using the movies dataset filtered to only include the top 3 genres.1\n\n\n\nConsider the IQ1-IQ50 variables, are they discrete or continuous?2\nCurrently the data are in wide format, but we want a single column of IQ scores. Pivot the data so that it is in long format.3\n\n\nAs each movie was rated by a different pool of 50 audience raters, discuss how this new column of IQ scores can be considered an independent sample of IQ scores.\nPlot the sample distribution of the IQ scores variable either using a histogram or density plot.4\n\n\nWhat kind of distribution does your plot follow? Estimate the parameters of your distribution from the sample data.5\n\n\n\nPlot the fitted normal distribution on top of the sample distribution. Is the normal distribution a good fit?6\n\n\n\nCalculate the following:7\n\nThe probability of a movie viewer having an IQ score &lt;97.4, i.e. less than the Texas average.\nThe probability of a movie viewer having an IQ score &gt;97.4, i.e. more than the Texas average.\nThe probability of a movie viewer having an IQ score between 90-109, corresponding to the Average category in the Wechsler Intelligence Scale.\n\n\n\n\nCalculate the following:8\n\nThe value of IQ that 25% of movie watchers have scores equal to or less than\nThe value of IQ that 50% of movie watchers have scores equal to or less than\nThe value of IQ that 75% of movie watchers have scores equal to or less than\nThe interval comprising 95% of the people’s IQ scores in the sample\n\n\n\n\nHow do the normal quartiles computed above compare to those obtained from the summary() function?\nBased on the probabilities you have reported above, do you think that the new movie theatre should simplify the questions for their trivia night, make them harder, or make no changes? Justify your answer.\nIn the analysis section of your report, write up a summary of what you have reported above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions."
  },
  {
    "objectID": "1_10_cont_dist.html#worked-example",
    "href": "1_10_cont_dist.html#worked-example",
    "title": "Random Variables (Continuous)",
    "section": "\n3 Worked Example",
    "text": "3 Worked Example\nThe dataset available at https://uoepsy.github.io/data/RestaurantTips2.csv was collected by the owner of a US bistro, and contains 99 observations on 10 variables. It is a subset of the RestaurantTips.csv data presented in the past weeks, focusing only on parties of 2 people.9\nFollowing from the bistro owner’s interest in whether they should consider introducing a 2 for 1 coffee deal or a loyalty scheme, they are also considering running a weekly quiz night in the bistro. The quiz is only open to pairs of individuals, and given that the bistro is located in a student area, the owner wants to make the quiz very demanding, and therefore wants the questions to be at a level where someone with a ‘superior’ or ‘well above average’ IQ (i.e., scores 120+) would be challenged. We need to advise the bistro owner whether the questions that they have generated are at their desired level of difficulty.\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\nHadCoffee\nNumber of guests in the group who had coffee\n\n\nIQ1\nScore on IQ test for guest 1\n\n\nIQ2\nScore on IQ test for guest 2\n\n\n\n\n\n\nlibrary(tidyverse)\ntips2 &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips2.csv\")\nhead(tips2)\n\n# A tibble: 6 × 10\n   Bill   Tip Credit Guests Day   Server PctTip HadCoffee   IQ1   IQ2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2         2    93   100\n2  32.0  5.01 y           2 f     A        15.7         2    96    98\n3  17.4  3.61 y           2 f     B        20.8         2    94    99\n4  15.4  3    n           2 f     B        19.5         2    99   108\n5  18.6  2.5  n           2 f     A        13.4         2   129   106\n6  21.6  3.44 n           2 f     B        16           2    82   118\n\n\n\nIf we were asked to describe what kind of variables IQ1 and IQ2 were, and to comment on the kind of probability distribution it may follow, we could say:\n\nIQ1 and IQ2 represent the IQ scores of each individual within a group of two. These are both continuous variables that could be modeled by a normal distribution.\n\nWe could transform the tips2 dataset from wide to long format using the following command:\n\n\ntips2_long &lt;- tips2 %&gt;%\n    pivot_longer(IQ1:IQ2, names_to = \"ID\", values_to = \"IQ_Scores\")\ntips2_long\n\n# A tibble: 198 × 10\n    Bill   Tip Credit Guests Day   Server PctTip HadCoffee ID    IQ_Scores\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1  23.7 10    n           2 f     A        42.2         2 IQ1          93\n 2  23.7 10    n           2 f     A        42.2         2 IQ2         100\n 3  32.0  5.01 y           2 f     A        15.7         2 IQ1          96\n 4  32.0  5.01 y           2 f     A        15.7         2 IQ2          98\n 5  17.4  3.61 y           2 f     B        20.8         2 IQ1          94\n 6  17.4  3.61 y           2 f     B        20.8         2 IQ2          99\n 7  15.4  3    n           2 f     B        19.5         2 IQ1          99\n 8  15.4  3    n           2 f     B        19.5         2 IQ2         108\n 9  18.6  2.5  n           2 f     A        13.4         2 IQ1         129\n10  18.6  2.5  n           2 f     A        13.4         2 IQ2         106\n# ℹ 188 more rows\n\n\n\n\npivot_longer()\nPivot longer takes “wide” data and transforms them to “long” form. An example is given below:\n\ntoy_data &lt;- tibble(\n    a = c(1, 2, 3),\n    x1 = c(11, 12, 13),\n    x2 = c(21, 22, 23)\n)\ntoy_data\n\n# A tibble: 3 × 3\n      a    x1    x2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    11    21\n2     2    12    22\n3     3    13    23\n\n\n\nlong_toy_data &lt;- toy_data %&gt;%\n    pivot_longer(x1:x2, names_to = \"x\", values_to = \"score\")\nlong_toy_data\n\n# A tibble: 6 × 3\n      a x     score\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 x1       11\n2     1 x2       21\n3     2 x1       12\n4     2 x2       22\n5     3 x1       13\n6     3 x2       23\n\n\n\nWe can plot the sample distribution of the IQ_Scores variable either using a histogram or a density plot.\n\nLet’s plot both the histogram and density plot for instructional purposes only, in a report you would only report one of the two as they show similar information. We plot both now for instructional purposes and, in particular, we ask you to focus on the differences in the y axes.\n\nlibrary(patchwork)\nplt_hist &lt;- ggplot(tips2_long, aes(x = IQ_Scores)) + \n    geom_histogram(colour = 'white') +\n    labs(x = \"IQ scores\")\n\nplt_dens &lt;- ggplot(tips2_long, aes(x = IQ_Scores)) + \n    geom_density() + \n    labs(x = \"IQ scores\")\n\nplt_hist | plt_dens \n\n\n\n\nAs you will notice, the plot on the left panel shows counts on the y axis, i.e. the absolute frequency of each bin interval. The plot on the right panel, instead, shows the density on the y axis. If we wanted to plot on top of the histogram a normal curve, we would need to change the y axis of the histogram to be a comparable measure, i.e. also a density rather than a count.\nYou can do so by adding y = after_stat(density) in the aes() specification, and you can now see that both y axes agree:\n\nplt_hist &lt;- ggplot(tips2_long, aes(x = IQ_Scores, y = after_stat(density))) + \n    geom_histogram(colour = 'white') +\n    labs(x = \"IQ scores\")\n\nplt_dens &lt;- ggplot(tips2_long, aes(x = IQ_Scores)) + \n    geom_density() + \n    labs(x = \"IQ scores\")\n\nplt_hist | plt_dens \n\n\n\n\n\n\n\n\n\n\nFitting a distribution\n\n\n\n\n\nFitting a normal distribution to data involves estimating the parameters of the distribution from the data. In other words, we want to find values for \\(\\mu\\) and \\(\\sigma\\) from the variable IQ_Scores in the our data, and we denote the estimated parameters with \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\). You create those symbols, you can type in the Rmd file $\\hat{\\mu}$ and $\\hat{\\sigma}$.\n\n\n\n\n\nWe can now fit a normal distribution to the variable. To do so, we need to start by estimating the parameters of the normal distribution:\n\n\n\\(\\hat{\\mu}\\), the mean IQ score in the sample\n\n\\(\\hat{\\sigma}\\), the estimated standard deviation in the sample\n\n\n\n\n# The mean of the sample of IQ scores\ntips_mu_hat &lt;- mean(tips2_long$IQ_Scores) \ntips_mu_hat\n\n[1] 99.33333\n\n# The standard deviation of the sample of IQ scores\ntips_sigma_hat &lt;- sd(tips2_long$IQ_Scores)\ntips_sigma_hat\n\n[1] 15.7025\n\n\n\nWe can then compare the sample distribution to the normal distribution, and comment on whether the normal fit is good:\n\nFirst we will create a tibble with the normal distribution at a grid of x values.\n\n\nM \\(\\pm\\) 4 * SD vs M \\(\\pm\\) 3 * SD\nTypically, the interval M \\(\\pm\\) 3 * SD includes roughly all of the distribution values (99.7%). However, in this case a few IQ scores escape the interval (as they are so widespread). As such, we extended the plotting interval to M \\(\\pm\\) 4 * SD.\nFeel free to try with a grid of x values between mean \\(\\pm\\) 3 * SD, and you will see that some scores exceed the plot range.\n\nnormal_distr &lt;- tibble(\n    x_grid = seq(tips_mu_hat - 4 * tips_sigma_hat, \n                 tips_mu_hat + 4 * tips_sigma_hat, \n                 by = 0.1),\n    y_grid = dnorm(x_grid, mean = tips_mu_hat, sd = tips_sigma_hat)\n)\n\nWe can plot the sample distribution and put the fitted normal distribution on top:\n\nbistro_iq &lt;- ggplot() + \n    geom_histogram(data = tips2_long, aes(x = IQ_Scores, y = after_stat(density)), \n                   color = 'white') +\n    geom_line(data = normal_distr, aes(x = x_grid, y = y_grid),\n              color = 'red') +\n    labs(x = \"IQ scores\")\nbistro_iq\n\n\n\nFigure 1: Distribution of Bistro Customer IQ Scores.\n\n\n\nThe normal distribution seems to be a good fit for the sample distribution, as the shape of the histogram tends to agree with the normal distribution across all possible values. The histogram appears to be symmetric around the mean and follows the normal bell-shaped curve.\n\nWe can use the fitted normal distribution to calculate the probabilities, for example:\n\n\n\nP(X &lt; 120)\nP(X &gt; 120)\nP(X &gt; 100)\nP(100 &lt; X &lt; 130)\n\n\n\nTo calculate the probability that a bistro customer has an IQ below 120, we can compute the following:\n\npnorm(120, tips_mu_hat, tips_sigma_hat)\n\n[1] 0.9059362\n\n\n\n\nTo calculate the probability that a bistro customer has an IQ above 120, we can compute the following:\n\n# P(X &gt; 120)\npnorm(120, tips_mu_hat, tips_sigma_hat, lower.tail = FALSE)\n\n[1] 0.09406377\n\n\nor alternatively\n\n# 1 - P(X &lt; 120)\n1 - pnorm(120, tips_mu_hat, tips_sigma_hat)\n\n[1] 0.09406377\n\n\n\n\nTo calculate the probability that a bistro customer has an IQ above 100, we can compute the following:\n\n# P(X &gt; 100)\npnorm(100, tips_mu_hat, tips_sigma_hat, lower.tail = FALSE)\n\n[1] 0.4830676\n\n\nor alternatively\n\n# 1 - P(X &lt; 100)\n1 - pnorm(100, tips_mu_hat, tips_sigma_hat)\n\n[1] 0.4830676\n\n\n\n\nTo calculate the probability that a bistro customer has an IQ between 100 and 130, we can compute the following:\nP(a &lt; X &lt; b) = P(X &lt; b) - P(X &lt; a) when a = 100 and b = 130 so that P(100 &lt; X &lt; 130) = P(X &lt; 130) - P(X &lt; 100)\n\n# P(100 &lt; X &lt; 130) = \n# P(X &lt; 130) - P(X &lt; 100)\npnorm(130, tips_mu_hat, tips_sigma_hat) - pnorm(100, tips_mu_hat, tips_sigma_hat)\n\n[1] 0.4576566\n\n\n\n\n\n\nWe can calculate the value of IQ that a specific percentage of bistro customers have scores equal to or less than, for example:\n\n\ns.t. means such that\n\n\n\nx s.t. P(X &lt;= x) = 0.25\nx s.t. P(X &lt;= x) = 0.50\nx s.t. P(X &lt;= x) = 0.75\n\n\n\nTo calculate the value of IQ that 25% of bistro customers have scores equal to or less than, we can compute the following:\n\nqnorm(p = 0.25, mean = tips_mu_hat, sd = tips_sigma_hat)\n\n[1] 88.74216\n\n\n\n\nTo calculate the value of IQ that 50% of bistro customers have scores equal to or less than, we can compute the following:\n\nqnorm(0.5, tips_mu_hat, tips_sigma_hat)\n\n[1] 99.33333\n\n\n\n\nTo calculate the value of IQ that 75% of bistro customers have scores equal to or less than, we can compute the following:\n\nqnorm(0.75, tips_mu_hat, tips_sigma_hat)\n\n[1] 109.9245\n\n\n\n\n\n\nWe can also comment on how these normal quartiles values compare to those from the summary() output:\n\n\nsummary(tips2_long$IQ_Scores)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  57.00   90.25   98.00   99.33  109.75  154.00 \n\n\nThe estimated quartiles are very similar to those from the summary output, providing further indication that the sample of IQ scores follows a normal distribution.\n\nTo summarise the IQ scores of the bistro customers, we could compute the interval comprising the middle 95% of the customers IQ scores:\n\nIf we want 0.95 probability in the middle, we need to split the leftover 0.05 probability equally between the two tails: 0.025 in the left, and 0.025 in the right. However, qnorm() wants the probability to the left, so we will give 0.025 and 0.975 = 1 - 0.025 as values.\n\nqnorm(c(0.025, 0.975), mean = tips_mu_hat, sd = tips_sigma_hat)\n\n[1]  68.5570 130.1097\n\n\n95% of the bistro customers had IQ scores between 69 and 130.\n\n\n\n\n\n\nAlternatively\n\n\n\n\n\nAlternatively, to specify that one probability is on the left tail and the other is on the upper tail, you have to run two separate statements:\n\nqnorm(0.025, mean = tips_mu_hat, sd = tips_sigma_hat)\n\n[1] 68.557\n\nqnorm(0.025, mean = tips_mu_hat, sd = tips_sigma_hat, lower.tail = FALSE)\n\n[1] 130.1097\n\n\n\n\n\nFor a classification of IQ scores, see the Wechsler Intelligence Scales classification table.\n\n\n\n\n\n\nExample writeup\n\n\n\nFigure 1 displays the distribution of IQ scores, with a normal distribution superimposed as a red histogram. Most of the bistro customers (95%) have IQ scores between 69 and 130. Based on our analysis above, 48% of bistro customers are likely to have IQ scores above 100, though there is less than a 10% chance that customers have superior IQ scores (i.e., above 120). Therefore the bistro owner has set the questions to his quiz at the desired level of difficulty - the vast majority of his customers should find the questions extremely challenging. However, if the Bistro owner did want to make some changes, they should only consider making the quiz more difficult."
  },
  {
    "objectID": "1_10_cont_dist.html#student-glossary",
    "href": "1_10_cont_dist.html#student-glossary",
    "title": "Random Variables (Continuous)",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\ndnorm()\n?\n\n\npnorm()\n?\n\n\npivot_longer()\n?\n\n\nseq()\n?\n\n\ngeom_histogram()\n?\n\n\ngeom_density()\n?\n\n\ngeom_line()\n?\n\n\nafter_stat()\n?"
  },
  {
    "objectID": "1_10_cont_dist.html#footnotes",
    "href": "1_10_cont_dist.html#footnotes",
    "title": "Random Variables (Continuous)",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: Ask last week’s driver for the Rmd file, they should share it with the group via email or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\nHint: Technically, the values you see in the data are discrete, but in Psychology they are considered as coming from a continuous scale that we can only measure with discrete precision.↩︎\n\nHint: When the data is wide, we can make it long using pivot_longer(). When we make data longer, we’re essentially making lots of columns into 2 longer columns. Below, in the animation, the wide variable x, y and z go into a new longer column called name (specified by the argument names_to =) that specifies which (x/y/z) it came from, and the values get put into the ‘val’ column (specified by the argument values_to =).\n\n\n\n\nSource: https://fromthebottomoftheheap.net/2019/10/25/pivoting-tidily/\n\n\n\n↩︎\n\n\nHint: Here you will need to specify geom_histogram() or geom_density().\nTo save space, if you wish to plot this next to another plot, recall that you can use the patchwork package to arrange plots either adjacent to one another (via |) or stacked (via /).↩︎\n\nHint: A normal distribution has two parameters, the mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)).↩︎\nHint: Here you will need to consider using dnorm(), as you are interested in the normal distribution at values of x. First you will need to consider how many standard deviations above and below the mean to give when specifying your grid of x-axis values (hint: it’s between 1-5!). Next you will need to specify the grid for your y-axis - here use the dnorm() function, specifying your x-axis grid, sample mean, and sample standard deviation.↩︎\n\nHint: Here you are being asked to calculate:\n\nP(X &lt;= x) = P(X &lt; x)\n\nP(X &gt;= x) = P(X &gt; x)\n\nP(a &lt; X &lt; b) = P(X &lt; b) - P(X &lt; a)\n\n↩︎\n\n\nHint:\nFor the first three questions, you are being asked to calculate the quantile x such that P(X &lt;= x) is 0.25, 0.50, and 0.75 respectively. That is, you are asked to find the first, second, and third quartiles respectively.\nFor the final question, you are looking for those values that comprise the middle 0.95 of the distribution, i.e. cutting an area of 0.025 to the left and 0.975 = 1 - 0.025 to the right. The 5% probability is split between the lower and upper tails of the distribution. In symbols, you are asked to calculate P(a &lt; X &lt; b) where a is the 2.5th percentile and b is the 97.5th percentile.↩︎\n\nData adapted from Lock et al. (2020).↩︎"
  },
  {
    "objectID": "2_01_confint.html",
    "href": "2_01_confint.html",
    "title": "Confidence intervals",
    "section": "",
    "text": "Formative report C - Instructions\n\n\n\n\n\n\nIn the next five weeks of the course you should produce a PDF report using Rmarkdown for which you will receive formative feedback in Flexible Learning Week.\nThe report should not include any reference to R code or functions, but be written for a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\nYou will be required to submit a PDF file by 12 noon on Friday the 17th of February 2023 via Learn. One person needs to submit on behalf of your group.\n\nThe report should be at most 6 pages long. At the end of the report, you are allowed two appendices which both don’t count towards the page limit.\n\nAppendix A will contain any tables or figures which you cannot fit in the page limit (no text allowed).\nAppendix B will contain the code to reproduce the report results.\n\n\nNo extensions allowed. As this is group-based work, no extensions are possible.\n\n\n\n\n\n\n\n\n\n\nAre you registered for your group on Learn?\n\n\n\n\n\nGo to the course Learn page, on the left-hand side click “Groups information”, then the lab, and then the group name. Click Sign up.\n\n\n\n\n\n\n\n\n\nChange driver this week\n\n\n\n\n\nIn the next five weeks your group will be creating a new formative report, Formative Report C.\n\nChoose a driver for this week\n\nThe driver should login to the PC provided with the desk, and access RStudio Server\nThe driver is the only person allowed to type the report during this lab\n\n\nThe others in the group are the navigators\n\nNavigators are responsible for suggesting and commenting on the strategy that the driver needs to follow to answer the tasks, as well as correct typos and coding errors.\n\n\nIt is important that your group chooses a driver for this week, and in the next weeks the driver rotates every week to ensure that everyone in the group has contributed to the writing of the report.\n\n\n\n\n\n\n\n\n\n\nCreate a new RMD file\n\n\n\n\n\n\nCreate a new Rmd file for formative report C which you will build upon each week in your group.\nAt the end of each lab, save the Rmd file and share it with your group. If you go to your group area on Learn, you can click “Send Email” to share the file with your group."
  },
  {
    "objectID": "2_01_confint.html#formative-report-c-instructions",
    "href": "2_01_confint.html#formative-report-c-instructions",
    "title": "Confidence intervals",
    "section": "",
    "text": "Formative report C - Instructions\n\n\n\n\n\n\nIn the next five weeks of the course you should produce a PDF report using Rmarkdown for which you will receive formative feedback in Flexible Learning Week.\nThe report should not include any reference to R code or functions, but be written for a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\nYou will be required to submit a PDF file by 12 noon on Friday the 17th of February 2023 via Learn. One person needs to submit on behalf of your group.\n\nThe report should be at most 6 pages long. At the end of the report, you are allowed two appendices which both don’t count towards the page limit.\n\nAppendix A will contain any tables or figures which you cannot fit in the page limit (no text allowed).\nAppendix B will contain the code to reproduce the report results.\n\n\nNo extensions allowed. As this is group-based work, no extensions are possible.\n\n\n\n\n\n\n\n\n\n\nAre you registered for your group on Learn?\n\n\n\n\n\nGo to the course Learn page, on the left-hand side click “Groups information”, then the lab, and then the group name. Click Sign up.\n\n\n\n\n\n\n\n\n\nChange driver this week\n\n\n\n\n\nIn the next five weeks your group will be creating a new formative report, Formative Report C.\n\nChoose a driver for this week\n\nThe driver should login to the PC provided with the desk, and access RStudio Server\nThe driver is the only person allowed to type the report during this lab\n\n\nThe others in the group are the navigators\n\nNavigators are responsible for suggesting and commenting on the strategy that the driver needs to follow to answer the tasks, as well as correct typos and coding errors.\n\n\nIt is important that your group chooses a driver for this week, and in the next weeks the driver rotates every week to ensure that everyone in the group has contributed to the writing of the report.\n\n\n\n\n\n\n\n\n\n\nCreate a new RMD file\n\n\n\n\n\n\nCreate a new Rmd file for formative report C which you will build upon each week in your group.\nAt the end of each lab, save the Rmd file and share it with your group. If you go to your group area on Learn, you can click “Send Email” to share the file with your group."
  },
  {
    "objectID": "2_01_confint.html#tasks",
    "href": "2_01_confint.html#tasks",
    "title": "Confidence intervals",
    "section": "\n2 Tasks",
    "text": "2 Tasks\nThe data dataset-ipeds-2012-subset2, available at https://uoepsy.github.io/data/dataset-ipeds-2012-subset2.csv, is a subset of data derived from the Integrated Postsecondary Education Data System (IPEDS) at the National Center for Education Statistics, 2012. The data were collected for a random sample from all colleges and universities in the United States in that year. The variables include:\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\ntype\nCollege Type:−1 = Not reported;\n1 = Public;\n2 = Private for-profit;\n3 = Private not-for-profit (no religious affiliation);\n4 = Private not-for-profit (religious affiliation)\n\n\n\nregion\nRegion:0 = US Service schools;\n1 = New England;\n2 = Mid East;\n3 = Great Lakes;\n4 = Plains;\n5 = Southeast;\n6 = Southwest;\n7 = Rocky Mountains;\n8 = Far West;\n9 = Outlying areas\n\n\n\ngradrate\nGraduation Rate – All\n\n\ngradratem\nGraduation Rate – Men\n\n\ngradratew\nGraduation Rate – Women\n\n\n\n\n\nIn formative report C, you will investigate the mean graduation rate for female students at colleges and universities in the United States. Specifically, you are asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task C1.\n\n\n\n\n\n\nThis week’s task\n\n\n\nC1) Read the data into R, describe the variable of interest both visually and numerically, and provide a 95% CI for the mean graduation rate of female students at colleges and universities in the United States.\n\n\nC2) At the 5% significance level and using the p-value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\nC3) At the 5% significance level and using the critical value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\nC4) Tidy up your report so far, making sure to have 3 sections: introduction, analysis and discussion.\nC5) Compute and report the effect size, check if the assumptions underlying the t-test are violated."
  },
  {
    "objectID": "2_01_confint.html#c1-sub-tasks",
    "href": "2_01_confint.html#c1-sub-tasks",
    "title": "Confidence intervals",
    "section": "\n3 C1 sub-tasks",
    "text": "3 C1 sub-tasks\nIn this section you will find some guided sub-steps you may want to consider to complete task C1.\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nRead the data into R and inspect it.1\nHow many units are there?2\nVisualise the distribution of the variable of interest (gradratew). What is the shape of the distribution? Are there any outliers?3\nCompute and interpret a table of descriptive statistics for the variable of interest. At a minimum, ensure that it includes both a measure of centre and spread.4\nCompute a 95% confidence interval for the mean graduation rate of female college students in 2012.5\n\n\nFor the report introduction, write a brief introduction to the data and question being investigated. How many cases are there? Is there any impossible values? What is the type of the variables and which one is used for the investigation?\nProvide a write up of your results so far, using proper rounding and making sure to report your results in context of the investigation."
  },
  {
    "objectID": "2_01_confint.html#worked-example",
    "href": "2_01_confint.html#worked-example",
    "title": "Confidence intervals",
    "section": "\n4 Worked Example",
    "text": "4 Worked Example\nThe Procrastination Assessment Scale for Students (PASS) was designed to assess how individuals approach decision situations, specifically the tendency of individuals to postpone decisions (Solomon & Rothblum, 1984).\nThe PASS assesses the prevalence of procrastination in six areas: writing a paper; studying for an exam; keeping up with reading; administrative tasks; attending meetings; and performing general tasks. For a measure of total endorsement of procrastination, responses to 18 questions (each measured on a 1-5 scale) are summed together, providing a single score for each participant (range 0 to 90). The mean score from Solomon & Rothblum, 1984 was 33.\n\nInvestigation:\nWhat is the average procrastination score of Edinburgh University students?\n\nTo answer this question, we will use data collected for a random sample of students from the University of Edinburgh: https://uoepsy.github.io/data/pass_scores.csv\n\n\n\n\nVariable Name\nDescription\n\n\n\nsid\nSubject identifier\n\n\nschool\nSchool each subject belonged to\n\n\nPASS\nTotal endorsement of procrastination score\n\n\n\n\n\nNecessary packages:\n\n\n\ntidyverse for using read_csv(), using summarise() and ggplot().\npatchwork for arranging plots side by side or underneath\nkableExtra for creating user-friendly tables\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(kableExtra)\n\nRead the data into R:\n\npass_scores &lt;- read_csv(\"https://uoepsy.github.io/data/pass_scores.csv\")\ndim(pass_scores)\n\n[1] 20  3\n\n\nTo inspect the data:\n\n\nhead()\nglimpse()\nsummary()\n\n\n\n\nhead(pass_scores)\n\n# A tibble: 6 × 3\n  sid   school       PASS\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 s_1   GeoSciences    31\n2 s_2   ECA            24\n3 s_3   LAW            32\n4 s_4   ECA            40\n5 s_5   LAW            28\n6 s_6   SSPS           31\n\n\n\n\n\nglimpse(pass_scores)\n\nRows: 20\nColumns: 3\n$ sid    &lt;chr&gt; \"s_1\", \"s_2\", \"s_3\", \"s_4\", \"s_5\", \"s_6\", \"s_7\", \"s_8\", \"s_9\", …\n$ school &lt;chr&gt; \"GeoSciences\", \"ECA\", \"LAW\", \"ECA\", \"LAW\", \"SSPS\", \"PPLS\", \"SLL…\n$ PASS   &lt;dbl&gt; 31, 24, 32, 40, 28, 31, 30, 28, 32, 29, 28, 33, 35, 33, 30, 31,…\n\n\n\n\n\nsummary(pass_scores)\n\n     sid               school               PASS      \n Length:20          Length:20          Min.   :24.00  \n Class :character   Class :character   1st Qu.:28.75  \n Mode  :character   Mode  :character   Median :31.00  \n                                       Mean   :30.70  \n                                       3rd Qu.:32.00  \n                                       Max.   :40.00  \n\n\n\n\n\nVisualise the distribution of PASS scores:\n\n\nNote\nThe boxplot highlights an outlier (40). However, this value is well within the plausible range of the scale (0 – 90), hence it is of no concern and the point can be kept for the analysis.\n\nplt_hist &lt;- ggplot(pass_scores, aes(x = PASS)) + \n    geom_histogram(color = 'white')\n\nplt_box &lt;- ggplot(pass_scores, aes(x = PASS)) + \n    geom_boxplot()\n\nplt_hist / plt_box\n\n\n\n\nDescriptive statistics:\n\nstats &lt;- pass_scores %&gt;%\n    summarise(n = n(),\n              Min = min(PASS),\n              Max = max(PASS),\n              M = mean(PASS),\n              SD = sd(PASS))\n\n\nstats %&gt;%\n    kbl(booktabs = TRUE, digits = 2, \n        caption = \"Descriptive statistics for PASS scores\")\n\n\n\n\nDescriptive statistics for PASS scores\n\nn\nMin\nMax\nM\nSD\n\n\n20\n24\n40\n30.7\n3.31\n\n\n\n\nWhen estimating a parameter, in this case the mean score on the Procrastination Assessment Scale for Students (PASS) for all Edinburgh University students, we do not just report the estimate (sample average score), but also something that reflects our uncertainty in the estimate. This can either be the standard error or a confidence interval. If asked to compute a 95% confidence interval for the mean score on the Procrastination Assessment Scale for Students (PASS) for all Edinburgh University students, we could do:\n\n# Sample mean\nxbar &lt;- stats$M\n\n# Standard error\ns &lt;- stats$SD\nn &lt;- stats$n\nse &lt;- s / sqrt(n)\nse\n\n[1] 0.7401991\n\n# Quantiles\ntstar &lt;- qt(c(0.025, 0.975), df = n - 1)\ntstar\n\n[1] -2.093024  2.093024\n\n# CI\nxbar + tstar * se\n\n[1] 29.15075 32.24925\n\n\n\n\nWARNING!\nThis code won’t work if stats stores a kable, i.e. the result of kbl(). Make sure this only stores the tibble, rather than the pretty version from kbl()!\nReport\nThese three code chunks should not be visible in the report. You can simply report the CI in a paragraph using the style [LowerCI, UpperCI].\n\n\n\n\n\n\nExample introduction\n\n\n\nA random sample of 20 students from the University of Edinburgh completed a questionnaire measuring their total endorsement of procrastination. The data, available from https://uoepsy.github.io/data/pass_scores.csv, were used to estimate the average procrastination score of all Edinburgh University students. The recorded variables included a subject identifier (sid), the school of each subject (school), and the total score on the Procrastination Assessment Scale for Students (PASS). The data do not include any impossible values for the PASS scores, as they were all within the possible range of 0 – 90. To answer the question of interest, in the following we will only focus on the total PASS score variable.\n\n\n\n\n\n\n\n\nExample CI interpretation\n\n\n\nFrom the sample data we obtain an average procrastination score of \\(M = 30.7\\), 95% CI [29.15, 32.25]. Hence, we are 95% confident that a Edinburgh University student will have a procrastination score between 29.15 and 32.25, which is between 0.75 and 3.85 lower than the average score of 33 reported by Solomon & Rothblum."
  },
  {
    "objectID": "2_01_confint.html#student-glossary",
    "href": "2_01_confint.html#student-glossary",
    "title": "Confidence intervals",
    "section": "\n5 Student Glossary",
    "text": "5 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\ngeom_histogram\n?\n\n\ngeom_boxplot\n?\n\n\nsummarise\n?\n\n\nn()\n?\n\n\nmean\n?\n\n\nsd\n?\n\n\nqt\n?"
  },
  {
    "objectID": "2_01_confint.html#footnotes",
    "href": "2_01_confint.html#footnotes",
    "title": "Confidence intervals",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: Some of the following functions may be useful: read_csv() from tidyverse, head(), glimpse(), summary()↩︎\nHint: Some of the following functions may be useful: nrow(DATA), dim(DATA), length(DATA$Y), summarise(n = n())↩︎\nHint: geom_histogram(), geom_density(), geom_boxplot() may be useful functions.↩︎\nHint: summarise() from the tidyverse package or describe() from the psych package↩︎\n\nHints:\nStep 1: Compute the average gradution rate of female college students\nStep 2: Compute the standard error of the mean\nStep 3: Compute the quantiles of a t distribution with \\(n-1\\) degrees of freedom, where \\(n\\) = sample size, cutting a probability of 0.95 in between them.\nStep 4: Obtain the confidence interval using the formula:\n\n\\[95\\% \\text{ CI: } \\left[ \\bar x - t^* SE_{\\bar{x}}, \\  \\bar x + t^* SE_{\\bar{x}} \\right]\\] \\[\\text{where} \\qquad SE_{\\bar{x}} = \\frac{s}{\\sqrt n}\\]↩︎"
  },
  {
    "objectID": "2_03_ht_critvalues.html",
    "href": "2_03_ht_critvalues.html",
    "title": "Hypothesis testing: critical values",
    "section": "",
    "text": "The data dataset-ipeds-2012-subset2, available at https://uoepsy.github.io/data/dataset-ipeds-2012-subset2.csv, is a subset of data derived from the Integrated Postsecondary Education Data System (IPEDS) at the National Center for Education Statistics, 2012. The data were collected for a random sample from all colleges and universities in the United States in that year. The variables include:\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\ntype\nCollege Type:−1 = Not reported;\n1 = Public;\n2 = Private for-profit;\n3 = Private not-for-profit (no religious affiliation);\n4 = Private not-for-profit (religious affiliation)\n\n\n\nregion\nRegion:0 = US Service schools;\n1 = New England;\n2 = Mid East;\n3 = Great Lakes;\n4 = Plains;\n5 = Southeast;\n6 = Southwest;\n7 = Rocky Mountains;\n8 = Far West;\n9 = Outlying areas\n\n\n\ngradrate\nGraduation Rate – All\n\n\ngradratem\nGraduation Rate – Men\n\n\ngradratew\nGraduation Rate – Women\n\n\n\n\n\nIn formative report C, you will investigate the mean graduation rate for female students at colleges and universities in the United States. Specifically, you are asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task C3.\nC1) Read the data into R, describe the variable of interest both visually and numerically, and provide a 95% CI for the mean graduation rate of female students at colleges and universities in the United States.\nC2) At the 5% significance level and using the p-value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\n\n\n\n\n\n\nThis week’s task\n\n\n\nC3) At the 5% significance level and using the critical value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\n\n\nC4) Tidy up your report so far, making sure to have 3 sections: introduction, analysis and discussion.\nC5) Compute and report the effect size, check if the assumptions underlying the t-test are violated."
  },
  {
    "objectID": "2_03_ht_critvalues.html#tasks",
    "href": "2_03_ht_critvalues.html#tasks",
    "title": "Hypothesis testing: critical values",
    "section": "",
    "text": "The data dataset-ipeds-2012-subset2, available at https://uoepsy.github.io/data/dataset-ipeds-2012-subset2.csv, is a subset of data derived from the Integrated Postsecondary Education Data System (IPEDS) at the National Center for Education Statistics, 2012. The data were collected for a random sample from all colleges and universities in the United States in that year. The variables include:\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\ntype\nCollege Type:−1 = Not reported;\n1 = Public;\n2 = Private for-profit;\n3 = Private not-for-profit (no religious affiliation);\n4 = Private not-for-profit (religious affiliation)\n\n\n\nregion\nRegion:0 = US Service schools;\n1 = New England;\n2 = Mid East;\n3 = Great Lakes;\n4 = Plains;\n5 = Southeast;\n6 = Southwest;\n7 = Rocky Mountains;\n8 = Far West;\n9 = Outlying areas\n\n\n\ngradrate\nGraduation Rate – All\n\n\ngradratem\nGraduation Rate – Men\n\n\ngradratew\nGraduation Rate – Women\n\n\n\n\n\nIn formative report C, you will investigate the mean graduation rate for female students at colleges and universities in the United States. Specifically, you are asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task C3.\nC1) Read the data into R, describe the variable of interest both visually and numerically, and provide a 95% CI for the mean graduation rate of female students at colleges and universities in the United States.\nC2) At the 5% significance level and using the p-value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\n\n\n\n\n\n\nThis week’s task\n\n\n\nC3) At the 5% significance level and using the critical value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\n\n\nC4) Tidy up your report so far, making sure to have 3 sections: introduction, analysis and discussion.\nC5) Compute and report the effect size, check if the assumptions underlying the t-test are violated."
  },
  {
    "objectID": "2_03_ht_critvalues.html#c3-sub-tasks",
    "href": "2_03_ht_critvalues.html#c3-sub-tasks",
    "title": "Hypothesis testing: critical values",
    "section": "\n2 C3 sub-tasks",
    "text": "2 C3 sub-tasks\nIn this section you will find some guided sub-steps you may want to consider to complete task C3.\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\n\n\n\n\n\nThe steps below will largely overlap with last week’s lab, as you are using an equivalent approach to test a hypothesis. Of course, this will lead to repetition and duplicate information.\nNext week you will tidy up your report text to only include one of the two approaches to hypothesis testing, and for the reporting you can use the one that you prefer! We should always avoid duplication of information in the final report.\n\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\nState the null and alternative hypotheses.2\n\n\n\nCompute the value of the t-statistic from the sample mean graduation rate of female students.3\n\n\n\nIdentify the null distribution.4\nCompute the critical values of the null distribution using the appropriate significance level.5\nMake a decision on whether or not to reject the null hypothesis.\nProvide a write up of your results in the context of the research question.\n\n\n\n\n\n\n\nReporting with critical values When you use the critical value method, you don’t have a computed p-value, so the reporting will only say whether p &lt; .05 or p &gt; .05 depending on whether the observed t-statistic is beyond or not beyond the critical value(s).\n\n\nIf \\(t\\) is in the rejection region:\n\np &lt; .05 (if you use a different \\(\\alpha\\), change accordingly)\n\n\n\nIf \\(t\\) is not in the rejection region:\n\np &gt; .05 (if you use a different \\(\\alpha\\), change accordingly)"
  },
  {
    "objectID": "2_03_ht_critvalues.html#worked-example",
    "href": "2_03_ht_critvalues.html#worked-example",
    "title": "Hypothesis testing: critical values",
    "section": "\n3 Worked Example",
    "text": "3 Worked Example\nThe Procrastination Assessment Scale for Students (PASS) was designed to assess how individuals approach decision situations, specifically the tendency of individuals to postpone decisions (Solomon & Rothblum, 1984).\nThe PASS assesses the prevalence of procrastination in six areas: writing a paper; studying for an exam; keeping up with reading; administrative tasks; attending meetings; and performing general tasks. For a measure of total endorsement of procrastination, responses to 18 questions (each measured on a 1-5 scale) are summed together, providing a single score for each participant (range 0 to 90). The mean score from Solomon & Rothblum, 1984 was 33.\n\nResearch question:\nDoes the mean procrastination score of Edinburgh University students differ from the Solomon & Rothblum average of 33?\n\nTo answer this question, we will use data collected for a random sample of students from the University of Edinburgh: https://uoepsy.github.io/data/pass_scores.csv\n\n\n\n\nVariable Name\nDescription\n\n\n\nsid\nSubject identifier\n\n\nschool\nSchool each subject belonged to\n\n\nPASS\nTotal endorsement of procrastination score\n\n\n\n\n\nNecessary packages:\n\n\n\ntidyverse for using read_csv(), using summarise() and ggplot().\npatchwork for arranging plots side by side or underneath\nkableExtra for creating user-friendly tables\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(kableExtra)\n\nRead the data into R:\n\npass_scores &lt;- read_csv(\"https://uoepsy.github.io/data/pass_scores.csv\")\ndim(pass_scores)\n\n[1] 20  3\n\n\nTo inspect the data:\n\n\nhead()\nglimpse()\nsummary()\n\n\n\n\nhead(pass_scores)\n\n# A tibble: 6 × 3\n  sid   school       PASS\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 s_1   GeoSciences    31\n2 s_2   ECA            24\n3 s_3   LAW            32\n4 s_4   ECA            40\n5 s_5   LAW            28\n6 s_6   SSPS           31\n\n\n\n\n\nglimpse(pass_scores)\n\nRows: 20\nColumns: 3\n$ sid    &lt;chr&gt; \"s_1\", \"s_2\", \"s_3\", \"s_4\", \"s_5\", \"s_6\", \"s_7\", \"s_8\", \"s_9\", …\n$ school &lt;chr&gt; \"GeoSciences\", \"ECA\", \"LAW\", \"ECA\", \"LAW\", \"SSPS\", \"PPLS\", \"SLL…\n$ PASS   &lt;dbl&gt; 31, 24, 32, 40, 28, 31, 30, 28, 32, 29, 28, 33, 35, 33, 30, 31,…\n\n\n\n\n\nsummary(pass_scores)\n\n     sid               school               PASS      \n Length:20          Length:20          Min.   :24.00  \n Class :character   Class :character   1st Qu.:28.75  \n Mode  :character   Mode  :character   Median :31.00  \n                                       Mean   :30.70  \n                                       3rd Qu.:32.00  \n                                       Max.   :40.00  \n\n\n\n\n\nVisualise the distribution of PASS scores:\n\n\nNote\nThe boxplot highlights an outlier (40). However, this value is well within the plausible range of the scale (0 – 90), hence it is of no concern and the point can be kept for the analysis.\n\nplt_hist &lt;- ggplot(pass_scores, aes(x = PASS)) + \n    geom_histogram(color = 'white')\n\nplt_box &lt;- ggplot(pass_scores, aes(x = PASS)) + \n    geom_boxplot()\n\nplt_hist / plt_box\n\n\n\n\nDescriptive statistics:\n\nstats &lt;- pass_scores %&gt;%\n    summarise(n = n(),\n              Min = min(PASS),\n              Max = max(PASS),\n              M = mean(PASS),\n              SD = sd(PASS))\n\n\nstats %&gt;%\n    kbl(booktabs = TRUE, digits = 2, \n        caption = \"Descriptive statistics for PASS scores\")\n\n\n\n\nDescriptive statistics for PASS scores\n\nn\nMin\nMax\nM\nSD\n\n\n20\n24\n40\n30.7\n3.31\n\n\n\n\nStep 1: Identify the null and alternative hypotheses.\nFirst we need to write the null and alternative hypothesis, which take the form \\(H_0 : \\mu = \\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\). From the research question, we identify the hypothesised value \\(\\mu_0\\) to be 33, hence:\n\n\nThese are written as:\n$$H_{0}: \\mu = 33$$\n$$H_{1}: \\mu \\neq 33$$\nIn H_{0} and H_{1} the 0 and 1 within curly braces are written as subscripts. The curly braces delimit what goes in the subscript. The symbol \\mu denotes the greek letter “mu” that stands for the population mean (a parameter). The symbol \\neq means not equal.\n\\[H_0: \\mu = 33\\] \\[H_1: \\mu \\neq 33\\]\nStep 2: Compute the t-statistic\nNext, we compute the t-statistics, which compares the difference between the sample and hypothesised mean (\\(\\bar{x} - \\mu_0\\)) to the variation due to random sampling (\\(SE_{\\bar{x}}\\)).\nTo test the hypothesis, we need to compute the t-statistic,\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{SE_{\\bar{x}}} \\qquad \\text{where} \\qquad SE_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\n\\]\n\n# Sample mean\nxbar &lt;- stats$M\n\n# Standard error\ns &lt;- stats$SD\nn &lt;- stats$n\nse &lt;- s / sqrt(n)\n\n# Observed t-statistic\ntobs &lt;- (xbar - 33) / se\ntobs\n\n[1] -3.107272\n\n\nStep 3: Identify the null distribution, i.e. the distribution of the t-statistic assuming the null to be true.\nAs the sample size is \\(n\\) = 20, if the null hypothesis is true the t-statistic will follow a t(19) distribution.\nStep 4: Compute the critical values for \\(\\alpha = 0.05\\)\nAs the alternative hypothesis is two-sided (or two-tailed), we have two critical values \\(\\pm t^*\\) that jointly cut an area of 0.05 beyond them. That area must be equally split in both tails, so 0.025 to the left and 0.025 to the right\n\ntstar &lt;- qt(c(0.025, 0.975), df = n - 1)\ntstar\n\n[1] -2.093024  2.093024\n\n\nHence:\n\n\n\\(-t^*\\) = -2.093024\n\n\\(+t^*\\) = +2.093024\n\nStep 5: Make a decision\nCompare the observe statistic to the critical values:\n\ntobs\n\n[1] -3.107272\n\ntstar\n\n[1] -2.093024  2.093024\n\n\nIs the observed t-statistic within the critical values (hence we do not reject) or beyond (hence we reject)?\n\ntobs &lt;= tstar[1]\n\n[1] TRUE\n\ntobs &gt;= tstar[2]\n\n[1] FALSE\n\n\nUsing the critical-value method, we compare the observed t-statistic, -3.11, with the critical values for the appropriate significance level, -2.09 and 2.09. As -3.11 is not within the two critical values, we reject \\(H_0\\). In fact, -3.11 is smaller than the lower critical value -2.09.\nA potential way to report the t-test results:\n\n\n\n\n\n\nExample hypothesis test write-up\n\n\n\nAt the 5% significance level, the sample data provide significant evidence against the null hypothesis and in favour of the alternative one that the mean procrastination score of Edinburgh University students is different from the Solomon & Rothblum reported average of 33: \\(t(19) = -3.11, p &lt; .05\\), two-sided.\n\n\n\n\n\n\n\n\n\nImportant: Difference between \\(t\\) and \\(\\pm t^*\\)\n\n\n\nNote the difference between \\(\\pm t^*\\) and the t-statistic \\(t\\):\n\nThe quantiles or critical values are \\(-t^*\\) and \\(+t^*\\). These are computed with qt(..., df = ...). In the quantiles, the superscript \\(*\\) is not a multiplication sign!\nThe t-statistic \\(t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\\) computes how many standard errors away from the hypothesised value the sample mean is."
  },
  {
    "objectID": "2_03_ht_critvalues.html#student-glossary",
    "href": "2_03_ht_critvalues.html#student-glossary",
    "title": "Hypothesis testing: critical values",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\ngeom_histogram\n?\n\n\ngeom_boxplot\n?\n\n\nsummarise\n?\n\n\nn()\n?\n\n\nmean\n?\n\n\nsd\n?\n\n\nabs\n?\n\n\nqt\n?"
  },
  {
    "objectID": "2_03_ht_critvalues.html#footnotes",
    "href": "2_03_ht_critvalues.html#footnotes",
    "title": "Hypothesis testing: critical values",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: Ask last week’s driver for the Rmd file, they should share it with the group via email or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nHint: Identify the hypothesised value of the mean, \\(\\mu_0\\), and replace that value in the following equations:\n\\[H_{0} : \\mu = \\mu_{0}\\] \\[H_{1} : \\mu \\neq \\mu_{0}\\]\nThe equations can be written with the following code, where the text within curly braces after an underscore is rendered as a subscript:\n$$ H_{0} : \\mu = \\mu_{0} $$\n$$ H_{1} : \\mu \\neq \\mu_{0} $$\n↩︎\n\n\nHint: Use the following formula for the t-statistic:\n\\[t = \\frac{\\bar{x} - \\mu_0}{SE_{\\bar{x}}} \\qquad \\text{where} \\qquad SE_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\]\nIn the above:\n\n\n\\(\\bar{x}\\) is the sample mean\n\n\\(\\mu_0\\) is the hypothesised value for the population mean\n\n\\(s\\) is the sample standard deviation\n\n\\(n\\) is the sample size\n\n↩︎\n\nHint: the t-statistic follows a \\(t(n-1)\\) distribution where \\(n-1\\) is the degrees of freedom.\nThe question asks you: what are the degrees of freedom to use in the t distribution?↩︎\nHint: The qt() function may be useful↩︎"
  },
  {
    "objectID": "2_05_hterrorspower.html",
    "href": "2_05_hterrorspower.html",
    "title": "Errors, Power, Effect size, Assumptions",
    "section": "",
    "text": "Submission of Formative Report C (PDF file only)\n\n\n\n\n\n\nYou are required to submit a PDF file by 12 noon on Friday the 17th of February 2023 via Learn. One person needs to submit on behalf of your group.\nNo extensions allowed. As this is group-based work, no extensions are possible.\n\nThe report should be at most 6 pages long. At the end of the report, you are allowed two appendices which don’t count towards the page limit.\n\nAppendix A will contain any tables or figures which you cannot fit in the page limit (no text allowed).\nAppendix B will contain the code to reproduce the report results.\n\n\nExcluding Appendix B, the report should only include text, figures or tables. It should not include any reference to R code or functions, but be written for a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\nThe report title should be “Formative Report C (Group 0.A)”. Replace Group 0.A to be your group name.\n\nIn the author section of the PDF file write the exam number of each person within the group. The exam number starts with the letter B and can be found on your student ID card.\n\nFor example: B000001, B000002, B000003, B000004\n\n\nYou will receive formative feedback on your submission during Flexible Learning Week (next week). Please keep going to the labs next week in order to receive feedback."
  },
  {
    "objectID": "2_05_hterrorspower.html#tasks",
    "href": "2_05_hterrorspower.html#tasks",
    "title": "Errors, Power, Effect size, Assumptions",
    "section": "\n1 Tasks",
    "text": "1 Tasks\nThe data dataset-ipeds-2012-subset2, available at https://uoepsy.github.io/data/dataset-ipeds-2012-subset2.csv, is a subset of data derived from the Integrated Postsecondary Education Data System (IPEDS) at the National Center for Education Statistics, 2012. The data were collected for a random sample from all colleges and universities in the United States in that year. The variables include:\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\ntype\nCollege Type:−1 = Not reported;\n1 = Public;\n2 = Private for-profit;\n3 = Private not-for-profit (no religious affiliation);\n4 = Private not-for-profit (religious affiliation)\n\n\n\nregion\nRegion:0 = US Service schools;\n1 = New England;\n2 = Mid East;\n3 = Great Lakes;\n4 = Plains;\n5 = Southeast;\n6 = Southwest;\n7 = Rocky Mountains;\n8 = Far West;\n9 = Outlying areas\n\n\n\ngradrate\nGraduation Rate – All\n\n\ngradratem\nGraduation Rate – Men\n\n\ngradratew\nGraduation Rate – Women\n\n\n\n\n\nIn formative report C, you will investigate the mean graduation rate for female students at colleges and universities in the United States. Specifically, you are asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task C5.\nC1) Read the data into R, describe the variable of interest both visually and numerically, and provide a 95% CI for the mean graduation rate of female students at colleges and universities in the United States.\nC2) At the 5% significance level and using the p-value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\nC3) At the 5% significance level and using the critical value method, test whether the mean graduation rate for female students at colleges and universities in the United States is significantly different from a rate of 50 percent.\nC4) Tidy up your report so far, making sure to have 3 sections: introduction, analysis and discussion.\n\n\n\n\n\n\nThis week’s task\n\n\n\nC5) Compute and report the effect size, check if the assumptions underlying the t-test are violated."
  },
  {
    "objectID": "2_05_hterrorspower.html#c5-sub-tasks",
    "href": "2_05_hterrorspower.html#c5-sub-tasks",
    "title": "Errors, Power, Effect size, Assumptions",
    "section": "\n2 C5 sub-tasks",
    "text": "2 C5 sub-tasks\nIn this section you will find some guided sub-steps you may want to consider to complete task C5.\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\nCompute an effect size for the graduation rate of female students.2\n\n\n\nAdd a write-up of the effect size computed above to your report. In other words, after reporting whether the t-test results are statistically significant, are your results also of practical significance (i.e. important)?\nCheck and report whether the t-test assumptions are satisfied.3\n\n\nAdd a write-up of the assumptions checks to your report.\nKnit the report to PDF and submit it via LEARN."
  },
  {
    "objectID": "2_05_hterrorspower.html#worked-example",
    "href": "2_05_hterrorspower.html#worked-example",
    "title": "Errors, Power, Effect size, Assumptions",
    "section": "\n3 Worked Example",
    "text": "3 Worked Example\nThe R code is visible for instructional purposes only, but it should not be visible in a PDF report. No R code or output should be visible in a report, only text, figures, and tables.\n\n\n\n\n\n\nR code\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(kableExtra)\n\npass_scores &lt;- read_csv(\"https://uoepsy.github.io/data/pass_scores.csv\")\ndim(pass_scores)\nhead(pass_scores)\nglimpse(pass_scores)\n\nsummary(pass_scores)\n\nplt_hist &lt;- ggplot(pass_scores, aes(x = PASS)) + \n    geom_histogram(color = 'white') +\n    labs(x = \"PASS scores\", title = \"(a) Histogram\")\n\nplt_box &lt;- ggplot(pass_scores, aes(x = PASS)) + \n    geom_boxplot() +\n    labs(x = \"PASS scores\", title = \"(b) Boxplot\")\n\nplt_hist / plt_box\nstats &lt;- pass_scores %&gt;%\n    summarise(n = n(),\n              Min = min(PASS),\n              Max = max(PASS),\n              M = mean(PASS),\n              SD = sd(PASS))\n\nkbl(stats, booktabs = TRUE, digits = 2, \n    caption = \"Descriptive statistics for PASS scores\")\n\n# Confidence interval\nxbar &lt;- stats$M\ns &lt;- stats$SD\nn &lt;- stats$n\nse &lt;- s / sqrt(n)\ntstar &lt;- qt(c(0.025, 0.975), df = n - 1)\n\nxbar + tstar * se\n\n# observed t-statistic\ntobs &lt;- (xbar - 33) / se\ntobs\n\n# p-value method\npvalue &lt;- 2 * pt(abs(tobs), df = n - 1, lower.tail = FALSE)\npvalue\n\n# critical values method\ntstar\ntobs\n\n# effect size\nD &lt;- (xbar - 33) / s\nD\n\n# assumptions checks\ndim(pass_scores)\n\nplt_dens &lt;- ggplot(pass_scores, aes(x = PASS)) + \n    geom_density() +\n    labs(x = \"PASS scores\",\n         title = \"(a) Density plot\")\n\nplt_qq &lt;- ggplot(pass_scores, aes(sample = PASS)) + \n    geom_qq() +\n    geom_qq_line() +\n    labs(x = \"Theoretical quantiles\",\n         y = \"Sample quantiles\",\n         title = \"(b) QQ-plot\")\n\nplt_dens | plt_qq\nshapiro.test(pass_scores$PASS)\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\nA random sample of 20 students from the University of Edinburgh completed a questionnaire measuring their total endorsement of procrastination. The data, available from https://uoepsy.github.io/data/pass_scores.csv, were used to estimate the average procrastination score of all Edinburgh University students, as well as testing whether the mean procrastination score differed from the Solomon & Rothblum reported average of 33 at the 5% significance level. The recorded variables include a subject identifier (sid, categorical), the school each belongs to (school, categorical), and the total score on the Procrastination Assessment Scale for Students (PASS, numeric). The data did not include any impossible values for the PASS scores, as they were all within the possible range of 0 – 90. To answer the questions of interest, we only focused on the total PASS score variable.\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\n\nThroughout the report we used a significance level \\(\\alpha\\) of 5%.\nThe distribution of PASS scores, as shown in Figure 1(a), is roughly bell shaped and does not have any impossible values. The outlier (40) depicted in the boxplot shown in Figure 1(b) is well within the range of plausible values for the PASS scale (0–90) and as such was not removed for the analysis.\n\n\n\n\nFigure 1: Distribution of PASS scores for a sample of Edinburgh University students\n\n\n\n\n\n\n\nTable 1: Descriptive statistics for PASS scores\n\nn\nMin\nMax\nM\nSD\n\n\n20\n24\n40\n30.7\n3.31\n\n\n\n\n\n\n\nTable 1 displays summary statistics for the PASS scores in the sample of Edinburgh University students. From the sample data we obtain an average procrastination score of \\(M = 30.7\\), 95% CI [29.15, 32.25]. Hence, we are 95% confident that a Edinburgh University student will have a procrastination score between 29.15 and 32.25, which is between 0.75 and 3.85 lower than the average score of 33 reported by Solomon & Rothblum.\nTo investigate whether the mean PASS scores of all Edinburgh University students, \\(\\mu\\) say, differs from the Solomon & Rothblum reported average of 33, we performed a one sample t-test of \\(H_0 : \\mu = 33\\) against \\(H_1 : \\mu \\neq 33\\). The sample data provided very strong evidence against the null hypothesis and in favour of the alternative one that the mean procrastination score of Edinburgh University students is significantly different from the Solomon & Rothblum reported average of 33: \\(t(19) = -3.11, p = .006\\), two-sided. The size of the effect was also found to be medium to large \\((D = -0.69)\\).\n\n\n\n\nFigure 2: Density plot (a) and QQ-plot (b) of PASS scores for a sample of Edinburgh University students\n\n\n\nThe sample data did not show violations of the assumptions required for the t-test results to be valid. Specifically, the data were collected on a random sample of students from Edinburgh University, hence independence was met. Figure 2(a) shows that the distribution of PASS scores is roughly bell-shaped, with a single mode and as such does not raise any concerns of violations of normality. Similarly, the QQ-plot in Figure 2(b) shows agreement between the sample and theoretical quantiles, as they almost all fall on the line. We also performed a Shapiro-Wilk test against the null hypothesis of normality of the population data: \\(W = 0.94\\), \\(p = .20\\). The sample data did not provide sufficient evidence at the 5% level to reject the null hypothesis that the population data follow a normal distribution.\n\n\n\n\n\n\n\n\n\nDiscussion\n\n\n\n\n\nData including the Procrastination Assessment Scale for Students (PASS) scores for a random sample of 20 students at Edinburgh University were used to estimate the average procrastination score for a student of that university. In addition, the data were used to test whether there is a significant difference between that average score and the Solomon & Rothblum reported average of 33.\nThe data provided very strong evidence that the mean procrastination score of Edinburgh University students differs from 33. Furthermore, the data indicate that a Edinburgh University student tends to have a mean procrastination score between 29.15 and 32.35, which is 0.75 and 3.85 lower than the Solomon & Rothblum reported average of 33.\n\n\n\nWhat is missing from this instructional example:\n\nAppendix A\nAppendix B"
  },
  {
    "objectID": "2_05_hterrorspower.html#footnotes",
    "href": "2_05_hterrorspower.html#footnotes",
    "title": "Errors, Power, Effect size, Assumptions",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: Ask last week’s driver for the Rmd file, they should share it with the group via email or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nHint: Cohen’s \\(D\\) for a one sample t-test is given by\n\\[D = \\frac{\\bar{x} - \\mu_0}{s}\\]\nwhere \\(\\bar{x}\\) is the sample mean, \\(\\mu_0\\) is the value for the population mean that we hypothesised in the null hypothesis (50), and \\(s\\) is the sample standard deviation.↩︎\n\n\nHint: Recall that, for the t-test results to be valid, conditions (1) and (2) below need to be met:\n\nThe data need to come from a random sample of the population\nEither one of these holds:\n\nThe population follows a normal distribution (irrespectively of the sample size)\nThe sample size is large enough, \\(n \\geq 30\\) as a guideline.\n\n\n\nFor (1), this is known from the study design description. For (2) some of these functions may be useful: dim, nrow, geom_qq, geom_qq_line, shapiro.test.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The lab materials were created by:\n\nDr Umberto Noe\nDr Josiah King\nDr Emma Waterston\nDepartment of Psychology, The University of Edinburgh\n\n\n\n\n Back to top"
  },
  {
    "objectID": "rd1_01.html",
    "href": "rd1_01.html",
    "title": "Getting started with R and RStudio",
    "section": "",
    "text": "Recommended option: access it via the RStudio server. Try these steps first to register for RStudio server online:\n\nLog in to EASE using your university UUN and password.\nSet your RStudio password here, the username will be the same as your UUN (make sure you type your UUN correctly).\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and the password you set above in (2).\n\nIf the steps above did not work:\n\nPlease complete this form and wait for an email. Please note that this can take up to four working days.\n\nOnce you receive an email from us, please follow the following instructions:\n\nSet your here, the username will be the same as your UUN (make sure you type your UUN correctly).\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and the password you just set above.\n\n\n\nAlternative: install it locally on your PC following these instructions.\n\n\nCreate a new folder on the server. Give it a useful name like the name of the course: DAPR1. (In the picture we have used “USMR” but please use “DAPR1”).\n\n\n\nIn the top right, click Project &gt; New Project\n\n\n\nClick “Existing Directory”\n\n\n\nClick “Browse” and then choose the folder you just created. Click “Choose” and then click “Create Project”.\n\n\n\nYou should now be able to tell that you have the project open, because it shows you in the top right.\n\n\nYou’re ready to go!"
  },
  {
    "objectID": "rd1_01.html#getting-r-and-rstudio",
    "href": "rd1_01.html#getting-r-and-rstudio",
    "title": "Getting started with R and RStudio",
    "section": "",
    "text": "Recommended option: access it via the RStudio server. Try these steps first to register for RStudio server online:\n\nLog in to EASE using your university UUN and password.\nSet your RStudio password here, the username will be the same as your UUN (make sure you type your UUN correctly).\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and the password you set above in (2).\n\nIf the steps above did not work:\n\nPlease complete this form and wait for an email. Please note that this can take up to four working days.\n\nOnce you receive an email from us, please follow the following instructions:\n\nSet your here, the username will be the same as your UUN (make sure you type your UUN correctly).\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and the password you just set above.\n\n\n\nAlternative: install it locally on your PC following these instructions.\n\n\nCreate a new folder on the server. Give it a useful name like the name of the course: DAPR1. (In the picture we have used “USMR” but please use “DAPR1”).\n\n\n\nIn the top right, click Project &gt; New Project\n\n\n\nClick “Existing Directory”\n\n\n\nClick “Browse” and then choose the folder you just created. Click “Choose” and then click “Create Project”.\n\n\n\nYou should now be able to tell that you have the project open, because it shows you in the top right.\n\n\nYou’re ready to go!"
  },
  {
    "objectID": "rd1_01.html#a-first-look-at-rstudio",
    "href": "rd1_01.html#a-first-look-at-rstudio",
    "title": "Getting started with R and RStudio",
    "section": "\n2 A first look at RStudio",
    "text": "2 A first look at RStudio\nOkay, now you should have RStudio and a project open, and you should see something which looks more or less like the image below, where there are several little windows.\n\nWe are going to explore what each of these little windows offer by just diving in and starting to do things.\n\n2.1 R as a calculator\nStarting in the left-hand window, you’ll notice the blue sign &gt; which is where we R code gets executed.\nType 2+2, and hit Enter ↵. You should discover that R is a calculator.\nLet’s work through some of the basic operations (adding, subtracting, etc). Try these commands yourself:\n\n2 + 5\n10 - 4\n2 * 5\n10 - (2 * 5)\n(10 - 2) * 5\n10 / 2\n\n3^2 (Hint, interpret the ^ symbol as “to the power of”)\n\n\n\n\n\n\n\nWhenever you see the blue sign &gt;, it means R is ready and waiting for you to provide a command.\nIf you type 10 + and press Enter, you’ll see that instead of a blue &gt; you are left with a blue +. This means that R is waiting for more. Either give it more, or cancel the command by pressing the escape key on your keyboard.\n\n\n\nAs well as performing calculations, we can ask R things, such as “Is 3 less than 5?”:\n\n3 &lt; 5\n\n[1] TRUE\n\n\nAs the computation above returns TRUE, we notice that such questions return either TRUE or FALSE. These are not numbers and are called logical values.\nTry the following:\n\n\n3 &gt; 5 “is 3 greater than 5?”\n\n3 &lt;= 5 “is 3 less than or equal to 5?”\n\n3 &gt;= 3 “is 3 greater than or equal to 3?”\n\n3 == 5 “is 3 equal to 5?”\n\n(2 * 5) == 10 “is 2 times 5 equal to 10?”\n\n(2 * 5) != 11 “is 2 times 5 NOT equal to 11?”\n\n2.2 R as a calculator with a memory\nWe can also store things in R’s memory, and to do that we just need to give them a name. Type x &lt;- 5 and press Enter.\nWhat has happened? We’ve just stored something named x which has the value 5. We can now refer to the name and it will give us the value! Try typing x and hitting Enter. It should give you the number 5. What about x * 3?\n:::{.callout-info} #### Storing things in R\nThe &lt;- symbol, pronounced arrow, is used to assign a value to a named object:\n[name] &lt;- [value]\nNote, there are a few rules about names in R:\n\nNo spaces - spaces inside a name are not allowed (the spaces around the &lt;- don’t matter):\n\n\nlucky_number &lt;- 5 ✔   lucky number &lt;- 5 ❌\n\n\n\nNames must start with a letter:\n\n\nlucky_number &lt;- 5 ✔   1lucky_number &lt;- 5 ❌\n\n\n\nCase sensitive:\n\n\nlucky_number is different from Lucky_Number\n\n\n\nReserved words - there is a set of words you can’t use as names, including: if, else, for, in, TRUE, FALSE, NULL, NA, NaN, function\n(Don’t worry about remembering these, R will tell you if you make the mistake of trying to name a variable after one of these).\n\nYou might have noticed that something else happened when you executed the code x &lt;- 5. The thing we named x with a value of 5 suddenly appeared in the top-right window. This is known as the environment, and it shows everything that we store things in R:\n\nWe’ve now used a couple of the windows - we’ve been executing R code in the console, and learned about how we can store things in R’s memory (the environment) by assigning a name to them:\n\nNotice that in the screenshot above, we have moved the console down to the bottom-left, and introduced a new window above it. This is the one that we’re going to talk about next."
  },
  {
    "objectID": "rd1_01.html#r-scripts-and-rmarkdown",
    "href": "rd1_01.html#r-scripts-and-rmarkdown",
    "title": "Getting started with R and RStudio",
    "section": "\n3 R scripts and Rmarkdown",
    "text": "3 R scripts and Rmarkdown\nWhat if we want to edit our code? Whatever we write in the console just disappears upwards. What if we want to change things we did earlier on?\nWell, we can write and edit our code in a separate place before sending it to the console to be executed!!\n\n3.1 R scripts\n\n\n\n\n\n\nTask\n\n\n\n\nOpen an R script\n\nFile &gt; New File &gt; R script\n\n\nCopy and paste the following into the R script\n\n\nx &lt;- 210\ny &lt;- 15\nx / y\n\n\nPosition your text-cursor (blinking vertical line) on the top line and press:\n\nCtrl + Enter on Windows\nCmd + Enter on macOS\n\n\n\n\n\nNotice what has happened - it has sent the command x &lt;- 210 to the console, where it has been executed, and x is now in your environment. Additionally, it has moved the text-cursor to the next line.\n\n\n\n\n\n\n\nTask\n\n\n\nPress Ctrl + Enter (Windows) or Cmd + Enter (macOS) again. Do it twice (this will run the next two lines).\nThen, change x to some other number in your R script, and run the lines again (starting at the top).\n\n\n\n\n\n\n\n\nTask\n\n\n\nAdd the following line to your R script and execute it (send it to the console pressing Ctrl/Cmd + Enter):\n\nplot(1,5)\n\n\n\nA very basic plot should have appeared in the bottom-right of RStudio. The bottom-right window actually does some other useful things.\n\n\n\n\n\n\nTask\n\n\n\n\nSave the R script you have been working with:\n\nFile &gt; Save\ngive it an appropriate name, and click save.\n\n\nCheck that you can now see that file in the file pane, by clicking on the “Files” tab of the bottom-right window.\n\n\n\nNOTE: When you save R script files, they terminate with a .R extension.\n\n3.2 Rmarkdown\n\n\n\n\nArtwork by @allison_horst\n\n\n\nIn addition to R scripts, there is another type of document we can create, known as an “Rmarkdown”.\nRmarkdown documents combine the analytical power of R and the utility of a text-processor. We can have one document which contains all of our analysis as well as our written text, and can be compiled into a nicely formatted report. This saves us doing analysis in R and copying results across to Microsoft Word. It ensures our report accurately reflects our analysis. Everything that you’re reading now has all been written in Rmarkdown!\nWe’re going to use Rmarkdown documents throughout this course. We’ll get into it how to write them lower down, but it basically involves writing normal text interspersed with “code-chunks” (i.e., chunks of code!). In the example below, you can see the grey boxes indicating the R code, with text in between. We can then compile the document into either a .pdf or a .html file."
  },
  {
    "objectID": "rd1_01.html#recap",
    "href": "rd1_01.html#recap",
    "title": "Getting started with R and RStudio",
    "section": "\n4 Recap",
    "text": "4 Recap\nOkay, so we’ve now seen all of the different windows in RStudio in action:\n\nThe console is where R code gets executed\nThe environment is R’s memory, you can assign something a name and store it here, and then refer to it by name in your code.\nThe editor is where you can write and edit R code and Rmarkdown documents. You can then send this to the console for it to be executed.\nThe bottom-right window shows you the plots that you create, the files in your project, and some other things (we’ll get to these later)."
  },
  {
    "objectID": "rd1_01.html#take-a-breather",
    "href": "rd1_01.html#take-a-breather",
    "title": "Getting started with R and RStudio",
    "section": "\n5 Take a breather",
    "text": "5 Take a breather\nBelow are a couple of our recommended settings for you to change as you begin your journey in R. After you’ve changed them, take a 5 minute break before moving on to learning about how we store data in R.\n\n\n\n\n\n\nUseful Settings 1: Clean environments\nAs you use R more, you will store lots of things with different names. Throughout this course alone, you’ll probably name hundreds of different things. This could quickly get messy within our project.\nWe can make it so that we have a clean environment each time you open RStudio. This will be really handy.\n\nIn the top menu, click Tools &gt; Global Options…\n\nThen, untick the box for “Restore .RData into workspace at startup”, and change “Save workspace to .RData on exit” to Never:\n\n\n\n\n\n\n\n\n\n\n\nUseful Settings 2: Wrapping code\nIn the editor, you might end up with a line of code which is really long, but you can make RStudio ‘wrap’ the line, so that you can see it all, without having to scroll:\n\nx &lt;- 1+2+3+6+3+45+8467+356+8565+34+34+657+6756+456+456+54+3+78+3+3476+8+4+67+456+567+3+34575+45+2+6+9+5+6\n\n\nIn the top menu, click Tools &gt; Global Options…\n\nIn the left menu of the box, click “Code”\n\nTick the box for “Soft-wrap R source files”"
  },
  {
    "objectID": "rd1_01.html#r-packages",
    "href": "rd1_01.html#r-packages",
    "title": "Getting started with R and RStudio",
    "section": "\n6 R Packages",
    "text": "6 R Packages\n\n6.1 Installing R packages\nAlongside the basic installation of R and RStudio, there are many add-on packages which the R community create and maintain.\nThe thousands of packages are part of what makes R such a powerful and useful tool - there is a package for almost everything you could want to do in R.\n\n\n\n\n\n\nTask\n\n\n\nIn the console, type install.packages(\"cowsay\") and hit Enter.\nLots of red text will come up, and it will take a bit of time.\nWhen it has finished, and R is ready for you to use again, you will see the blue sign &gt;.\n\n\n\n6.2 Using R packages\nIt’s not enough just to install a package - to actually use the package, we need to load it using library().\nWe install a package only once. But each time we open RStudio, we have to load the packages we need.\n\n\n\n\n\n\n\nTask\n\n\n\nIn the console again, type library(cowsay) and hit enter. This loads the package for us to use it.\nThen, type say(\"hello world\", by = \"cow\") and hit enter.\nHopefully you got a similar result to ours:\n\nlibrary(cowsay)\nsay(\"Hi Folks!\", by = \"cow\")\n\n\n ----- \nHi Folks! \n ------ \n    \\   ^__^ \n     \\  (oo)\\ ________ \n        (__)\\         )\\ /\\ \n             ||------w|\n             ||      ||"
  },
  {
    "objectID": "rd1_01.html#your-first-.rmd-file",
    "href": "rd1_01.html#your-first-.rmd-file",
    "title": "Getting started with R and RStudio",
    "section": "\n7 Your first .Rmd file",
    "text": "7 Your first .Rmd file\nIn order to be able to write and compile Rmarkdown documents (and do a whole load of other things which we are going to need throughout the course) we are now going to install a set of packages known collectively as the “tidyverse” (this includes the “rmarkdown” package).\n\n\nIf you installed R/Rstudio on your own computer, then in the console, type install.packages(\"tidyverse\") and hit Enter. You may have to wait a while.\n\nIf you are using rstudio.ppls.ed.ac.uk, then we have already installed “tidyverse” and a few other useful packages for you, so you don’t have to do anything!\n\n\n\n\n\n\n\nTask\n\n\n\nOpen a new Rmarkdown document: File &gt; New File &gt; R Markdown…\nWhen the box pops-up, give a title of your choice (“Intro lab”, maybe?) and your name as the author.\n\n\n\n7.1 Writing code in a .Rmd file\nThe file which you have just created will have some template stuff in it. Delete everything below the first code chunk to start with a fresh document:\n\n\n\n\n\n\n\nTask\n\n\n\nInsert a new code chunk by either using the Insert button in the top right of the document and selecting R, or by typing Ctrl + Alt + i on Windows or Option + Cmd + i on MacOS.\nInside the chunk, type:\nprint(\"Hello world! My name is ?\")\nTo execute the code inside the chunk, you can either:\n\ndo as you did in the R script - put the text-cursor on the first line, and hit Ctrl/Cmd + Enter to run the lines sequentially;\nclick the little green arrow at the top right of your code-chunk to run all of the code inside the chunk;\nwhile your cursor is inside the code chunk, press Cmd + Shift + Enter to run all of the code inside the chunk.\n\nYou can see that the output gets printed below.\n\n\n\nWe’re going to use some functions which are in the tidyverse package, which we already installed above (or which we installed for you on the server).\nTo use the package, we need to load it.\nWhen writing analysis code, we want it to be reproducible - we want to be able to give somebody else our code and the data, and ensure that they can get the same results. To do this, we need to show what packages we use.\nIt is good practice to load any packages you use at the top of your code, so that users of your code will know what packages they will need to install to run your code.\nIn your first code chunk, type:\n\n# I'm going to use these packages in this document:\nlibrary(tidyverse)\n\nand run the chunk.\nNOTE: You might get various messages popping up below when you run this chunk, that is fine.\nComments in code\nNote that using # in R code makes that line a comment, which basically means that R will ignore the line. Comments are useful for you to remind yourself of what your code is doing.\n\n7.2 Writing text in a .Rmd file\nPlace your cursor outside the code chunk, and below the code chunk add a new line with the following:\n# R code examples\nNote that when the # is used in a Rmarkdown file outside of a code-chunk, it will make that line a heading when we finally get to compiling the document. Below, what you see on the left will be compiled to look like those on the right:\n\n\n7.2.1 RECALL:\n\n\nInside a code-chunk, one or more #s will create a comment\n\n\nOutside a code-chunk, one ore more #s will create headings\n\nIn your Rmarkdown document, choose a few of the symbols below, and write an explanation of what it does, giving an example in a code chunk. You can see an example of the first few below.\n\n+\n-\n*\n/\n()\n^\n&lt;-\n&lt;\n&gt;\n&lt;=\n&gt;=\n==\n!=\n\n\n\n7.3 Storing data into R\nWe’ve already seen how to assign a value to a name/symbol using &lt;-. However, we’ve only seen how to assign a single number, e.g, x &lt;- 5.\nTo store a sequence of numbers into R, we combine the values using the combine function c() and give the sequence a name. A sequence of elements all of the same type is called a vector. To view the stored content, simply type the name of the vector.\n\nmyfirstvector &lt;- c(1, 5, 3, 7)\nmyfirstvector\n\n[1] 1 5 3 7\n\n\nWe can perform arithmetic operations on each value of the vector. For example, to add five to each entry:\n\nmyfirstvector + 5\n\n[1]  6 10  8 12\n\n\nRecall that vectors are sequences of elements all of the same type. They do not have to be always numbers; they could be words such as real or fictional animals. Words need to be written inside quotations, e.g. “anything”, and instead of being of numeric type, we say they are characters.\n\nwordsvector &lt;- c(\"cat\", \"dog\", \"parrot\", \"peppapig\")\nwordsvector\n\n[1] \"cat\"      \"dog\"      \"parrot\"   \"peppapig\"\n\n\n\nNOTE\nYou can use either double-quote or single-quote:\n\nc(\"cat\", \"dog\", \"parrot\", \"peppapig\")\n\n[1] \"cat\"      \"dog\"      \"parrot\"   \"peppapig\"\n\nc('cat', 'dog', 'parrot', 'peppapig')\n\n[1] \"cat\"      \"dog\"      \"parrot\"   \"peppapig\"\n\n\n\nThe function class() will tell you the type of the object. In this case, it is a character vector.\n\nclass(wordsvector)\n\n[1] \"character\"\n\n\nIt does not make sense to add a number to words, hence some operations like addition and multiplication are only defined on vectors of numeric type. If you make a mistake, R will warn you with a red error message.\n\nwordsvector + 5\n\n\nError in wordsvector + 5 : non-numeric argument to binary operator\n\nFinally, it is important to notice that if you combine together in a vector a number and a word, R will transform all elements to be of the same type. Why? Recall: vectors are sequences of elements all of the same type. Typically, R chooses the most general type between the two. In this particular case, it would make everything a character, check the ““, as it would be harder to transform a word into a number!\n\nmysecondvector &lt;- c(4, \"cat\")\nmysecondvector\n\n[1] \"4\"   \"cat\"\n\n\n\n7.4 Reading data into R\nWhile we can manually input data like we did above, more often, we will need to read in data which has been created elsewhere (like in excel, or by some software which is used to present participants with experiments).\n\n\n\n\n\n\nTask\n\n\n\nAdd a new heading by typing the following:\n# Reading and storing data\nRemember: We make headings using the # outside of a code chunk.\n\n\n\n\n\n\n\n\nTask\n\n\n\nOpen Microsoft Excel, or LibreOffice Calc, or whatever spreadsheet software you have available to you, and create some data with more than one variable.\nIt can be whatever you want, but we’ve used a very small example here for you to follow, so feel free to use it if you like.\nWe’ve got two sets of values here: the names and the birth-years of each member of the Beatles. The easiest way to think of this would be to have a row for each Beatle, and a column for each of name and birth-year.\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nSave the data as a .csv file.\nAlthough R can read data when it’s saved in Microsoft/LibreOffice formats, the simplest, and most universal way to save data is as simple text, with the values separated by some character - .csv stands for comma separated values.\nIn Microsoft Excel, if you go to File &gt; Save as\nIn the Save as Type box, choose to save the file as CSV (Comma delimited).\nImportant: save your data in the project folder you created at the start of this lab.\n\n\nBack in RStudio…\nNext, we’re going to read the data into R. We can do this by using the read_csv() function, and directing it to the file you just saved.\nIf you are using RStudio on the server, you will need to upload the file you just saved to the server. The video below shows an example of this:\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nCreate a new code-chunk in your Rmarkdown and, in the chunk, type: read_csv(\"name-of-your-data.csv\"), where you replace name-of-your-data with whatever you just saved your data as in your spreadsheet software.\n\n\nHelpful tip\nIf you have your text-cursor inside the quotation marks, and press the tab key on your keyboard, it will show you the files inside your project. You can then use the arrow keys to choose between them and press Enter to add the code:\n\n\n\n\n\n\nWhen you run the line of code you just wrote, it will print out the data, but will not store it. To do that, we need to assign it as something:\n\nbeatles &lt;- read_csv(\"data_from_excel.csv\")\n\nNote that this will now turn up in the Environment pane of RStudio.\nNow that we’ve got our data in R, we can print it out by simply invoking its name:\n\nbeatles\n\n# A tibble: 4 × 2\n  name   birth_year\n  &lt;chr&gt;       &lt;dbl&gt;\n1 John         1940\n2 Paul         1942\n3 George       1943\n4 Ringo        1940\n\n\nAnd we can do things such as ask R how many rows and columns there are:\n\ndim(beatles)\n\n[1] 4 2\n\n\nThis says that there are 4 members of the Beatles, and for each we have 2 measurements.\nTo get more insight into what the data actually are, you can either use the structure str() function, or glimpse() function to get a glimpse at the data:\n\nstr(beatles)\n\nspc_tbl_ [4 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ name      : chr [1:4] \"John\" \"Paul\" \"George\" \"Ringo\"\n $ birth_year: num [1:4] 1940 1942 1943 1940\n - attr(*, \"spec\")=\n  .. cols(\n  ..   name = col_character(),\n  ..   birth_year = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nglimpse(beatles)\n\nRows: 4\nColumns: 2\n$ name       &lt;chr&gt; \"John\", \"Paul\", \"George\", \"Ringo\"\n$ birth_year &lt;dbl&gt; 1940, 1942, 1943, 1940\n\n\n\n\n\n\n\n\nTask\n\n\n\nUse dim() to confirm how many rows and columns are in your data.\nUse str() or glimpse() to take a look at the structure of the data. Don’t worry about the output of str() right now, we’ll pick up with this in the next chapter.\n\n\n\n7.5 Getting help in R\ndim(), str(), read_csv() are all functions.\nFunctions perform specific operations / transformations in computer programming.\nThey can have inputs and outputs. For example, dim() takes some data you have stored in R as its input, and gives the dimensions of the data as its output.\nIn R, functions come with help pages, where you can see information about the various inputs and outputs, and examples of how to use them.\nIn the console, type ?dim (or ?dim() will work too) and press Enter.\nThe bottom-right pane (where things like plots are also shown), should switch to the Help tab, and open the documentation page for the dim() function!\n\n\n\n\n\n\nWhy did we ask you to write this bit in the console, whereas previously we’ve been writing stuff in the RMarkdown document in the editor?\nWell, when writing an RMarkdown document, the aim at the end is to have a nice document which we can read. For instance, we can write statistical reports, journal papers, coursework reports etc, in Rmarkdown. But the reader doesn’t need to see that we’re looking up how to use some function - just like they don’t need to know that we might look up a word in the dictionary before using it.\n\n\n\n\n7.6 Compiling a .Rmd file\n\n\n\n\n\n\nTask\n\n\n\nBy now, you should have an Rmardkown document (.Rmd) with your answers to the tasks we’ve been through today.\nCompile the document by clicking on the Knit button at the top (it will ask you to save your document first). The little arrow to the right of the Knit button allows you to compile to either .pdf or .html."
  },
  {
    "objectID": "rd1_01.html#checklist-for-today",
    "href": "rd1_01.html#checklist-for-today",
    "title": "Getting started with R and RStudio",
    "section": "\n8 Checklist for today",
    "text": "8 Checklist for today\n\n\nEITHER:\n\nOption A: Get started with the PPLS RStudio Server   ✔\nOption B: Install R and RStudio   ✔\n\n\nStart a new project for the course   ✔\nChange a few RStudio settings (recommended)   ✔\nInstall some R packages (the “tidyverse”)   ✔\nCreate a new Rmarkdown document   ✔\nComplete today’s tasks and exercises on storing data in R   ✔\nCompile your Rmarkdown document   ✔\nCelebrate!   ✔ 🎉"
  },
  {
    "objectID": "rd1_01.html#glossary",
    "href": "rd1_01.html#glossary",
    "title": "Getting started with R and RStudio",
    "section": "\n9 Glossary",
    "text": "9 Glossary\n\nConsole: where the code gets executed\nEnvironment: R’s memory, it lists all the names of things with stuff stored into them\nEditor: where we edit code\nR script: a file with R code and comments\nRmarkdown document: an enhanced file where you can combine together R code, explanatory text, and plots.\npackages (also library): user-created bundles providing additional functionality to your local R installation\nfunctions: they take inputs, do some transformation or computation on them, and return a result (output)\n?: returns the help page of a function, e.g. ?dim.\n\n\n\n\n\n\n\n\nSymbol\nDescription\nExample\n\n\n\n+\nAdds two numbers together\n\n2+2 - two plus two\n\n\n-\nSubtract one number from another\n\n3-1 - three minus one\n\n\n*\nMultiply two numbers together\n\n3*3 - three times three\n\n\n/\nDivide one number by another\n\n9/3 - nine divided by three\n\n\n()\ngroup operations together\n\n(2+2)/4 is different from 2+2/4\n\n\n\n^\nto the power of..\n\n4^2 - four to the power of two, or four squared\n\n\n&lt;-\nstores an object in R with the left hand side (LHS) as the name, and the RHS as the value\nx &lt;- 10\n\n\n=\nstores an object in R with the left hand side (LHS) as the name, and the RHS as the value\nx = 10\n\n\n&lt;\nis less than?\n2 &lt; 3\n\n\n&gt;\nis greater than?\n2 &gt; 3\n\n\n&lt;=\nis less than or equal to?\n2 &lt;= 3\n\n\n&gt;=\nis greater than or equal to?\n2 &gt;= 2\n\n\n==\nis equal to?\n(5+5) == 10\n\n\n!=\nis not equal to?\n(2+3) != 4\n\n\nc()\ncombines values into a vector (a sequence of values)\nc(1,2,3,4)"
  },
  {
    "objectID": "rd1_03.html",
    "href": "rd1_03.html",
    "title": "Numeric data",
    "section": "",
    "text": "In the previous week, we looked into describing and visualising categorical data. We looked at using the mode and median as measures of central tendency, before discussing how for ordered categorical data we could look at the quartiles (the points in rank-ordered data below which falls 25%, 50%, 75% and 100% of the data) to gain an understanding of how spread out the data are.\nWe now move to looking at measures of central tendency and of spread for numeric data."
  },
  {
    "objectID": "rd1_03.html#introduction",
    "href": "rd1_03.html#introduction",
    "title": "Numeric data",
    "section": "",
    "text": "In the previous week, we looked into describing and visualising categorical data. We looked at using the mode and median as measures of central tendency, before discussing how for ordered categorical data we could look at the quartiles (the points in rank-ordered data below which falls 25%, 50%, 75% and 100% of the data) to gain an understanding of how spread out the data are.\nWe now move to looking at measures of central tendency and of spread for numeric data."
  },
  {
    "objectID": "rd1_03.html#central-tendency",
    "href": "rd1_03.html#central-tendency",
    "title": "Numeric data",
    "section": "\n2 Central tendency",
    "text": "2 Central tendency\nIn the following examples, we are going to use some data on 120 participants’ IQ scores (measured on the Wechsler Adult Intelligence Scale (WAIS)), their ages, and their scores on 2 other tests.\nIt is available at https://uoepsy.github.io/data/wechsler.csv\n\nwechsler &lt;- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\nsummary(wechsler)\n\n participant              iq              age            test1      \n Length:120         Min.   : 58.00   Min.   :20.00   Min.   :30.00  \n Class :character   1st Qu.: 88.00   1st Qu.:29.50   1st Qu.:45.75  \n Mode  :character   Median :100.50   Median :37.00   Median :49.00  \n                    Mean   : 99.33   Mean   :36.63   Mean   :49.33  \n                    3rd Qu.:109.00   3rd Qu.:44.00   3rd Qu.:53.25  \n                    Max.   :137.00   Max.   :71.00   Max.   :67.00  \n     test2      \n Min.   : 2.00  \n 1st Qu.:42.00  \n Median :52.50  \n Mean   :51.24  \n 3rd Qu.:62.00  \n Max.   :80.00  \n\n\nMode and median revisited\nWe saw for categorical data two different measures of central tendency:\n\n\nMode: The most frequent value (the value that occurs the greatest number of times).\n\n\nMedian: The value for which 50% of observations a lower and 50% are higher. It is the mid-point of the values when they are rank-ordered.\n\nWe applied both of these to categorical data, but we can also use them for numeric data.\n\n\n\n\n\n\n\n\n\nMode\nMedian\nMean\n\n\n\nNominal (unordered categorical)\n✔\n✘\n✘\n\n\nOrdinal (ordered categorical)\n✔\n✔\n\n?(you may see it sometimes for certain types of ordinal data - there’s no consensus)\n\n\nNumeric Continuous\n✔\n✔\n✔\n\n\n\n The mode of numeric variables is not frequently used. Unlike categorical variables where there are a distinct set of possible values which the data can take, for numeric variables, data can take a many more (or infinitely many) different values. Finding the “most common” is sometimes not possible. The most frequent value (the mode) of the iq variable is 97:\n\n# take the \"wechsler\" dataframe %&gt;%\n# count() the values in the \"iq\" variable (creates an \"n\" column), and\n# from there, arrange() the data so that the \"n\" column is descending - desc()\nwechsler %&gt;%\n    count(iq) %&gt;%\n    arrange(desc(n))\n\n# A tibble: 53 × 2\n      iq     n\n   &lt;dbl&gt; &lt;int&gt;\n 1    97     7\n 2   108     6\n 3    92     5\n 4   103     5\n 5   105     5\n 6   110     5\n 7    85     4\n 8    99     4\n 9   107     4\n10   113     4\n# ℹ 43 more rows\n\n\nRecall that the median is found by ordering the data from lowest to highest, and finding the mid-point. In the wechsler dataset we have IQ scores for 120 participants. We find the median by ranking them from lowest to highest IQ, and finding the mid-point between the \\(60^{th}\\) and \\(61^{st}\\) participants’ scores.\nWe can also use the median() function with which we are already familiar:\n\nmedian(wechsler$iq)\n\n[1] 100.5\n\n\nMean\nOne of the most frequently used measures of central tendency for numeric data is the mean.\n\n\n\n\n\n\nMean: \\(\\bar{x}\\)\nThe mean is calculated by summing all of the observations together and then dividing by the total number of obervations (\\(n\\)).\nWhen we have sampled some data, we denote the mean of our sample with the symbol \\(\\bar{x}\\) (sometimes referred to as “x bar”). The equation for the mean is:\n\\[\\bar{x} = \\frac{\\sum\\limits_{i = 1}^{n}x_i}{n}\\]\n\n\n\n\n\n\nHelp reading mathematical formulae\n\n\n\n\n\nThis might be the first mathematical formula you have seen in a while, so let’s unpack it.\nThe \\(\\sum\\) symbol is used to denote a series of additions - a “summation”.\nWhen we include the bits around it: \\(\\sum\\limits_{i = 1}^{n}x_i\\) we are indicating that we add together all the terms \\(x_i\\) for values of \\(i\\) between \\(1\\) and \\(n\\): \\[\\sum\\limits_{i = 1}^{n}x_i \\qquad = \\qquad x_1+x_2+x_3+...+x_n\\]\nSo in order to calculate the mean, we do the summation (adding together) of all the values from the \\(1^{st}\\) to the \\(n^{th}\\) (where \\(n\\) is the total number of values), and we divide that by \\(n\\).\n\n\n\n\n\n\n\n\n\nSamples and populations\n\n\n\n\n\nStatistics is all about drawing inferences from some sampled data about the larger population from which it is sampled.\nA statistic which we calculate from our sample provides us with an estimate of something in the population (for instance, we might take the average age of students at Edinburgh University as an estimate of the age of all students).\nBecause of this, statisticians have different notations for when we are talking about populations vs talking about samples:\n\n\n\n\n\n\n\n\nSample\nPopulation\n\n\n\nNumber of observations\n\\(n\\)\n\\(N\\)\n\n\nMean\n\\(\\bar{x} = \\frac{\\sum\\limits_{i = 1}^{n}x_i}{n}\\)\n\\(\\mu = \\frac{\\sum\\limits_{i = 1}^{N}x_i}{N}\\)\n\n\n\n\n\n\n\n\n\nWe can do the calculation by summing the iq variable, and dividing by the number of observations (in our case we have 120 participants):\n\n# get the values in the \"iq\" variable from the \"wechsler\" dataframe, and\n# sum them all together. Then divide this by 120\nsum(wechsler$iq)/120\n\n[1] 99.33333\n\n\nOr, more easily, we can use the mean() function:\n\nmean(wechsler$iq)\n\n[1] 99.33333\n\n\nSummarising variables\nFunctions such as mean(), median(), min() and max() can quickly summarise data, and we can use them together really easily in combination with summarise().\n\n\n\n\n\n\nsummarise()\nThe summarise() function is used to reduce variables down to a single summary value.\n\n# take the data %&gt;%\n# summarise() it, such that there is a value called \"summary_value\", which\n# is the sum() of \"variable1\" column, and a value called \n# \"summary_value2\" which is the mean() of the \"variable2\" column.\ndata %&gt;%\n  summarise(\n    summary_value = sum(variable1),\n    summary_value2 = mean(variable2)\n  )\n\nNote: Just like with mutate() we don’t have to keep using the dollar sign $, as we have already told it what dataframe to look for the variables in.\n\n\n\nSo if we want to show the mean IQ score and the mean age of our participants:\n\n# take the \"wechsler\" dataframe %&gt;%\n# summarise() it, such that there is a value called \"mean_iq\", which\n# is the mean() of the \"iq\" variable, and a value called \n# \"mean_age\" which is the mean() of the \"age\" variable. \nwechsler %&gt;%\n    summarise(\n        mean_iq = mean(iq),\n        mean_age = mean(age)\n    )\n\n# A tibble: 1 × 2\n  mean_iq mean_age\n    &lt;dbl&gt;    &lt;dbl&gt;\n1    99.3     36.6"
  },
  {
    "objectID": "rd1_03.html#spread",
    "href": "rd1_03.html#spread",
    "title": "Numeric data",
    "section": "\n3 Spread",
    "text": "3 Spread\nInterquartile range\nIf we are using the median as our measure of central tendency and we want to discuss how spread out the spread are around it, then we will want to use quartiles (recall that these are linked: the \\(2^{nd}\\) quartile = the median).\nWe have already briefly introduced how for ordinal data, the 1st and 3rd quartiles give us information about how spread out the data are across the possible response categories. For numeric data, we can likewise find the 1st and 3rd quartiles in the same way - we rank-order all the data, and find the point at which 25% and 75% of the data falls below.\nThe difference between the 1st and 3rd quartiles is known as the interquartile range (IQR).( Note, we couldn’t take the difference for ordinal data, because “difference” would not be quantifiable - the categories are ordered, but intervals are between categories are unknown)\nIn R, we can find the IQR as follows:\n\nIQR(wechsler$age)\n\n[1] 14.5\n\n\nAlternatively, we can use this inside summarise():\n\n# take the \"wechsler\" dataframe %&gt;%\n# summarise() it, such that there is a value called \"median_age\", which\n# is the median() of the \"age\" variable, and a value called \"iqr_age\", which\n# is the IQR() of the \"age\" variable.\nwechsler %&gt;% \n  summarise(\n    median_age = median(age),\n    iqr_age = IQR(age)\n  )\n\n# A tibble: 1 × 2\n  median_age iqr_age\n       &lt;dbl&gt;   &lt;dbl&gt;\n1         37    14.5\n\n\nVariance\nIf we are using the mean as our as our measure of central tendency, we can think of the spread of the data in terms of the deviations (distances from each value to the mean).\nRecall that the mean is denoted by \\(\\bar{x}\\). If we use \\(x_i\\) to denote the \\(i^{th}\\) value of \\(x\\), then we can denote deviation for \\(x_i\\) as \\(x_i - \\bar{x}\\).\nThe deviations can be visualised by the red lines in Figure @ref(fig:deviations).\n\n\n\n\nDeviations from the mean\n\n\n\n\n\n\n\n\n\nThe sum of the deviations from the mean, \\(x_i - \\bar x\\), is always zero\n\\[\n\\sum\\limits_{i = 1}^{n} (x_i - \\bar{x}) = 0\n\\]\nThe mean is like a center of gravity - the sum of the positive deviations (where \\(x_i &gt; \\bar{x}\\)) is equal to the sum of the negative deviations (where \\(x_i &lt; \\bar{x}\\)).\n\n\n\nBecause deviations around the mean always sum to zero, in order to express how spread out the data are around the mean, we must we consider squared deviations.\nSquaring the deviations makes them all positive. Observations far away from the mean in either direction will have large, positive squared deviations. The average squared deviation is known as the variance, and denoted by \\(s^2\\)\n\n\n\n\n\n\nVariance: \\(s^2\\)\nThe variance is calculated as the average of the squared deviations from the mean.\nWhen we have sampled some data, we denote the mean of our sample with the symbol \\(\\bar{x}\\) (sometimes referred to as “x bar”). The equation for the variance is:\n\\[s^2 = \\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}\\]\n\n\n\n\n\n\nWhy n minus 1?\n\n\n\n\n\nThe top part of the equation \\(\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2\\) can be expressed in \\(n-1\\) terms, so we divide by \\(n-1\\) to get the average.Example: If we only have two observations \\(x_1\\) and \\(x_2\\), then we can write out the formula for variance in full quite easily. The top part of the equation would be: \\[\n\\sum\\limits_{i=1}^{2}(x_i - \\bar{x})^2 \\qquad = \\qquad (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2\n\\]\nThe mean for only two observations can be expressed as \\(\\bar{x} = \\frac{x_1 + x_2}{2}\\), so we can substitute this in to the formula above. \\[\n(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 \\qquad = \\qquad \\left(x_1 - \\frac{x_1 + x_2}{2}\\right)^2 + \\left(x_2 - \\frac{x_1 + x_2}{2}\\right)^2\n\\] Which simplifies down to one value: \\[\n\\left(x_1 - \\frac{x_1 + x_2}{2}\\right)^2 + \\left(x_2 - \\frac{x_1 + x_2}{2}\\right)^2 \\qquad = \\qquad  \\left(\\frac{x_1 - x_2}{\\sqrt{2}}\\right)^2\n\\]  So although we have \\(n=2\\) datapoints (\\(x_1\\) and \\(x_2\\)), the top part of the equation for the variance has only 1 (\\(n-1\\)) units of information. In order to take the average of these bits of information, we divide by \\(n-1\\).\n\n\n\n\n\n\nWe can get R to calculate this for us using the var() function:\n\nwechsler %&gt;%\n  summarise(\n    variance_iq = var(iq)\n  )\n\n# A tibble: 1 × 1\n  variance_iq\n        &lt;dbl&gt;\n1        238.\n\n\nStandard deviation\nOne difficulty in interpreting variance as a measure of spread is that it is in units of squared deviations. It relects the typical squared distance from a value to the mean.\nConveniently, by taking the square root of the variance, we can translate the measure back into the units of our original variable. This is known as the standard deviation.\n\n\n\n\n\n\nStandard Deviation: \\(s\\)\nThe standard deviation, denoted by \\(s\\), is a rough estimate of the typical distance from a value to the mean.\nIt is the square root of the variance (the typical squared distance from a value to the mean).\n\\[\ns = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}\n\\]\n\n\n\nWe can get R to calculate the standard deviation of a variable sd() function:\n\nwechsler %&gt;%\n  summarise(\n    variance_iq = var(iq),\n    sd_iq = sd(iq)\n  )\n\n# A tibble: 1 × 2\n  variance_iq sd_iq\n        &lt;dbl&gt; &lt;dbl&gt;\n1        238.  15.4"
  },
  {
    "objectID": "rd1_03.html#visualisations",
    "href": "rd1_03.html#visualisations",
    "title": "Numeric data",
    "section": "\n4 Visualisations",
    "text": "4 Visualisations\nBoxplots\nBoxplots provide a useful way of visualising the interquartile range (IQR). You can see what each part of the boxplot represents in Figure @ref(fig:boxplotdesc).\n\n\n\n\nAnatomy of a boxplot\n\n\n\nWe can create a boxplot of our age variable using the following code:\n\n# Notice, we put age on the x axis, making the box plot vertical. \n# If we had set aes(y = age) instead, then it would simply be rotated 90 degrees \nggplot(data = wechsler, aes(x = age)) +\n  geom_boxplot()\n\n\n\n\nHistograms\nNow that we have learned about the different measures of central tendency and of spread, we can look at how these influence visualisations of numeric variables.\nWe can visualise numeric data using a histogram, which shows the frequency of values which fall within bins of an equal width.\n\n# make a ggplot with the \"wechsler\" data. \n# on the x axis put the possible values in the \"iq\" variable,\n# add a histogram geom (will add bars representing the count \n# in each bin of the variable on the x-axis)\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram()\n\n\n\n\nWe can specifiy the width of the bins:\n\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram(binwidth = 5)\n\n\n\n\nLet’s take a look at the means and standard deviations of participants’ scores on the other tests (the test1 and test2 variables):\n\nwechsler %&gt;% \n  summarise(\n    mean_test1 = mean(test1),\n    sd_test1 = sd(test1),\n    mean_test2 = mean(test2),\n    sd_test2 = sd(test2)\n  )\n\n# A tibble: 1 × 4\n  mean_test1 sd_test1 mean_test2 sd_test2\n       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1       49.3     7.15       51.2     14.4\n\n\nTests 1 and 2 have similar means (around 50), but the standard deviation of Test 2 is almost double that of Test 1. We can see this distinction in the visualisation below - the histograms are centered at around the same point (50), but the one for Test 2 is a lot wider than that for Test 1.\n\n\n\n\n\nDensity curves\nIn addition to grouping numeric data into bins in order to produce a histogram, we can also visualise a density curve.\nFor the time being, you can think of the density as a bit similar to the notion of relative frequency, in that for a density curve, the values on the y-axis are scaled so that the total area under the curve is equal to 1. Because there are infinitely many values that numeric variables could take (e.g., 50, 50.1, 50.01, 5.001, …), we could group the data into infinitely many bins. In creating a curve for which the total area underneath is equal to one, we can use the area under the curve in a range of values to indicate the proportion of values in that range.\n\nggplot(data = wechsler, aes(x = iq)) + \n  geom_density()+\n  xlim(50,150)"
  },
  {
    "objectID": "rd1_03.html#skew",
    "href": "rd1_03.html#skew",
    "title": "Numeric data",
    "section": "\n5 Skew",
    "text": "5 Skew\nSkewness is a measure of asymmetry in a distribution. Distributions can be positively skewed or negatively skewed, and this influences our measures of central tendency and of spread to different degrees (see Figure @ref(fig:skewplot)).\n\n\n\n\nSkew influences the mean and median to different degrees."
  },
  {
    "objectID": "rd1_03.html#glossary",
    "href": "rd1_03.html#glossary",
    "title": "Numeric data",
    "section": "\n6 Glossary",
    "text": "6 Glossary\n\n\nInterquartile Range (IQR): The \\(3^{rd}\\) quartile minus the \\(1^{st}\\) quartile.\n\nMean: The sum of all observations divided by the total number of observations. The center of gravity of a variable.\n\nDeviation: The distance from an observation to the mean value.\n\nVariance: The average squared distance from observations to the mean value.\n\nStandard deviation: Square root of variance - can be thought of as the average distance from observations to the mean value.\n\nBoxplot: Displays the median and the IQR, and any extreme values.\n\n\nHistogram: Shows the frequency of values which fall within bins of an equal width.\n\nDensity curve: A curve for reflecting the distribution of a variable, for which the area under the curve sums to 1.\n\nSkew: A measure of asymmetry in a distribution. \n\n\nsummarise() To summarise variables into a single value according to whatever calculation we give it.\n\nIQR() To calculate the interquartile range for a given variable.\n\nmean() To calculate the mean of a given variable.\n\nsd() To calculate the standard deviation of a given variable.\n\nvar() To calculate the variance of a given variable.\n\ngeom_boxplot() To add a boxplot to a ggplot.\n\ngeom_histogram() To add a histogram to a ggplot.\n\ngeom_density() To add a density curve to a ggplot.\n\ngeom_vline() To add a vertical line to a ggplot."
  },
  {
    "objectID": "rd1_05.html",
    "href": "rd1_05.html",
    "title": "Types of relationships",
    "section": "",
    "text": "You have seen by now how to visualise the distribution of a variable, and how to visualise a relationship between two variables. Relationships between two variables can look very different, and can follow different patterns. These patterns can be expressed mathematically in the form of functions.\n\n\n\n\n\n\nFunctions\nA function is a mapping between two sets of numbers (\\(x\\) and \\(y\\)) - associating every element of \\(x\\) with an element in \\(y\\).\nWe often denote functions using the letter \\(f\\), in that we state that \\(y = f(x)\\) (“y equals f of x”).\nFor example, there is a mapping between these two sets:\n\\[x=\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ \\end{bmatrix}, \\qquad\ny=\\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ 9 \\\\ 10 \\\\ \\end{bmatrix}\\]\nAnd we can write this mapping as:\\[f(x) = x + 2\\]\nAnd we could visualise this relationship between \\(x\\) and \\(y\\):\n\n\n\n\n\n\n\n\n\nIn statistics, we often attempt to summarise the pattern that is present in the data using linear functions.\nImagine that we plant 10 trees, and measure their heights each year for 10 years. We could visualise this data (the relationship between time and tree height) on a scatterplot (we have added some lines to the plot to show which tree is which):\n\n\n\n\n\nWe might sensibly choose to describe this pattern as a line: \n\n\n\n\n\nAnd in order to describe a line like this, we require two things:\n\nThe starting point (i.e., where it crosses the y-axis)\nThe amount it goes up every year.\n\nWhen we planted the trees (at year 0), they were on average about 5cm tall. So this is where our line starts.\nFor every year, the trees grew by about 10cm on average. So we can now describe tree height as a function of time:\\[\\textrm{Tree height} = 5 + (10 \\times \\textrm{Years})\\]\nWe can write this in terms of \\(x\\) and \\(y\\):\n\n\n\\(y = f(x)\\)         “\\(y\\) is some function \\(f\\) of \\(x\\)”\n\n\\(f(x) = 5 + 10x\\)         “the function \\(f\\) maps each value \\(x_i\\) to \\(5 + (10 \\times x_i)\\)”\n\nFunctions don’t have to be linear. Often, we might want to describe relationships which appear to be more complex than a straight line.\nFor example, it is often suggested that for difficult tasks, some amount of stress may improve performance (but not too little or too much). We might think of the relationship between performance and stress as a curve (Figure 1). \n\n\n\n\nFigure 1: Yerkes Dodson Law\n\n\n\n One way to describe curves is to use polynomials (\\(x^2\\), \\(x^3\\), etc.).\nFor example, in the following two sets, \\(y\\) can be described as \\(f(x)\\) where \\(f(x)=x^2\\):\n\\[x=\\begin{bmatrix} -5 \\\\ -4 \\\\ -3 \\\\ -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}, \\qquad\ny=\\begin{bmatrix} 25 \\\\ 16 \\\\ 9 \\\\ 4 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 4 \\\\ 9 \\\\ 16 \\\\ 25 \\end{bmatrix}\\]\nand when we plot each value of \\(x\\) against the corresponding value of \\(y\\):\n\n\n\n\n\n\n\n\n\n\n\nThe code to create the above plot\n\n\n\n\n\n\n# the tibble() function can be used to create a dataframe\n# here, we create one with two variables, x and y.\n# x is the sequence from -5 to 5,\n# y is equal to x squared. \n# we save this data as \"poly\"\npoly &lt;- \n    tibble(\n        x = c(-5,-4,-3,-2,-1,0,1,2,3,4,5),\n        y = x^2\n    )\n# create a plot with \"poly\", with variable \"x\" on the x-axis,\n# and variable \"y\" on the y-axis. Add geom_points for each entry\nggplot(data = poly, aes(x = x, y = y)) + \n    geom_point()"
  },
  {
    "objectID": "rd1_05.html#functions",
    "href": "rd1_05.html#functions",
    "title": "Types of relationships",
    "section": "",
    "text": "You have seen by now how to visualise the distribution of a variable, and how to visualise a relationship between two variables. Relationships between two variables can look very different, and can follow different patterns. These patterns can be expressed mathematically in the form of functions.\n\n\n\n\n\n\nFunctions\nA function is a mapping between two sets of numbers (\\(x\\) and \\(y\\)) - associating every element of \\(x\\) with an element in \\(y\\).\nWe often denote functions using the letter \\(f\\), in that we state that \\(y = f(x)\\) (“y equals f of x”).\nFor example, there is a mapping between these two sets:\n\\[x=\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ \\end{bmatrix}, \\qquad\ny=\\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ 9 \\\\ 10 \\\\ \\end{bmatrix}\\]\nAnd we can write this mapping as:\\[f(x) = x + 2\\]\nAnd we could visualise this relationship between \\(x\\) and \\(y\\):\n\n\n\n\n\n\n\n\n\nIn statistics, we often attempt to summarise the pattern that is present in the data using linear functions.\nImagine that we plant 10 trees, and measure their heights each year for 10 years. We could visualise this data (the relationship between time and tree height) on a scatterplot (we have added some lines to the plot to show which tree is which):\n\n\n\n\n\nWe might sensibly choose to describe this pattern as a line: \n\n\n\n\n\nAnd in order to describe a line like this, we require two things:\n\nThe starting point (i.e., where it crosses the y-axis)\nThe amount it goes up every year.\n\nWhen we planted the trees (at year 0), they were on average about 5cm tall. So this is where our line starts.\nFor every year, the trees grew by about 10cm on average. So we can now describe tree height as a function of time:\\[\\textrm{Tree height} = 5 + (10 \\times \\textrm{Years})\\]\nWe can write this in terms of \\(x\\) and \\(y\\):\n\n\n\\(y = f(x)\\)         “\\(y\\) is some function \\(f\\) of \\(x\\)”\n\n\\(f(x) = 5 + 10x\\)         “the function \\(f\\) maps each value \\(x_i\\) to \\(5 + (10 \\times x_i)\\)”\n\nFunctions don’t have to be linear. Often, we might want to describe relationships which appear to be more complex than a straight line.\nFor example, it is often suggested that for difficult tasks, some amount of stress may improve performance (but not too little or too much). We might think of the relationship between performance and stress as a curve (Figure 1). \n\n\n\n\nFigure 1: Yerkes Dodson Law\n\n\n\n One way to describe curves is to use polynomials (\\(x^2\\), \\(x^3\\), etc.).\nFor example, in the following two sets, \\(y\\) can be described as \\(f(x)\\) where \\(f(x)=x^2\\):\n\\[x=\\begin{bmatrix} -5 \\\\ -4 \\\\ -3 \\\\ -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}, \\qquad\ny=\\begin{bmatrix} 25 \\\\ 16 \\\\ 9 \\\\ 4 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 4 \\\\ 9 \\\\ 16 \\\\ 25 \\end{bmatrix}\\]\nand when we plot each value of \\(x\\) against the corresponding value of \\(y\\):\n\n\n\n\n\n\n\n\n\n\n\nThe code to create the above plot\n\n\n\n\n\n\n# the tibble() function can be used to create a dataframe\n# here, we create one with two variables, x and y.\n# x is the sequence from -5 to 5,\n# y is equal to x squared. \n# we save this data as \"poly\"\npoly &lt;- \n    tibble(\n        x = c(-5,-4,-3,-2,-1,0,1,2,3,4,5),\n        y = x^2\n    )\n# create a plot with \"poly\", with variable \"x\" on the x-axis,\n# and variable \"y\" on the y-axis. Add geom_points for each entry\nggplot(data = poly, aes(x = x, y = y)) + \n    geom_point()"
  },
  {
    "objectID": "rd1_05.html#transformations",
    "href": "rd1_05.html#transformations",
    "title": "Types of relationships",
    "section": "\n2 Transformations",
    "text": "2 Transformations\nWe have seen previously how we might change all the values in a variable, for instance if we want to turn heights from centimetres to metres:\n\n# read in the starwars dataset and assign it the name \"starwars2\"\nstarwars2 &lt;- read_csv(\"https://uoepsy.github.io/data/starwars2.csv\")\n\n# take the starwars2 dataframe %&gt;%\n# mutate it such that there is a new variable called \"height_m\",\n# the values of which are equal to the \"height\" variable divided by 100.\n# then, select only the \"height\" and \"height_m\" columns (this is just \n# to make it easier to see without all the other variables)\nstarwars2 %&gt;%\n    mutate(\n        height_m = height/100\n    ) %&gt;% \n    select(height, height_m)\n\n# A tibble: 75 × 2\n   height height_m\n    &lt;dbl&gt;    &lt;dbl&gt;\n 1    172     1.72\n 2    167     1.67\n 3     96     0.96\n 4    202     2.02\n 5    150     1.5 \n 6    178     1.78\n 7    165     1.65\n 8     97     0.97\n 9    183     1.83\n10    182     1.82\n# ℹ 65 more rows\n\n\nWhat we have done here, can be described as a transformation, in that we have applied a mathematical function to the values in the height variable.\n\n\n\n\n\n\nTransformation\nData transformation is when we apply a deterministic function to map each value of a variable to a transformed value.\nWe transform for various reasons. For instance, we can use it to change the units we are interpreting (e.g., cm to m), or to change the shape of a distribution (e.g., make it less skewed).\n\n\n\nWe could even plot the heights in cm and heights in m against one another (note what units are on each axis):\n\n\n\n\n\n The relationship between a variable and a transformed variable need be linear, for example, log transformation: \n\n\n\n\n\n\n2.1 A recap of Logarithms and Natural Logarithms\n\n\n\n\n\n\nLogarithm\nA logarithm (log) is the power to which a number must be raised in order to get some other number.\nTake as examples \\(10^2\\) and \\(10^3\\):\n\\[\n10^2 = 100 \\qquad \\Longleftrightarrow \\qquad \\log_{10} (100) = 2 \\\\\n10^3 = 1000 \\qquad \\Longleftrightarrow \\qquad \\log_{10} (1000) = 3\n\\]\nWe refer to \\(\\log_{10}\\) as “Log base 10”.\n\n\n\n\n\n\n\n\n\nNatural log\nA special case of the logarithm is referred to as the natural logarithm, and is denoted by \\(ln\\) or \\(log_e\\), where \\(e = 2.718281828459...\\)\n\\(e\\) is a special number for which \\(log_e(e) = 1\\).\nIn R, this is just the log() function, which uses the base \\(e\\) by default.\n\n# Natural logarithm of e is 1:\nlog(2.718281828459, base = 2.718281828459)\n\n[1] 1\n\nlog(2.718281828459)\n\n[1] 1"
  },
  {
    "objectID": "rd1_05.html#centering-and-standardisation",
    "href": "rd1_05.html#centering-and-standardisation",
    "title": "Types of relationships",
    "section": "\n3 Centering and Standardisation",
    "text": "3 Centering and Standardisation\nRecall our dataset from our introduction to handling numerical data, in which we had data on 120 participants’ IQ scores (measured on the Wechsler Adult Intelligence Scale, WAIS), their ages, and their scores on 2 other tests. We know how to calculate the mean and standard deviation of the IQ scores:\n\n# read in the data\nwechsler &lt;- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\n\n# calculate the mean and sd of IQs\nwechsler %&gt;% \n  summarise(\n    mean_iq = mean(iq),\n    sd_iq = sd(iq)\n  )\n\n# A tibble: 1 × 2\n  mean_iq sd_iq\n    &lt;dbl&gt; &lt;dbl&gt;\n1    99.3  15.4\n\n\nTwo very useful transformations we can apply to a variable are centering and standardisation.\n\n\nCentering A transformation which re-expresses each value as the distance from a given number (e.g., the mean).\n\n\nStandardising A transformation which re-expresses each value as the distance from the mean in units of standard deviations.\n\n\n\n\n\n\n\n\nMean-centering\nTo Mean-center a variable, we simply subtract the mean from each value, \\(x_i - \\bar{x}\\): \\[\n\\textrm{raw IQ} = \\begin{bmatrix} 71 \\\\ 103 \\\\ 74 \\\\ 108 \\\\ 118 \\\\ 129 \\\\ ... \\end{bmatrix}, \\qquad\n\\textrm{mean centered IQ} = \\begin{bmatrix} 71-99.3 \\\\ 103-99.3 \\\\ 74-99.3 \\\\ 108-99.3 \\\\ 118-99.3 \\\\ 129-99.3 \\\\ ... \\end{bmatrix} = \\begin{bmatrix} -28.3 \\\\ 3.7 \\\\ -25.3 \\\\ 8.7 \\\\ 18.7 \\\\ 29.7 \\\\ ... \\end{bmatrix}\n\\]\n\n\n\nTo mean-center in R, we can simply add a new variable using mutate() and subtract the mean IQ from the IQ variable:\n\n# Take the \"wechsler\" dataframe, and mutate it,\n# such that there is a variable called \"iq_meancenter\" for which\n# the entries are equal to the \"iq\" variable entries minus the \n# mean of the \"iq\" variable\nwechsler %&gt;%\n  mutate(\n    iq_meancenter = iq - mean(iq)\n  )\n\n# A tibble: 120 × 6\n   participant    iq   age test1 test2 iq_meancenter\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n 1 ppt_1          71    27    46    50        -28.3 \n 2 ppt_2         103    38    42    29          3.67\n 3 ppt_3          74    20    50    77        -25.3 \n 4 ppt_4         108    46    50    62          8.67\n 5 ppt_5         118    45    60    29         18.7 \n 6 ppt_6         129    33    45    45         29.7 \n 7 ppt_7         103    49    42    41          3.67\n 8 ppt_8         120    27    63    33         20.7 \n 9 ppt_9          96    37    53    44         -3.33\n10 ppt_10         80    26    53    21        -19.3 \n# ℹ 110 more rows\n\n\n\n\n\n\n\n\nStandardisation\nWhen we standardise a variable, we call the transformed values z-scores. To transform a given value \\(x_i\\) into a z-score, we simply calculate the distance from \\(x_i\\) to the mean, \\(\\bar{x}\\), and divide this by the standard deviation, \\(s\\)\n\\[\nz_i = \\frac{x_i - \\bar{x}}{s}\n\\]\nSo for each of the raw IQ scores, we can transform them to z-scores by subtracting the mean and then dividing by the standard deviation. The resulting values tell us how low/high each participant’s IQ score is compared to observed distribution of scores:\\[\n\\textrm{raw IQ} = \\begin{bmatrix} 71 \\\\ 103 \\\\ 74 \\\\ 108 \\\\ 118 \\\\ 129 \\\\ ... \\end{bmatrix}, \\qquad\n\\textrm{standardised IQ} = \\begin{bmatrix} \\frac{71-99.3}{15.43} \\\\ \\frac{103-99.3}{15.43} \\\\ \\frac{74-99.3}{15.43} \\\\ \\frac{108-99.3}{15.43} \\\\ \\frac{118-99.3}{15.43} \\\\ \\frac{129-99.3}{15.43} \\\\ ... \\end{bmatrix} = \\begin{bmatrix} -1.84 \\\\ 0.238 \\\\ -1.64 \\\\ 0.562 \\\\ 1.21 \\\\ 1.92 \\\\ ... \\end{bmatrix}\n\\]\n\n\n\nWe can achieve this in R either by manually performing the calculation:\n\n# Take the \"wechsler\" dataframe, and mutate it,\n# such that there is a variable called \"iq_z\" for which\n# the entries are equal to the \"iq\" variable entries minus the mean of the \"iq\"\n# variable, divided by the standard deviation of the \"iq\" variable.  \nwechsler %&gt;% \n  mutate(\n    iq_z = (iq - mean(iq)) / sd(iq)\n  )\n\n# A tibble: 120 × 6\n   participant    iq   age test1 test2   iq_z\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 ppt_1          71    27    46    50 -1.84 \n 2 ppt_2         103    38    42    29  0.238\n 3 ppt_3          74    20    50    77 -1.64 \n 4 ppt_4         108    46    50    62  0.562\n 5 ppt_5         118    45    60    29  1.21 \n 6 ppt_6         129    33    45    45  1.92 \n 7 ppt_7         103    49    42    41  0.238\n 8 ppt_8         120    27    63    33  1.34 \n 9 ppt_9          96    37    53    44 -0.216\n10 ppt_10         80    26    53    21 -1.25 \n# ℹ 110 more rows\n\n\nOr we can use the scale() function:\n\n# Take the \"wechsler\" dataframe, and mutate it,\n# such that there is a variable called \"iq_std\" for which\n# the entries are equal to the scaled values of the \"iq\" variable.  \nwechsler %&gt;%\n  mutate(\n    iq_std = scale(iq)\n  )\n\n# A tibble: 120 × 6\n   participant    iq   age test1 test2 iq_std[,1]\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 ppt_1          71    27    46    50     -1.84 \n 2 ppt_2         103    38    42    29      0.238\n 3 ppt_3          74    20    50    77     -1.64 \n 4 ppt_4         108    46    50    62      0.562\n 5 ppt_5         118    45    60    29      1.21 \n 6 ppt_6         129    33    45    45      1.92 \n 7 ppt_7         103    49    42    41      0.238\n 8 ppt_8         120    27    63    33      1.34 \n 9 ppt_9          96    37    53    44     -0.216\n10 ppt_10         80    26    53    21     -1.25 \n# ℹ 110 more rows\n\n\nWe can also use the scale() function to mean-center a variable, by setting scale(variable, center = TRUE, scale = FALSE):\n\n# create two new variables in the \"wechsler\" dataframe, one which is \n# mean centered iq, and one which is standardised iq:\nwechsler %&gt;%\n  mutate(\n    iq_mc = scale(iq, center = TRUE, scale = FALSE),\n    iq_std = scale(iq, center = TRUE, scale = TRUE) # these are the default settings\n  )\n\n# A tibble: 120 × 7\n   participant    iq   age test1 test2 iq_mc[,1] iq_std[,1]\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 ppt_1          71    27    46    50    -28.3      -1.84 \n 2 ppt_2         103    38    42    29      3.67      0.238\n 3 ppt_3          74    20    50    77    -25.3      -1.64 \n 4 ppt_4         108    46    50    62      8.67      0.562\n 5 ppt_5         118    45    60    29     18.7       1.21 \n 6 ppt_6         129    33    45    45     29.7       1.92 \n 7 ppt_7         103    49    42    41      3.67      0.238\n 8 ppt_8         120    27    63    33     20.7       1.34 \n 9 ppt_9          96    37    53    44     -3.33     -0.216\n10 ppt_10         80    26    53    21    -19.3      -1.25 \n# ℹ 110 more rows\n\n\n\n3.1 Test norms\nMany neuropsychological tests will have norms - parameters which describe the distribution of scores on the test in a normal population. For instance, in a normal adult population, scores on the WAIS have a mean of 100 and a standard deviation of 15.  What this means is that rather than calculating a standardised score against the observed mean of our sample, we might calculate a standardised score against the test norms. In the formula for \\(z\\), we replace our sample mean \\(\\bar{x}\\) with the population mean \\(\\mu\\), and the sample standard deviation \\(s\\) with the population standard deviation \\(\\sigma\\):\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nThe resulting values tell us how low/high each participant’s IQ score is compared to the distribution of IQ scores in the population."
  },
  {
    "objectID": "rd1_05.html#glossary",
    "href": "rd1_05.html#glossary",
    "title": "Types of relationships",
    "section": "\n4 Glossary",
    "text": "4 Glossary\n\n\nFunction: A mapping between two sets of numbers, associating every element of the first set with an elemet in the second.\n\n\nTransformation: Applying a function to a variable to map each value to a transformed value.\n\nLogarithm: The power to which a number must be raised in order to get some other number.\n\nCentering: Transformation which re-expresses each value as the distance from a given number (e.g., the mean).\n\n\nStandardisation: Transformation which re-expresses each value as the distance from the mean in units of standard deviations. \n\n\nscale() To mean center or standardise a variable (depending upon whether center=TRUE/FALSE and scale=TRUE/FALSE)."
  },
  {
    "objectID": "rd1_08.html",
    "href": "rd1_08.html",
    "title": "Probability 2",
    "section": "",
    "text": "Let’s recap the probability rules discussed last week.\nRule 1: Probability assignment rule\nThe probability of an impossible event (an event which never occurs) is 0 and the probability of a certain event (an event which always occurs) is 1.\nHence, we have that the probability is a number between 0 and 1: \\[\\text{for any event }A, \\\\ 0 \\leq P(A) \\leq 1\\]\nRule 2: Total probability rule\nIf an experiment has a single possible outcome, it is not random as that outcome will happen with certainty (i.e. probability 1).\nWhen dealing with two or more possible outcomes, we need to be sure to distribute the entire probability among all of the possible outcomes in the sample space \\(S\\).\nThe sample space must have probability 1: \\[P(S) = 1\\]\nIt must be that the will observe one of the outcomes listed within the collection of all possible outcomes of the experiment.\nRule 3: Complement rule\nIf the probability of observing the face “2” in a die is 1/6 = 0.17, what’s the probability of not observing the face “2”? It must be 1 - 1/6 = 5/6 = 0.83.\nIf \\(A = \\{2\\}\\), the event not A is written \\(\\sim A\\), which is a shortcut for \\(S\\) without \\(A\\), that is \\(\\{1, 3, 4, 5, 6\\}\\).\n\\[P(\\sim A) = 1 - P(A)\\]\nRule 4: Addition rule for disjoint events\nSuppose the probability that a randomly picked person in a town is \\(A\\) = “a high school student” is \\(P(A) = 0.3\\) and that the probability of being \\(B\\) = “a university student” is \\(P(B) = 0.5\\).\nWhat is the probability that a randomly picked person from the town is either a high school student or a university student? We write the event “either A or B” as \\(A \\cup B\\), pronounced “A union B”.\nIf you said 0.8, because it is 0.3 + 0.5, then you just applied the addition rule: \\[\n\\text{If }A \\text{ and } B \\text{ are mutually exclusive events,}\\\\\nP(A \\cup B) = P(A) + P(B)\n\\]\nRule 5: Multiplication rule for independent events\nWe saw that probability of observing an even number (\\(E\\)) when throwing a die is 0.5.\nYou also know that the probability of observing heads (\\(H\\)) when throwing a fair coin is 0.5.\nWhat’s the probability of observing an even number and heads (that is, \\(E\\) and \\(H\\), written \\(E \\cap H\\)) when throwing both items together?\nThe rule simply says that in this case we multiply the two probabilities together: 0.5 * 0.5 = 0.25.\nThe multiplication rule for independent events says: \\[\n\\text{If }A \\text{ and } B \\text{ are independent events,}\\\\\nP(A \\cap B) = P(A) \\times P(B)\n\\]"
  },
  {
    "objectID": "rd1_08.html#recap",
    "href": "rd1_08.html#recap",
    "title": "Probability 2",
    "section": "",
    "text": "Let’s recap the probability rules discussed last week.\nRule 1: Probability assignment rule\nThe probability of an impossible event (an event which never occurs) is 0 and the probability of a certain event (an event which always occurs) is 1.\nHence, we have that the probability is a number between 0 and 1: \\[\\text{for any event }A, \\\\ 0 \\leq P(A) \\leq 1\\]\nRule 2: Total probability rule\nIf an experiment has a single possible outcome, it is not random as that outcome will happen with certainty (i.e. probability 1).\nWhen dealing with two or more possible outcomes, we need to be sure to distribute the entire probability among all of the possible outcomes in the sample space \\(S\\).\nThe sample space must have probability 1: \\[P(S) = 1\\]\nIt must be that the will observe one of the outcomes listed within the collection of all possible outcomes of the experiment.\nRule 3: Complement rule\nIf the probability of observing the face “2” in a die is 1/6 = 0.17, what’s the probability of not observing the face “2”? It must be 1 - 1/6 = 5/6 = 0.83.\nIf \\(A = \\{2\\}\\), the event not A is written \\(\\sim A\\), which is a shortcut for \\(S\\) without \\(A\\), that is \\(\\{1, 3, 4, 5, 6\\}\\).\n\\[P(\\sim A) = 1 - P(A)\\]\nRule 4: Addition rule for disjoint events\nSuppose the probability that a randomly picked person in a town is \\(A\\) = “a high school student” is \\(P(A) = 0.3\\) and that the probability of being \\(B\\) = “a university student” is \\(P(B) = 0.5\\).\nWhat is the probability that a randomly picked person from the town is either a high school student or a university student? We write the event “either A or B” as \\(A \\cup B\\), pronounced “A union B”.\nIf you said 0.8, because it is 0.3 + 0.5, then you just applied the addition rule: \\[\n\\text{If }A \\text{ and } B \\text{ are mutually exclusive events,}\\\\\nP(A \\cup B) = P(A) + P(B)\n\\]\nRule 5: Multiplication rule for independent events\nWe saw that probability of observing an even number (\\(E\\)) when throwing a die is 0.5.\nYou also know that the probability of observing heads (\\(H\\)) when throwing a fair coin is 0.5.\nWhat’s the probability of observing an even number and heads (that is, \\(E\\) and \\(H\\), written \\(E \\cap H\\)) when throwing both items together?\nThe rule simply says that in this case we multiply the two probabilities together: 0.5 * 0.5 = 0.25.\nThe multiplication rule for independent events says: \\[\n\\text{If }A \\text{ and } B \\text{ are independent events,}\\\\\nP(A \\cap B) = P(A) \\times P(B)\n\\]"
  },
  {
    "objectID": "rd1_08.html#general-addition-rule",
    "href": "rd1_08.html#general-addition-rule",
    "title": "Probability 2",
    "section": "\n2 General addition rule",
    "text": "2 General addition rule\nConsider throwing a six-sided die, for which we already know that the sample space is \\(S = \\{1, 2, 3, 4, 5, 6\\}\\). What’s the probability of observing an odd number or a number which is less than 4?\nThe relevant events are:\n\n\\(A = \\text{result is odd} = \\{1, 3, 5\\}\\)\n\\(B = \\text{result is less than four} = \\{1, 2, 3\\}\\)\n\nand we know that:\n\n\\(P(A) = 3/6 = 0.5\\)\n\\(P(B) = 3/6 = 0.5\\)\n\nHowever, the probability of \\(A\\) or \\(B\\), written \\(P(A \\cup B)\\), is not simply \\(P(A) + P(B)\\), because the events \\(A\\) and \\(B\\) are not disjoint; they overlap. The outcomes 1 and 3 play a crucial role here as they are both odd and are less than four, i.e. they are in both sets. They are both in \\(A\\) and \\(B\\), which places them in the intersection of the two circles.\nThe diagram below represents the sample space as \\(S\\). Notice that the outcomes 4 and 6 are neither odd nor less than four, so they sit outside both circles.\n\n\n\n\n\n\n\n\nThe reason we can’t simply add the probabilities of \\(A\\) and \\(B\\) is that we would count the outcomes 1 and 3 twice. If we did add the two probabilities, we could compensate by subtracting the probability of the outcomes 1 and 3.\n\\[\n\\begin{aligned}\nP(\\text{odd or less than 4}) &= P(\\text{odd}) + P(\\text{less than 4}) - P(\\text{odd and less than 4}) \\\\\n&= P(\\{1, 3, 5\\}) + P(\\{1, 2, 3\\}) - P(\\{1, 3\\}) \\\\\n&= 3/6 + 3/6 - 2/6 \\\\\n&= 0.667\n\\end{aligned}\n\\]\nThis is true in general and is called the general addition rule, which does not require disjoint events. Consider two generic events \\(A\\) and \\(B\\) such that\n\n\n\n\n\n\n\n\nThen, the probability of observing \\(A\\) or \\(B\\) is: \\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\nIntuitively, \\(P(A) + P(B)\\) counts \\(A \\cap B\\) twice, so we have to subtract \\(P(A \\cap B)\\) to make the net number of times \\(A \\cap B\\) is counted equal to 1.\nWhen the two events have no outcomes in common, i.e. \\(P(A \\cap B) = 0\\) as the two events can’t happen together, the general addition rule reduces to the addition rule for disjoint events.\n\n\n\n\n\n\nTERMINOLOGY\n\n\n\n\nWould you like fruit or dessert?\n\nEveryday language can be ambiguous. It’s not clear if by answering “yes” to the question above our choice is only one of the two options, or we can actually have both.\nTypically, everyday language uses “or” meaning the exclusive version. That is, you can have one or the other, but not both. In other words, you can have fruit or dessert, but not both.\nWhen talking about probability, being imprecise and ambiguous can get us into trouble. For this reason, we will here define once for all what we mean by “or” in a mathematical sense.\nIn statistics, when we say “or” we always mean the inclusive version. In other words, the probability of \\(A\\) or \\(B\\) means the probability of either \\(A\\) or \\(B\\) or both.\n\n\nLet’s consider another example. Suppose that 25% of people have an electric scooter, 29% of people have a bike, and 12% of people own both. What is the probability that someone owns an electric scooter or a bike?\nThis question concerns the following two events\n\n\\(A = \\text{owning an electric scooter}\\)\n\\(B = \\text{owning a bike}\\)\n\nand we are told that\n\n\\(P(A) = 0.25\\)\n\\(P(B) = 0.29\\)\n\\(P(A \\cap B) = 0.12\\)\n\nClearly, \\(A\\) and \\(B\\) are not mutually exclusive (or disjoint) events. Having a scooter doesn’t exclude owning a bike, and vice versa. In fact, the problem statements tells us exactly the percentage of people having both an electric scooter and a bike, 12%.\nSo… how do we calculate the probability of owning an electric scooter or a bike?\nIf we simply did \\(P(A) + P(B)\\), we would count twice those having both a scooter and a bike. Hence, we need to subtract \\(P(A \\cap B)\\): \\[\n\\begin{aligned}\nP(A \\cup B) &= P(A) + P(B) - P(A \\cap B) \\\\\n&= 0.25 + 0.29 - 0.12 \\\\\n&= 0.42\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rd1_08.html#conditional-probability",
    "href": "rd1_08.html#conditional-probability",
    "title": "Probability 2",
    "section": "\n3 Conditional probability",
    "text": "3 Conditional probability\nThe following contingency table displays the counts of passengers who did or did not survive the Titanic sinking by travelling class.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvived\n\n\n\n\nClass\nNo\nYes\nTotal\n\n\n\n\n1st\n122\n203\n325\n\n\n2nd\n167\n118\n285\n\n\n3rd\n528\n178\n706\n\n\nTotal\n817\n499\n1316\n\n\n\n\n\nYou already encountered contingency tables when studying the relationship between two categorical variables in week 4.\nIf you divide the frequency table by the total number of passengers, 1316, you obtain a table of relative frequencies. It is just a small step from these relative frequencies to probabilities.\nLet;s focus on the Titanic survival study and make the sample space just the set of these 1316 passengers. If we select a passenger at random from this study, the probability we select a first class passenger is just the corresponding relative frequency (since we are equally likely to select any of the 1316 passengers). There are 325 first class passengers in the data out of a total of 1316, giving a probability of \\[\nP( \\text{1st} ) = \\frac{325}{1316} = 0.247\n\\] The same method works for more complicated events like intersections. For example, what’s the probability of selecting a 1st class passenger who survived? Well, 203 first class passengers survived, so the probability is \\[\nP( \\text{1st} \\cap \\text{survived} ) = \\frac{203}{1316} = 0.154\n\\]\nThe probability of selecting a passenger who survived is \\[\nP( \\text{survived} ) = \\frac{499}{1316} = 0.379\n\\] What if we are given the information that the selected passenger was in first class? Would that change the probability that the selected passenger survived? You bet it would!\nWhen we restrict our focus to first class passengers, we look only at the row of the table where “Class” is “1st”. Of the 325 first class passengers, only 203 of them said survived. We write the probability that a selected passenger survived given that we have selected a first class passenger as \\[\nP(\\text{survived} \\mid \\text{1st}) = \\frac{203}{325} = 0.625\n\\]\nNow, imagine dividing numerator and denominator by the total, 1316: \\[\n\\begin{aligned}\nP(\\text{survived} \\mid \\text{1st})\n= \\frac{203 / 1316}{325 / 1316}\n= \\frac{P(\\text{1st} \\cap \\text{survived})}{P(\\text{1st})}\n\\end{aligned}\n\\]\nA probability that takes into account a given condition, such as being a first class passenger, is called a conditional probability. For generic events \\(A\\) and \\(B\\), we write the conditional probability as \\(P(B | A)\\) and pronounce it “the probability of \\(B\\) given \\(A\\)”.\nSuppose we are told that the event \\(A\\) with \\(P(A) &gt; 0\\) occurs. The conditional probability of \\(B\\) given \\(A\\) is given by: \\[\nP(B | A) = \\frac{P(A \\cap B)}{P(A)}\n\\] Intuitively, we need to divide the formula by \\(P(A)\\) to make sure that \\(P(A|A) = 1\\).\nThe formula wouldn’t work if \\(P(A) = 0\\), but this makes sense as we couldn’t be told that \\(A\\) occurred if it was an impossible event!\n\n\n\n\n\n\nIMPORTANT NOTE\n\n\n\nThe conditional probability \\(P(\\text{some event} | A)\\) satisfies the standard rules of probability. You can think of it as the same as \\(P(\\text{some event})\\) but after having replaced the sample space from \\(S\\) to \\(A\\), which is the event that we have been told happened. As the event happened, we know that the new sample space is now \\(A\\), so that \\(P(A) = 1\\).\nTo see this, remember that \\(P(S) = 1\\) and we can then rewrite\n\\[P(A) = \\frac{P(A)}{1} = \\frac{P(A)}{P(S)} = \\frac{P(A \\cap S)}{P(S)} = P(A | S)\\]\nThe rule \\(P(A) + P({} \\sim A) = 1\\) can be written as \\(P(A | S) + P({} \\sim A | S) = 1\\).\nComplement rule for conditional probabilities\nIn general, according to the complement rule, \\(P({}\\sim B|A) = 1 - P(B|A)\\).\nProduct rule for conditionally independent events\nWe also have a similar rule for independence. Two events \\(A\\) and \\(B\\) are conditionally independent given \\(C\\) if:\n\\[\nP(A \\cap B | C) = P(A | C) \\times P(B | C)\n\\]\n\n\nConsider the following plot:\n\n\n\n\n\n\n\n\nOn the y-axis we see the conditional probabilities of survival given the passenger class. That is \\(P(\\text{survived} | \\text{1st}) = 0.62\\), \\(P(\\text{survived} | \\text{2nd}) = 0.41\\), and \\(P(\\text{survived} | \\text{3rd}) = 0.25\\).\nThe x-axis displays the probability of a randomly selected passenger to be in each travelling class. For example, \\(P(\\text{1st}) = 0.25\\), \\(P(\\text{2nd}) = (0.46 - 0.25) = 0.21\\), and \\(P(\\text{3rd}) = 1 - (P(\\text{1st}) + P(\\text{2nd})) = 1 - 0.25 - 0.21 = 1 - 0.46 = 0.54\\)"
  },
  {
    "objectID": "rd1_08.html#general-multiplication-rule",
    "href": "rd1_08.html#general-multiplication-rule",
    "title": "Probability 2",
    "section": "\n4 General multiplication rule",
    "text": "4 General multiplication rule\nConsider two generic events \\(A\\) and \\(B\\). Before, we saw that \\[\nP(B | A) = \\frac{P(A \\cap B)}{P(A)}\n\\]\nRearranging the conditional probability formula we obtain the general multiplication rule: \\[\nP(A \\cap B) = P(A) P(B|A)\n\\]\nIf you were to start from \\(P(A | B)\\) you would reach to the equivalent version: \\[\nP(A \\cap B) = P(B) P(A|B)\n\\]"
  },
  {
    "objectID": "rd1_08.html#independence",
    "href": "rd1_08.html#independence",
    "title": "Probability 2",
    "section": "\n5 Independence",
    "text": "5 Independence\nWe say that two events \\(A\\) and \\(B\\) are independent, if knowing that one occurred doesn’t change the probability of the other occurring:\n\\[\nP(B|A) = P(B)\n\\]\nFrom this we have that\n\\[\nP(B) = P(B|A) = \\frac{P(A \\cap B)}{P(A)}\n\\]\nLeading to\n\\[\nP(A \\cap B) = P(A) P(B)\n\\] At the same time,\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A) P(B)}{P(B)} = P(A)\n\\]\nSo, we have three equivalent definitions of independent events! If one holds, the remaining ones hold as well. Two events \\(A\\) and \\(B\\) are independent if and only if \\[\n\\begin{aligned}\nP(A | B) &= P(A) \\\\\nP(B | A) &= P(B) \\\\\nP(A \\cap B) &= P(A) P(B)\n\\end{aligned}\n\\]\n\n\n\n\n\n\nIn pictures: Independence vs Dependence\n\n\n\n\n\nIndependence happens when the probability of “Survived = Yes” is the same regardless of whether the passenger was in 1st, 2nd, or 3rd class.\n\n\n\n\n\n\n\n\nPerfect dependence happens when knowing the passenger class leads to a perfect prediction of whether a passenger survived or not.\n\n\n\n\n\n\n\n\n\n\n\nNote that, for independent events \\(A\\) and \\(B\\) such that \\(P(B | A) = P(B)\\), the general multiplication rule reduces to the multiplication rule for independent events: \\[\nP(A \\cap B) = P(B|A)P(A) = P(B) P(A)\n\\]"
  },
  {
    "objectID": "rd1_08.html#bayes-rule-reversing-the-conditioning",
    "href": "rd1_08.html#bayes-rule-reversing-the-conditioning",
    "title": "Probability 2",
    "section": "\n6 Bayes’ rule: reversing the conditioning",
    "text": "6 Bayes’ rule: reversing the conditioning\nSuppose we have \\(P(A | B)\\) but we are interested in \\(P(B | A)\\). That is, we want to reverse the conditioning.\nBy applying the rule for conditional probability and the general multiplication rule, we have that: \\[\nP(B | A) = \\frac{P(A \\cap B)}{P(A)} = \\frac{P(A|B)P(B)}{P(A)}\n\\]"
  },
  {
    "objectID": "rd1_08.html#glossary",
    "href": "rd1_08.html#glossary",
    "title": "Probability 2",
    "section": "\n7 Glossary",
    "text": "7 Glossary\n\n\nGeneral addition rule. For any events \\(A\\) and \\(B\\), \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).\n\nGeneral multiplication rule. For any events \\(A\\) and \\(B\\), \\(P(A \\cap B) = P(B|A)P(A) = P(A | B) P(B)\\).\n\nIndependent events. Two events \\(A\\) and \\(B\\) are independent if and only if \\(P(A \\cap B) = P(A) P(B)\\)\n\n\nBayes’ rule. \\(P(B | A) = \\frac{P(A | B)P(B)}{P(A)}\\)"
  },
  {
    "objectID": "rd1_10.html",
    "href": "rd1_10.html",
    "title": "Continuous random variables",
    "section": "",
    "text": "Recall that a random variable is continuous if there are no gaps between the possible values it can take.\n\n\n\n\n\n\n\n\nImagine a spinner numbered with the hours of the day. If you were to spin it with some strength, what’s the probability of the hand stopping exactly at 3.5173847294 hours?\nWell, there is only one way to observe 3.5173847294, while there are an infinite number of possibilities between 0 and 24 hours (you could have as many decimals as you wish).\nDefine the event \\(A = \\{ 3.5173847294 \\}\\). If you were to calculate the probability of observing \\(A\\) using the standard formula you would get: \\[\nP(A) = \\frac{n_A}{n} = \\frac{1}{\\infty} = 0\n\\]\nThe probability of the hand stopping exactly at 3.5173847294 hours is 0.\nCheck this with R:\n\n1 / Inf\n\n[1] 0\n\n\nFor this reason, we cannot compute probabilities for continuous random variables in the same way as we did in the previous weeks (number of outcomes in the event / number of possible outcomes).\n\n\n\n\n\n\nKey point\nFor continuous random variables we can only compute the probability of observing a value in a given interval. For example, the probability that the hand stops in between 3 and 3.5 hours.\n\n\n\nConsider the histogram shown below. The height of each rectangle will tell you the number of people having a value within a specific interval. For example, the number of people having a value of \\(x\\) between 20 and 22.5."
  },
  {
    "objectID": "rd1_10.html#continuous-random-variables",
    "href": "rd1_10.html#continuous-random-variables",
    "title": "Continuous random variables",
    "section": "",
    "text": "Recall that a random variable is continuous if there are no gaps between the possible values it can take.\n\n\n\n\n\n\n\n\nImagine a spinner numbered with the hours of the day. If you were to spin it with some strength, what’s the probability of the hand stopping exactly at 3.5173847294 hours?\nWell, there is only one way to observe 3.5173847294, while there are an infinite number of possibilities between 0 and 24 hours (you could have as many decimals as you wish).\nDefine the event \\(A = \\{ 3.5173847294 \\}\\). If you were to calculate the probability of observing \\(A\\) using the standard formula you would get: \\[\nP(A) = \\frac{n_A}{n} = \\frac{1}{\\infty} = 0\n\\]\nThe probability of the hand stopping exactly at 3.5173847294 hours is 0.\nCheck this with R:\n\n1 / Inf\n\n[1] 0\n\n\nFor this reason, we cannot compute probabilities for continuous random variables in the same way as we did in the previous weeks (number of outcomes in the event / number of possible outcomes).\n\n\n\n\n\n\nKey point\nFor continuous random variables we can only compute the probability of observing a value in a given interval. For example, the probability that the hand stops in between 3 and 3.5 hours.\n\n\n\nConsider the histogram shown below. The height of each rectangle will tell you the number of people having a value within a specific interval. For example, the number of people having a value of \\(x\\) between 20 and 22.5."
  },
  {
    "objectID": "rd1_10.html#proportions-as-areas-the-density-histogram",
    "href": "rd1_10.html#proportions-as-areas-the-density-histogram",
    "title": "Continuous random variables",
    "section": "\n2 Proportions as areas: the density histogram",
    "text": "2 Proportions as areas: the density histogram\nThe natural gestation period for human births has a mean of about 266 days and a standard deviation of about 16 days. A group of researchers recorded the gestation period (in days) of 10,000 babies and the distribution is shown below as a frequency histogram (left panel) and relative frequency histogram (right panel).\n\n\n\n\nFrequency histogram (left panel) and relative frequency histogram (right panel) of gestation period.\n\n\n\nThe histogram is unimodal, with the modal interval extending from 266 to 270 days. The distribution is symmetric around the centre (266). This means that if you were the fold the histogram at 266 days, the two halves would match when superimposed.\nThe histogram also highlights some variability in gestation days from person to person, with some women giving birth after as low as 200 days or as high as 320 days.\nUsually, histograms are drawn with the height of the rectangle for the \\(j\\)th interval being either the frequency (count) \\(n_j\\) or the relative frequency \\(n_j / n\\) of observations falling into that interval.\nFor example, for roughly 400 women the gestation period was between 240 and 245 days. If we use the relative frequency, instead, the height of the rectangle tells us the proportion of observations in that interval. In this case, the proportion of women with a gestation period between 240 and 245 days is \\(400 / 10,000 = 0.04\\).\nWe now discuss a third type of histogram called the density histogram, in which we adjust the heigh of the rectangle so that the area is equal to the relative frequency. This is accomplished by changing the height to \\(n_j / (n w)\\) where \\(w\\) is the interval width.\nWhy does this work? \\[\narea = width \\cdot height = w \\cdot \\frac{n_j}{n w} = \\frac{n_j}{n}\n\\]\nSo now it is the area of the \\(j\\)th rectangle that tells us the proportion of observations in the \\(j\\)th interval.\n\n\n\n\n\nFigure 1: Density histogram of the gestation period for the 10,000 women in the study.\n\n\n\n\nIn Figure 1(a) the proportion of women under study whose gestation period falls within any class interval is the area of the corresponding rectangle. For example the proportion of women with a gestation period between 240 and 245 days is equal to \\(area = width \\cdot height = 5 \\cdot 0.008 = 0.04\\), which matches the relative frequency histogram.\nFurthermore, we can obtain the proportion of women whose gestation period falls between the limits \\(a = 240\\) days and \\(b = 255\\) days by adding the areas of all the rectangles between these limits. As the graph in Figure 1(b) tells us, the area of the shaded part is .196. Thus, the proportion of women with a gestation period between 240 and 255 days is 0.196 or 19.6% of the women in the study population.\nThis correspondence between area and proportions is the foundation on which the probability for continuous random variables is built. In fact, we can say that the probability that a randomly picked woman from the study population has a gestation period between 240 and 255 days is 0.196.\nAlso, note that the total area under a density histogram is \\[\n\\sum \\left( \\frac{n_j}{n w} \\cdot w \\right) = \\frac{\\sum  n_j}{n} = 1\n\\]\n\n\n\n\n\n\n\nLet’s now summarise what we have learned about density histograms:\n\nthe vertical scale is relative frequency / interval width\nthe total area under the histogram is 1\nthe proportion of data between \\(a\\) and \\(b\\) is the area under the histogram between \\(a\\) and \\(b\\)\n\n\n\n\n\n\n\n\n\nFigure 2: Density curve of the gestation period for the 10,000 women in the sample.\n\n\n\n\nIn Figure 2(c) we have superimposed an approximating smooth curve on top of the density histogram. Figure 2(d) displays only the approximating smooth curve. Furthermore, we have also highlighted in green the area under that curve in between the same limit as the histogram (\\(a = 240\\) and \\(b = 255\\)) and calculated the highlighted area. That area turned out to be 0.194, which is very close to the area of 0.196 computed from the density histogram.\nFrom this, it is quite easy to see how areas under the smooth curve in between two particular limits are in good agreement with the same areas computed using a density histogram, and thus with proportions of people in the study.\nSuch smooth curve is called a density curve or density function. The probability distribution of a continuous random variable \\(X\\) is specified in terms of its density function. We calculate the probability that the random variable \\(X\\) is between \\(a\\) and \\(b\\) by finding the area under the density curve between \\(a\\) and \\(b\\).\n\n\n\n\n\n\nLet’s now summarise what we have learned about density functions:\n\nThe probability distribution of a continuous random variable is specified in terms of a density function (i.e., a density curve).\nProbabilities are obtained as areas under the density curve.\n\nThe total area under the curve is one.\nThis is because the total probability must be one: \\(P(R_X) = 1\\).\n\n\nThe curve is always greater than or equal to zero for all possible values of the random variable.\nThis is similar to saying that probabilities cannot be negative.\n\n\n\n\n\nLast week we said that the probability distribution of a random variable \\(X\\) provides a concise representation of the random variable in terms of the set of all the possible values it can take and the corresponding probabilities. Clearly, this gives an immediate global picture of the random variable.\n\n\n\n\n\n\nProbability distribution for a continuous random variable\nThe probability distribution of a continuous random variable \\(X\\) provides the possible values of the random variable and their corresponding probability densities.\nA probability distribution can be in the form of a graph or mathematical formula.\n\n\n\nThe density curve is the graphical representation of the probability distribution, while the density function is the mathematical formula of the curve.\n\n\n\n\n\n\nHelp! I am slightly lost…\n\n\n\n\n\nTo revise the following topics click on the links below:\n\nSemester 1, Week 2: Frequency distributions\nSemester 1, Week 3: Histograms and densities"
  },
  {
    "objectID": "rd1_10.html#the-normal-distribution",
    "href": "rd1_10.html#the-normal-distribution",
    "title": "Continuous random variables",
    "section": "\n3 The normal distribution",
    "text": "3 The normal distribution\nA symmetric and bell-shaped density curve is called a normal density curve.\n\n\n\n\n\n\nNormal distribution\nA continuous random variable is said to be normally distributed, or to have a normal probability distribution, if its distribution has the shape of a normal curve (it is symmetric and bell-shaped).\n\n\n\nThe normal density function is the function used to calculate probabilities (as areas under the density curve) for a normally distributed random variable.\n\n\n\n\n\n\nKey question\nHow can we make sure that the normal curve can adapt to any symmetric and bell-shaped density histogram?\n\n\n\nEvery histogram can be centred at a different value and have a different spread. Hence, we need to specify:\n\nwhere the normal curve should be centred at;\nwhat should the spread of the normal curve be.\n\nFrom this we can see that the normal curve depends on two quantities called the parameters of the normal distribution (see Figure 3):\n\nthe mean \\(\\mu\\), specifying the centre of the distribution;\nthe standard deviation \\(\\sigma\\), specifying the spread of the distribution.\n\n\n\n\n\nFigure 3: Normal curves for different means and standard deviations.\n\n\n\nAs we can see, changing \\(\\mu\\) shifts the curve along the axis, while increasing \\(\\sigma\\) increases the spread and flattens the curve.\nThe formula for the normal density curve which we plotted above is provided below. However, you do not have to memorise it as this is already implemented in R: \\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}\n\\] for some values of \\(\\mu\\) and \\(\\sigma\\).\nThe same function in R is:\ndnorm(x, mean = &lt;mean&gt;, sd = &lt;sd&gt;)\nwhere:\n\n\nx represents the value of the random variable\n\nmean the mean of the normal curve\n\nsd the standard deviation of the curve\n\nWe write that a random variable \\(X\\) follows a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) as \\[\nX \\sim N(\\mu, \\sigma)\n\\]\nClearly, the mean and standard deviation of \\(X\\) will be \\(E(X) = \\mu\\) and \\(SD(X) = \\sigma\\) respectively.\nWe use lowercase letters such as \\(x, a, b\\) to denote an observed value, or a particular value, of the random variable \\(X\\)."
  },
  {
    "objectID": "rd1_10.html#the-68-95-99.7-rule",
    "href": "rd1_10.html#the-68-95-99.7-rule",
    "title": "Continuous random variables",
    "section": "\n4 The 68-95-99.7% rule",
    "text": "4 The 68-95-99.7% rule\nA variable which is normally distributed has:\n\n68% of the observations (values) within \\(1\\sigma\\) of the \\(\\mu\\)\n95% of its observations (values) within \\(2\\sigma\\) of the \\(\\mu\\)\n99.7% of its observations (values) within \\(3\\sigma\\) of the \\(\\mu\\)"
  },
  {
    "objectID": "rd1_10.html#obtaining-probabilities",
    "href": "rd1_10.html#obtaining-probabilities",
    "title": "Continuous random variables",
    "section": "\n5 Obtaining probabilities",
    "text": "5 Obtaining probabilities\nBefore describing how to compute the probabilities for a continuous random variable using R, we need to discuss an important property that only holds for continuous random variables:\n\n\n\n\n\n\nInterval endpoints for continuous random variables\nWhen performing calculations involving a continuous random variable, we do not have to worry about whether interval endpoints are included or not in the calculation: \\[\nP(a \\leq X \\leq b) = P(a \\leq X &lt; b) = P(a &lt; X \\leq b) = P(a &lt; X &lt; b)\n\\]\nThis is due to the fact that the probability of \\(X\\) being equal to a specific value is 0: \\[\n\\begin{aligned}\nP(X \\leq x) &= P(X &lt; x) + P(X = x) \\\\\n&= P(X &lt; x) + 0 \\\\\n&= P(X &lt; x)\n\\end{aligned}\n\\]\n\n\n\nWe compute areas under the normal curve using the function pnorm(x, mean, sd). This returns the area to the left of x in a normal curve centred at mean and having standard deviation sd: \\[\nP(X \\leq x) = P(X &lt; x) = \\texttt{pnorm}(x, \\texttt{ mean = } \\mu, \\texttt{ sd = } \\sigma)\n\\]\nThe following figure shows the type of areas returned by the pnorm function:\n\n\n\n\nR computes lower-tail probabilities.\n\n\n\nFor example, the probability that the random variable \\(X \\sim N(\\mu = 5, \\sigma = 2)\\) is less than 1 is:\n\npnorm(1, mean = 5, sd = 2)\n\n[1] 0.02275013\n\n\n\nThere are three types of probabilities that we might want to compute:\n\nThe probability that \\(X\\) is less than a given value: \\(P(X &lt; x)\\)\nThe probability that \\(X\\) is between a lower and an upper value: \\(P(x_l &lt; X &lt; x_u)\\)\nThe probability that \\(X\\) is greater than a given value: \\(P(X &gt; x)\\)\n\nWe will now analyse each case in turn and show how each of these is computed.\n\n5.1 1. Area to the left of a value \\(x\\)\n\nThe probability that \\(X &lt; x\\) is directly returned by pnorm():\n\n\n\n\n\n\n\n\n\n5.2 2. Area between the values \\(x_l\\) and \\(x_u\\)\n\nThe probability that \\(X\\) is between a lower (\\(x_l\\)) and an upper (\\(x_u\\)) value can be obtained as: \\[\nP(x_l &lt; X &lt; x_u) = P(X &lt; x_u) - P(X &lt; x_l)\n\\]\n\n\n\n\n\n\n\n\n\n5.3 3. Area to the right of \\(x\\)\n\nThe probability to the right of \\(x\\) is one minus the probability to the left of \\(x\\): \\[\nP(X &gt; x) = 1 - P(X &lt; x)\n\\]"
  },
  {
    "objectID": "rd1_10.html#the-inverse-problem-quantiles-and-percentiles",
    "href": "rd1_10.html#the-inverse-problem-quantiles-and-percentiles",
    "title": "Continuous random variables",
    "section": "\n6 The inverse problem: quantiles and percentiles",
    "text": "6 The inverse problem: quantiles and percentiles\nIn this section we will examine the inverse problem to the one of calculating the probability that \\(X &lt; x\\).\nGiven a probability \\(p\\), what’s the value \\(x\\) that cuts to its left a probability of \\(p\\)?\n\n\n\n\n\n\n\n\nThat value, called the \\(p\\)-quantile, is the value \\(x_p\\) for which the lower-tail probability is \\(p\\):\n\\[\n\\text{value }x_p\\text{ such that} \\qquad P(X \\leq x_p) = p\n\\]\nPercentiles and quantiles are two words used to express the same idea. We use percentile when speaking in terms of percentages and quantile when speaking in terms of proportions (or probabilities) between 0 and 1.\nThe 50th percentile (or \\(0.5\\)-quantile) is the value \\(x_{0.5}\\) that cuts a 0.5 probability to its left. You actually know that the value such that 50% of the observations are lower and 50% are higher is called the median.\n In R, you calculate the \\(p\\)-quantile with the function\nqnorm(p, mean, sd)\nThis will return the value \\(x\\) such that \\(P(X \\leq x) = p\\). That value is typically written \\(x_p\\).\n For example, if \\(X \\sim N(0, 1)\\), the value cutting an area of 0.025 to its left is\n\nqnorm(0.025, 0, 1)\n\n[1] -1.959964\n\n\nwhile that cutting an area of 0.975 to its left is\n\nqnorm(0.975, 0, 1)\n\n[1] 1.959964\n\n\nThe \\(0.025\\)-quantile is \\(x_{0.025} = -1.96\\), and the \\(0.975\\)-quantile is \\(x_{0.975} = 1.96\\)."
  },
  {
    "objectID": "rd1_10.html#working-in-standard-units",
    "href": "rd1_10.html#working-in-standard-units",
    "title": "Continuous random variables",
    "section": "\n7 Working in standard units",
    "text": "7 Working in standard units\nInstead of always specifying the mean and the standard deviation of the normal distribution, we can work with a reference normal distribution that has a mean equal to 0 and standard deviation equal to 1. This is known as the standard normal distribution.\n\n\n\n\n\n\nStandard normal distribution\nThe standard normal distribution, denoted \\(Z \\sim N(0, 1)\\), is a normal distribution with mean = 0 and standard deviation = 1.\n\n\n\nThe R functions dnorm(x, mean, sd) and pnorm(x, mean, sd) assume that mean = 0 and sd = 1 if not provided. In other words, they assume a standard normal distribution by default.\nFor example, the following two functions return the same output\n\ndnorm(2)\n\n[1] 0.05399097\n\ndnorm(2, mean = 0, sd = 1)\n\n[1] 0.05399097\n\n\nand similarly for pnorm().\nIn order to transform a value \\(x\\) from a normal distribution with mean = \\(\\mu\\) and sd = \\(\\sigma\\) to a score on the standard normal scale, we must use the z-score transformation.\nThe z-score measures distance in terms of the number of standard deviations from the mean: \\[\nz = \\frac{x - \\mu}{\\sigma} \\qquad (\\text{standard units or } z\\text{-}scores)\n\\]\n\n\n\n\n\n\nz-score\nThe z-score of \\(x\\) is the number of standard deviations \\(x\\) is from the mean.\n\n\n\n\nLet \\(X \\sim N(\\mu, \\sigma)\\) and \\(Z \\sim N(0, 1)\\). The following relation holds: \\[\nP(X &lt; x) = P \\left( \\frac{X - \\mu}{\\sigma} &lt; \\frac{x - \\mu}{\\sigma} \\right) = P(Z &lt; z)\n\\]\nThis means that the probability, when computed on the original normal distribution or on the standard normal distribution, will be the same as long as you use the z-score of \\(x\\) for the standard normal distribution.\n\nConsider the following cases:\n\nYou observe \\(x = 4.3\\) from a \\(N(3, 2)\\) distribution. The z-score is \\(z = (4.3 - 3) / 2 = 0.65\\)\n\nYou observe \\(x = 4.3\\) from a \\(N(3, 0.2)\\) distribution. The z-score is \\(z = (4.3 - 3) / 0.2 = 6.5\\)\n\n\nIn case (a), your observed value 4.3 is 0.65 standard deviations away from the mean. In case (b), the observed value 4.3 is 6.5 standard deviations away from the mean.\nObserving a value of 4.3 from a \\(N(3,2)\\) population is very likely, as 68% of the observations are within 1 \\(\\sigma\\) of the mean. However, observing a value of 4.3 from a \\(N(3,0.2)\\) distribution is very unlikely, as almost all the values will be within 3 \\(\\sigma\\) of the mean. Such a high z-score provides us with an element of surprise. We might want to inspect that value further as it could be an outlier or a typing error of who has entered the data.\nAs you can see, z-scores are particularly useful for comparing observations across distributions having a different mean and standard deviation."
  },
  {
    "objectID": "rd1_10.html#summary",
    "href": "rd1_10.html#summary",
    "title": "Continuous random variables",
    "section": "\n8 Summary",
    "text": "8 Summary\nWhen a histogram shows that a variable’s distribution is symmetric and bell-shaped, we can say that the variable is normally distributed and we can model the distribution with a mathematical curve called the normal density function.\nWe saw that the normal distribution depends on two parameters that control the centre and spread of the normal density curve:\n\nthe mean;\nthe standard deviation.\n\nThese two parameters make sure that the normal curve can fit histograms that have different centres and spreads.\nIn other words, if you know that a distribution is normal (shape), then the mean (centre) and standard deviation (spread) tell you everything else about the distribution.\nIn R, the equation of the normal density curve is given by the function dnorm(x, mean, sd).\nWe can use the normal curve to compute the probability of intervals as the area under the curve in that interval. The function to compute the area under a normal curve to the left of x is pnorm(x, mean, sd).\n\n\n\n\n\n\nProbability\nR command\n\n\n\nProbability of observing a value less than or equal x\n\npnorm(x, mean, sd)\n\n\nProbability of observing a value between xl and xu\n\npnorm(xu, mean, sd) - pnorm(xl, mean, sd)\n\n\nProbability of observing a value greater than xu\n\n1 - pnorm(xu, mean, sd)\n\n\n\nThe value \\(x_p\\) cutting an area = \\(p\\) to its left is returned by qnorm(p, mean, sd). This is called the \\(p\\)-quantile of \\(X\\)."
  },
  {
    "objectID": "rd1_10.html#glossary",
    "href": "rd1_10.html#glossary",
    "title": "Continuous random variables",
    "section": "\n9 Glossary",
    "text": "9 Glossary\n\n\nContinuous random variable. A variable that takes on numerical values determined by a chance process which can have any number of decimals.\n\nDensity function. The function which provides the probability that a random variable is within an interval in terms of the area under that curve between that interval.\n\nNormal distribution. The probability distribution of a random variable which has a symmetric and bell shaped density curve.\n\n\\(p\\)-quantile. The \\(x_p\\) value cutting an area = \\(p\\) to its left. In other words, the \\(x_p\\) value such that \\(P(X \\leq x_p) = p\\)."
  },
  {
    "objectID": "rd2_01.html",
    "href": "rd2_01.html",
    "title": "Confidence intervals",
    "section": "",
    "text": "Without loss of generality, we will consider the sample mean as the statistic of interest and we will consider two scenarios:\n\nEntire population data available (unlikely);\n\nEntire population data are NOT available (more common).\n\nFor both scenarios, we will investigate the following question:\n\nWhat was the average yearly salary of a National Football League (NFL) player in 2019?\n\nThis question is about the mean salary of all NFL players in 2019, which is denoted \\(\\mu\\) and is a population parameter."
  },
  {
    "objectID": "rd2_01.html#introduction",
    "href": "rd2_01.html#introduction",
    "title": "Confidence intervals",
    "section": "",
    "text": "Without loss of generality, we will consider the sample mean as the statistic of interest and we will consider two scenarios:\n\nEntire population data available (unlikely);\n\nEntire population data are NOT available (more common).\n\nFor both scenarios, we will investigate the following question:\n\nWhat was the average yearly salary of a National Football League (NFL) player in 2019?\n\nThis question is about the mean salary of all NFL players in 2019, which is denoted \\(\\mu\\) and is a population parameter."
  },
  {
    "objectID": "rd2_01.html#a.-population-data-available-unlikely",
    "href": "rd2_01.html#a.-population-data-available-unlikely",
    "title": "Confidence intervals",
    "section": "A. Population data available (unlikely)",
    "text": "A. Population data available (unlikely)\nIf we have data on the entire population of NFL players and we are interested in a parameter such as the population mean salary, we can simply compute the mean on the available population data. There would be no uncertainty and nothing to estimate: we have the entire population data, so we can easily compute the mean salary, and we would know what the actual parameter value is.\nIn this scenario, we would have a file containing the yearly salaries (in millions of dollars) for all players being paid in 2019 by a National Football League (NFL) team. This entire dataset represents the entire population of all National Football League players in that year.\nThe data on the entire population:\n\nlibrary(tidyverse)\n\nnfl_pop &lt;- read_csv(\"https://uoepsy.github.io/data/NFLContracts2019.csv\")\nhead(nfl_pop)\n\n# A tibble: 6 × 5\n  Player             Position Team     TotalMoney YearlySalary\n  &lt;chr&gt;              &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 Russell Wilson     QB       Seahawks        140         35  \n2 Ben Roethlisberger QB       Steelers         68         34  \n3 Aaron Rodgers      QB       Packers         134         33.5\n4 Jared Goff         QB       Rams            134         33.5\n5 Carson Wentz       QB       Eagles          128         32  \n6 Matt Ryan          QB       Falcons         150         30  \n\n\nThe population mean salary:\n\nmu &lt;- mean(nfl_pop$YearlySalary)\nmu\n\n[1] 3.033404\n\n\nSay we wanted to check whether 1 million was a plausible average yearly salary for a NFL player in 2019. The answer is no, it isn’t. We know the actual average salary which is 3.03 million of dollars, and this is different from 1 million."
  },
  {
    "objectID": "rd2_01.html#na.-population-data-not-available-more-common",
    "href": "rd2_01.html#na.-population-data-not-available-more-common",
    "title": "Confidence intervals",
    "section": "NA. Population data NOT available (more common)",
    "text": "NA. Population data NOT available (more common)\nNow suppose we didn’t know anything from the previous section.\nIn most research, the data for the entire population is not available as we cannot afford to collect data on the entire population. As a consequence, we cannot compute the population parameter \\(\\mu\\), which is unknown, and we must rely on random sampling to estimate it. It is good practice to use a sample size as large as we can afford in order to obtain good precision.\nLet’s pretend for now that we don’t have the entire NFL population data, and are only able to collect data for 50 players. Fortunately someone else did it for us: they chose 50 players at random and interviewed them to find out their salary. The sample data are:\n\nnfl_sample &lt;- read_csv(\"https://uoepsy.github.io/data/NFLSample2019.csv\")\nnfl_sample\n\n# A tibble: 50 × 5\n   Player          Position Team       TotalMoney YearlySalary\n   &lt;chr&gt;           &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n 1 Najee Goode     43OLB    Jaguars         0.805        0.805\n 2 Jack Crawford   43DT     Falcons         9.9          2.48 \n 3 Tra Carson      RB       Lions           1.23         0.615\n 4 Jordan Richards S        Ravens          0.805        0.805\n 5 Desmond Trufant CB       Falcons        68.8         13.8  \n 6 Alex Anzalone   43OLB    Saints          3.47         0.866\n 7 Trey Burton     TE       Bears          32            8    \n 8 Josh Martin     43DE     Saints          0.805        0.805\n 9 Lavonte David   ILB      Buccaneers     50.2         10.0  \n10 Maliek Collins  43DT     Cowboys         3.55         0.888\n# ℹ 40 more rows\n\n\nWe estimate the unknown population mean salary (what we are interested in) with the sample mean salary \\(\\bar x\\), which we can compute from the obtained sample. The sample mean salary represents our “best guess” of the population mean salary.\n\nxbar &lt;- mean(nfl_sample$YearlySalary)\nxbar\n\n[1] 3.35882\n\n\nThe average salary in the sample of 50 NFL players is 3.36 million dollars.\nWithout knowledge of the full data, we would say that we estimate the average salary of an NFL player in the year 2019 to be 3.36 million dollars.\nHowever, we know that if we had collected a different sample, the sample mean would be a different number from 3.36 as it varies from sample to sample.\nWe do not want to report a single number as the estimate, as we know that our estimate will almost certainly differ from the true value of the population parameter. We probably want to be cautious and report a range of plausible values for the parameter, known as confidence interval (CI), which is more likely to capture the true value of the parameter. See the following image for a summary of the idea:\n\n\n\n\nSource: moderndive.com\n\n\n\n\nTo get the range of plausible values, we will use the fact that in a Normal distribution 95% of the values are roughly between the mean - 2 SD and the mean + 2 SD.\nWe know that the sample mean follows a normal distribution, and we also know that the standard deviation of the sampling distribution of the mean is also known as the standard error (SE).\nHowever, we cannot obtain the standard error of the mean as we do not have the data on the entire population. If we used the formula for the standard error, we would need to know the population standard deviation \\(\\sigma\\) (which we don’t):\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\quad \\text{where}\n\\quad \\sigma = \\text{pop. standard deviation}\n\\]\nThis week we will learn how to estimate the standard error of a statistic when we only have one sample."
  },
  {
    "objectID": "rd2_01.html#confidence-intervals",
    "href": "rd2_01.html#confidence-intervals",
    "title": "Confidence intervals",
    "section": "\n2 Confidence intervals",
    "text": "2 Confidence intervals\nThis section investigates how to obtain the standard error and construct a confidence interval when only one sample is available.\nLet \\(\\mu\\) be the population mean and \\(\\sigma\\) the population standard deviation. In Week 11 of semester 1 you saw that\n\nthe sample mean \\(\\bar X\\) follows a normal distribution;\nits average \\(\\mu_{\\overline X}\\) is equal to the unknown population mean \\(\\mu\\);\nits standard deviation is equal to \\(\\sigma_{\\overline X} = SE = \\sigma / \\sqrt{n}\\), also known as the standard error of the mean.\n\n\\[\n\\bar X \\sim N(\\mu_{\\overline{X}}, \\sigma_{\\overline{X}}) \\quad \\text{where}\n\\quad \\begin{cases}\n\\mu_{\\overline{X}} = \\mu \\\\\n\\sigma_{\\overline{X}} = SE = \\frac{\\sigma}{\\sqrt{n}}\n\\end{cases}\n\\]\nProperty 1 tells us that if we computed the sample mean on many samples from the population and created a histogram, this would be bell-shaped like a Normal distribution. So, the sampling distribution of the mean is a Normal distribution.\nProperty 2 tells us that the sampling distribution of the mean is centred at the unknown population mean \\(\\mu\\). So, on average our sample means are close to the true but unknown population mean.\nProperty 3 tells us that the spread (standard deviation) of the sampling distribution is \\(\\sigma_{\\overline X} = \\sigma / \\sqrt{n}\\). The larger the sample size, the smaller the standard error, and the more precise the estimate is.\n\n\n\n\n\n\nFinding the central 95% probability\nRecall this image for a generic \\(X \\sim N(\\mu, \\sigma)\\) distribution:\n\n\n\n\n\n\n\n\nThe middle 95% probability lies between the values \\(x = \\mu - 2 \\sigma\\) and \\(x = \\mu + 2 \\sigma\\).\nTo be more precise, the values between which lies the central 95% probability are those quantiles cutting 0.025 probability to the left and 0.025 probability to the right (= 0.975 to the left).\n\nqnorm(c(0.025, 0.975))   # note that: 0.975 - 0.025 = 0.95\n\n[1] -1.959964  1.959964\n\n\nMeaning that 95% of the values in a normal distribution are between\n\\[\n[\\mu - 1.96 \\cdot \\sigma, \\\n\\mu + 1.96 \\cdot \\sigma]\n\\]\n\n\n\nIf we replace \\(\\mu\\) and \\(\\sigma\\) with \\(\\mu_{\\overline X}\\) and \\(\\sigma_{\\overline X}\\) in the interval above, we obtain the corresponding 95% confidence interval for the sample mean \\(\\bar X \\sim N(\\mu_{\\overline X}, \\sigma_{\\overline X})\\).\nThat is, roughly 95% of the values are between\n\\[\n[\\mu_{\\overline X} - 1.96 \\cdot \\sigma_{\\overline X},\\\n\\mu_{\\overline X} + 1.96 \\cdot \\sigma_{\\overline X}]\n\\]\nSubstituting the formula for the \\(SE = \\sigma_{\\overline X} = \\sigma / \\sqrt{n}\\), we know that 95% of the values are roughly between:\n\\[\n\\left[\n\\mu - 1.96 \\cdot \\frac{\\sigma}{\\sqrt n},\\  \n\\mu + 1.96 \\cdot \\frac{\\sigma}{\\sqrt n}\n\\right]\n\\]\nThe formula above is the 95% confidence interval for the population mean.\n\nThere is one problem though… The confidence interval depends on \\(\\mu\\) and \\(\\sigma\\), the population mean salary and population standard deviation.\nWhen we don’t have the entire population data and we can only afford ONE sample, we do not have \\(\\mu\\) and we also do not have \\(\\sigma\\). We estimated \\(\\mu\\) with the sample mean \\(\\bar x\\), and we must also estimate \\(\\sigma\\) with the sample standard deviation \\(s\\).\nThe standard error becomes:\n\\[\nSE = \\frac{s}{\\sqrt n} \\qquad \\text{where } s = \\text{sample standard deviation}\n\\]\nand the confidence interval for the population mean becomes:\n\\[\n\\left[\n\\bar x - 1.96 \\cdot \\frac{s}{\\sqrt n},\\  \n\\bar x + 1.96 \\cdot \\frac{s}{\\sqrt n}\n\\right]\n\\]\n\n\n\n\n\n\nHowever, that formula is not quite right! We estimated the unknown \\(\\sigma\\) with the sample standard deviation \\(s\\). This brings an extra element of uncertainty.\nBecause we are unsure about the actual value of the population standard deviation, the resulting distribution is no longer Normal, but a distribution that is more “uncertain” and places higher probability in the tails of the distribution, meaning that we have a wider range of values that we can observe (hence higher uncertainty).\nWhen the population standard deviation is unknown, the sample mean follows a t-distribution.\n\n\n\nThe t-distribution depends on a number called the degrees of freedom (DF) of the distribution, which is related to the sample size. The degrees of freedom is equal to the sample size - 1: \\[\ndf = n - 1\n\\] Because of this, we refer to the distribution of the sample mean as the \\(t(n-1)\\) distribution, and we write the degrees of freedom within parentheses.\nt-distributions with smaller degrees of freedom (corresponding to smaller samples) put more probability on the tails of the distribution, meaning more uncertainty. As the degree of freedom increases, the t-distribution is indistinguishable from the Normal distribution, and this happens approximately with df \\(\\geq\\) 30.\n\n\n\n\n\n\n\n\nBecause the distribution has changed, we need to find the new values in between which lies 95% of the probability. These are no longer -1.96 and 1.96 and will vary with the distribution.\nConsider a sample of size \\(n = 10\\). The corresponding df = 10 - 1 = 9. The values in between which lies the middle 0.95 probability are:\n\n# quantiles of a t-distribution with 9 df\nqt(c(0.025, 0.975), df = 9)\n\n[1] -2.262157  2.262157\n\n\nThese two values have, respectively, a probability of 0.025 to the left and 0.025 to the right.\nHence, for a t(9) distribution, 95% of the values are between:\n\\[\n\\left[\n\\bar x - 2.26 \\cdot \\frac{s}{\\sqrt n}\n,\\  \n\\bar x + 2.26 \\cdot \\frac{s}{\\sqrt n}\n\\right]\n\\]\nIn general, we denote the two quantiles of the t-distribution as \\(-t^*\\) and \\(+t^*\\), so the interval is written in general as:\n\\[\n\\left[\n\\bar x - t^* \\cdot \\frac{s}{\\sqrt n}\n,\\  \n\\bar x + t^* \\cdot \\frac{s}{\\sqrt n}\n\\right]\n\\]\n\n2.1 Example\nConsider again the sample of 50 NFL players. We want to report not only an estimate for the mean salary, but also a range of plausible values.\n\nnfl_sample\n\n# A tibble: 50 × 5\n   Player          Position Team       TotalMoney YearlySalary\n   &lt;chr&gt;           &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n 1 Najee Goode     43OLB    Jaguars         0.805        0.805\n 2 Jack Crawford   43DT     Falcons         9.9          2.48 \n 3 Tra Carson      RB       Lions           1.23         0.615\n 4 Jordan Richards S        Ravens          0.805        0.805\n 5 Desmond Trufant CB       Falcons        68.8         13.8  \n 6 Alex Anzalone   43OLB    Saints          3.47         0.866\n 7 Trey Burton     TE       Bears          32            8    \n 8 Josh Martin     43DE     Saints          0.805        0.805\n 9 Lavonte David   ILB      Buccaneers     50.2         10.0  \n10 Maliek Collins  43DT     Cowboys         3.55         0.888\n# ℹ 40 more rows\n\n\nWe need a few elements according to our discussion above:\n\n\n\\(\\bar{x}\\), the sample mean which we already computed before\n\n\\(s\\), the sample standard deviation\n\n\\(n\\), the sample size\nthe new multipliers to the SD based on the \\(t(n-1)\\) distribution\n\n\nxbar\n\n[1] 3.35882\n\ns &lt;- sd(nfl_sample$YearlySalary)\ns\n\n[1] 4.311738\n\nn &lt;- nrow(nfl_sample)\nn\n\n[1] 50\n\nqt(c(0.025, 0.975), df = n - 1)\n\n[1] -2.009575  2.009575\n\n\nThe formula to use:\n\\[\n\\left[\n\\bar x - 2.01 \\cdot \\frac{s}{\\sqrt n}\n,\\  \n\\bar x + 2.01 \\cdot \\frac{s}{\\sqrt n}\n\\right]\n\\]\nIn R:\n\nxbar - 2.01 * (s / sqrt(n))\n\n[1] 2.133179\n\nxbar + 2.01 * (s / sqrt(n))\n\n[1] 4.584461\n\n\nThe 95% confidence interval is then [2.13, 4.58] million dollars. We would write this up as:\n\n\n\n\n\n\nWe are 95% confident that the average salary of a NFL player in 2019 is between 2.13 and 4.58 million dollars.\n\n\n\n\n2.2 Interpreting a confidence interval?\nIf we were to do this whole process over and over again:\n\ntake a random sample of size \\(n\\);\nconstruct a 95% confidence interval.\n\nthen about 95% of the confidence intervals we created would contain the population mean. In turn, this also means that 5% of those intervals will not.\nSo if we did this 100 times, we would expect about five of our 95% confidence intervals to not contain the true population mean.\nAnd if we had been constructing 80% confidence intervals instead, we would expect roughly 80 of them to contain the population mean.\n\n\n\n\n\n\nWhen we have constructed a single interval, we do not say that the interval contains the population mean with probability of 95%, as this is wrong. We can only say that we are 95% confident it contains the population mean.\nWe speak about probability when we refer to a collection of confidence intervals. For example, if we have constructed one hundred 95% confidence intervals, then there is a probability of 0.95 that they (collectively) will contain the population parameter.\nOn the contrary, when we refer to just one confidence interval, we speak about confidence."
  },
  {
    "objectID": "rd2_01.html#glossary",
    "href": "rd2_01.html#glossary",
    "title": "Confidence intervals",
    "section": "\n3 Glossary",
    "text": "3 Glossary\n\n\nPopulation. The entire collection of units of interest.\n\nSample. A subset of the entire population.\n\nParameter. A fixed but typically unknown quantity describing the population.\n\nStatistic. A quantity computed on a sample.\n\nSampling distribution. The distribution of the values that a statistic takes on different samples of the same size and from the same population.\n\nStandard error. The standard error of a statistic is the standard deviation of the sampling distribution of the statistic.\n\nConfidence interval (CI). A range of plausible values around an estimate (e.g., a sample statistic), taking into account uncertainty in the statistic (e.g., sampling variability)\n\nConfidence level. The percentage of confidence intervals which will contain the true population parameter in the long run (i.e., if you sampled the population and constructed confidence intervals many times over). The proportion of all samples whose intervals contain the true parameter."
  },
  {
    "objectID": "rd2_03.html",
    "href": "rd2_03.html",
    "title": "Hypothesis testing: critical values",
    "section": "",
    "text": "Last week we saw that, to perform a hypothesis test, we need to:\n\nIdentify the null hypothesis, denoted \\(H_0\\).\nIdentify the alternative hypothesis, denoted \\(H_1\\).\nSelect the significance level, denoted \\(\\alpha\\). Typical values are 0.1, 0.05, or 0.01.\n\nCompute the test statistic. This is used to measure how consistent the data are with the null hypothesis.\n\nFor testing a mean we use the t-statistic, denoted \\(t\\), which takes the following form: \\[t = \\frac{\\bar x - \\mu_0}{s / \\sqrt{n}}\\]\n\n\n\nCompute the p-value.\n\nMake a decision:\n\nif p-value \\(\\leq \\alpha\\), reject \\(H_0\\).\nif p-value \\(&gt; \\alpha\\), fail to reject \\(H_0\\)."
  },
  {
    "objectID": "rd2_03.html#hypothesis-testing-p-value-method",
    "href": "rd2_03.html#hypothesis-testing-p-value-method",
    "title": "Hypothesis testing: critical values",
    "section": "",
    "text": "Last week we saw that, to perform a hypothesis test, we need to:\n\nIdentify the null hypothesis, denoted \\(H_0\\).\nIdentify the alternative hypothesis, denoted \\(H_1\\).\nSelect the significance level, denoted \\(\\alpha\\). Typical values are 0.1, 0.05, or 0.01.\n\nCompute the test statistic. This is used to measure how consistent the data are with the null hypothesis.\n\nFor testing a mean we use the t-statistic, denoted \\(t\\), which takes the following form: \\[t = \\frac{\\bar x - \\mu_0}{s / \\sqrt{n}}\\]\n\n\n\nCompute the p-value.\n\nMake a decision:\n\nif p-value \\(\\leq \\alpha\\), reject \\(H_0\\).\nif p-value \\(&gt; \\alpha\\), fail to reject \\(H_0\\)."
  },
  {
    "objectID": "rd2_03.html#hypothesis-testing-critical-value-method",
    "href": "rd2_03.html#hypothesis-testing-critical-value-method",
    "title": "Hypothesis testing: critical values",
    "section": "\n2 Hypothesis testing: critical value method",
    "text": "2 Hypothesis testing: critical value method\nThe method we used last week to perform a hypothesis test is called the p-value method because it reduces to comparing the area corresponding to the p-value with the area corresponding to \\(\\alpha\\). This week we investigate an equivalent approach, called the critical value method, which compares the t-statistics delimiting the \\(\\alpha\\) area with the observed t-statistic that is used to compute the p-value area.\nThe changes to the procedure are minimal:\n\nIdentify the null hypothesis, denoted \\(H_0\\).\nIdentify the alternative hypothesis, denoted \\(H_1\\).\nSelect the significance level, denoted \\(\\alpha\\). Typical values are 0.1, 0.05, or 0.01.\n\nCompute the test statistic. This is used to measure how consistent the data are with the null hypothesis.\n\nFor testing a mean we use the t-statistic, denoted \\(t\\), which takes the following form: \\[t = \\frac{\\bar x - \\mu_0}{s / \\sqrt{n}}\\]\n\n\n\nCompute the critical values.\n\nMake a decision:\n\nif the observed test statistic \\(t\\) lies beyond the critical values, reject \\(H_0\\).\nif the observed test statistic \\(t\\) lies within the critical values, fail to reject \\(H_0\\)."
  },
  {
    "objectID": "rd2_03.html#examples",
    "href": "rd2_03.html#examples",
    "title": "Hypothesis testing: critical values",
    "section": "\n3 Examples",
    "text": "3 Examples\n\n\n1. Two-sided alternative\n2. One-sided alternative: &lt;\n3. One-sided alternative: &gt;\n\n\n\nSuppose I give you a sample, for example:\n\nlibrary(tidyverse)\n\nsample_data &lt;- tibble(\n    score = c(0.1, 1, -0.2, 1.2)\n)\n\nUsing \\(\\alpha = 0.01\\), we wish to test whether the population the sample came from has a mean score that is different from 0:\n\\[H_0: \\mu = 0\\] \\[H_1: \\mu \\neq 0\\]\nSample mean:\n\nxbar &lt;- mean(sample_data$score)\nxbar\n\n[1] 0.525\n\n\nJust because the sample mean is different from 0, it doesn’t mean that the population mean necessarily must also be. This difference could perhaps only be due to random sampling variation.\n\ns &lt;- sd(sample_data$score)\ns\n\n[1] 0.6800735\n\nn &lt;- nrow(sample_data)\nn\n\n[1] 4\n\nSE &lt;- s / sqrt(n)\nSE\n\n[1] 0.3400368\n\nmu0 &lt;- 0   # hypothesised value for mu in H0\ntobs &lt;- (xbar - mu0) / SE\ntobs\n\n[1] 1.543951\n\n\nThe observed sample mean (\\(\\bar x =\\) 0.52) is 1.54 standard errors away from the hypothesised value of 0.\nWe need to compare the observed t-statistic (i.e. the one computed from the observed sample data) with the critical values from a t(3) distribution. These are the values that cut, respectively, an area of 0.005 to the left and 0.005 to the right.\nThose are the quantiles of a t-distribution, hence the function required is qt().\nIf we want to have 0.01 probability equally divided among both tails, we will have 0.01/2 = 0.005 in each tail. Remember that qt() uses the probability to the left by default.\n\ntstar &lt;- qt(c(0.005, 0.995), df = n-1)\ntstar\n\n[1] -5.840909  5.840909\n\n\n\n\nIn the t(3) distribution:\n\nThe middle 0.99 probability of the distribution lies in between the values \\(-t^*\\) = -5.84 and \\(t^*\\) = 5.84.\nTo the left of \\(-t^*\\) = -5.84 lies a probability of 0.005 = 0.01/2\nTo the right of \\(t^*\\) = 5.84 lies a probability of 0.005 = 0.01/2\nHence, below -5.84 and beyond 5.84 lies a total of 0.01 probability. The probability in the tails is then 0.01 = \\(\\alpha\\).\n\n\n\n\n\n\n\n\n\nVisually\n\n\n\n\n\nYou don’t need to plot a t-distribution usually, but if you want to, here is how you could do this. First, create a sequence of t values, this is an arbitrary choice of points you want to plot the t distribution for. You can either go from -5 to 5, or -10 to 10, as you wish. Then compute the t density for each value. You would only use the function dt() for plotting the t distribution, in fact this function doesn’t calculate a probability. Remember that a probability is an area, and it is computed by pt().\n\nplt &lt;- tibble(\n    t_values = seq(-10, 10, by = 0.1),\n    t_dens = dt(t_values, df = n-1)\n)\n\nggplot(plt, aes(x = t_values, y = t_dens)) +\n    geom_line() +\n    labs(x = 't values', y = 'Density')\n\n\n\n\n\n\n\nWe can also add the critical values that cut an area of 0.005 to the left and 0.005 to the right. We computed those earlier, remember?\n\ntstar &lt;- qt(c(0.005, 0.995), df = n-1)\ntstar\n\n[1] -5.840909  5.840909\n\n\nWe could add those as follows:\n\nggplot(plt, aes(x = t_values, y = t_dens)) +\n    geom_line() +\n    geom_vline(xintercept = tstar, col = 'red') +\n    labs(x = 't values', y = 'Density')\n\n\n\n\n\n\n\nAny t-statistic smaller than -6 or any t-statistic larger than 6 falls into the area corresponding to the significance level \\(\\alpha = 0.01\\).\nRemember that we use \\(\\alpha\\) to control the probability of the t-statistics in the null distribution that are considered “unlikely enough” to occur if \\(H_0\\) were true.\nWhere does our observed t-statistic lie?\n\nggplot(plt, aes(x = t_values, y = t_dens)) +\n    geom_line() +\n    geom_vline(xintercept = tstar, col = 'red') +\n    geom_vline(xintercept = tobs, col = 'darkgreen') +\n    labs(x = 't values', y = 'Density')\n\n\n\n\n\n\n\n\n\n\nThe observed t-statistic lies in between the critical values, rather than beyond. Hence, we fail to reject \\(H_0\\) at the 1% significance level.\nIf you were to compute the p-value, this would be larger than \\(\\alpha = 0.01\\).\n\ntobs\n\n[1] 1.543951\n\ntstar\n\n[1] -5.840909  5.840909\n\n\nIf you don’t like to check it visually by looking at the number, you can get R to check it for you:\n\ntobs &lt;= tstar[1]\n\n[1] FALSE\n\ntobs &gt;= tstar[2]\n\n[1] FALSE\n\n\n\n\nSuppose you were given a different sample, such as\n\nsample_data &lt;- tibble(\n    score = c(-2.1, -5.9, -3.8, -4.3)\n)\n\nUsing \\(\\alpha = 0.05\\), we wish to test whether the population it came from has a mean score that is less than 0:\n\\[H_0: \\mu = 0\\] \\[H_1: \\mu &lt; 0\\]\n\nxbar &lt;- mean(sample_data$score)\nxbar\n\n[1] -4.025\n\n\nCompute the observed value of the t-statistic:\n\ns &lt;- sd(sample_data$score)\ns\n\n[1] 1.564981\n\nn &lt;- nrow(sample_data)\nn\n\n[1] 4\n\nSE &lt;- s / sqrt(n)\nSE\n\n[1] 0.7824907\n\nmu0 &lt;- 0   # hypothesised value for mu in H0\ntobs &lt;- (xbar - mu0) / SE\ntobs\n\n[1] -5.143831\n\n\nCompute the critical value. In a one-sided hypothesis test there is only one critical value as the entire \\(\\alpha\\) probability (= 0.05 in this case) is assigned all in one tail.\nIn this case \\(H_1 : \\mu &lt; 0\\) so \\(\\alpha\\) goes all in the left tail. This is because t-statistics that are much smaller than the hypothesised value, i.e. 0, will be considered as providing strong evidence against the null hypothesis.\n\ntstar &lt;- qt(0.05, df = n - 1)\ntstar\n\n[1] -2.353363\n\n\nCompare the observed value of the t-statistic with the critical value:\n\ntobs\n\n[1] -5.143831\n\ntstar\n\n[1] -2.353363\n\n\nThe observed t-statistic is smaller than the critical value, so we reject \\(H_0\\) in favour of the alternative.\n\ntobs &lt;= tstar\n\n[1] TRUE\n\n\nAt the 5% significance level, the sample data provide strong evidence against the null hypothesis that the sample came from a population with a mean of 0, and in favour of the alternative that the population mean is less than 0.\nAs the observed \\(t\\) = -5.14 is smaller than the critical value \\(t^*\\) = -2.35, the p-value would also be smaller than \\(\\alpha = 0.05\\).\n\n\nSuppose you were given a different sample, such as\n\nsample_data &lt;- tibble(\n    score = c(2.1, 0.3, -0.9, 1.1)\n)\n\nUsing \\(\\alpha = 0.05\\), we wish to test whether the population it came from has a mean score that is larger than 0:\n\\[H_0: \\mu = 0\\] \\[H_1: \\mu &gt; 0\\]\n\nxbar &lt;- mean(sample_data$score)\nxbar\n\n[1] 0.65\n\n\nCompute the observed value of the t-statistic:\n\ns &lt;- sd(sample_data$score)\ns\n\n[1] 1.268858\n\nn &lt;- nrow(sample_data)\nn\n\n[1] 4\n\nSE &lt;- s / sqrt(n)\nSE\n\n[1] 0.6344289\n\nmu0 &lt;- 0   # hypothesised value for mu in H0\ntobs &lt;- (xbar - mu0) / SE\ntobs\n\n[1] 1.024544\n\n\nCompute the critical value. In a one-sided hypothesis test there is only one critical value as the entire \\(\\alpha\\) probability (= 0.05 in this case) is assigned all in one tail.\nIn this case \\(H_1 : \\mu &gt; 0\\) so \\(\\alpha\\) goes all in the right tail. This is because t-statistics that are much larger than the hypothesised value, i.e. 0, will be considered as providing strong evidence against the null hypothesis.\n\ntstar &lt;- qt(0.95, df = n - 1)\ntstar\n\n[1] 2.353363\n\n\nCompare the observed value of the t-statistic with the critical value:\n\ntobs\n\n[1] 1.024544\n\ntstar\n\n[1] 2.353363\n\n\nThe observed t-statistic is not more extreme than the critical value. It is smaller than the critical value, so we fail to reject \\(H_0\\). The sample data are consistent with the null hypothesis.\n\ntobs &gt;= tstar\n\n[1] FALSE\n\n\nAt the 5% significance level, the sample data do not provide sufficient evidence against the null hypothesis and hence we fail to reject the null that the sample came from a population with a mean of 0.\nIf you were to compute the p-value, as the observed t-statistic is smaller than the critical value, the p-value would be larger than \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "rd2_03.html#summary",
    "href": "rd2_03.html#summary",
    "title": "Hypothesis testing: critical values",
    "section": "\n4 Summary",
    "text": "4 Summary\n\nWe have learned to assess how much evidence the sample data bring against the null hypothesis and in favour of the alternative hypothesis.\nThe null hypothesis, denoted \\(H_0\\), is a claim about a population parameter that is initially assumed to be true. It typically represents “no effect” or “no difference between groups”.\nThe alternative hypothesis, denoted \\(H_1\\), is the claim we seek evidence for.\n\nWe performed a hypothesis test against \\(H_0\\) (and in favour of \\(H_1\\)) following these steps:\n\nFormally state your null and alternative hypotheses using precise symbols\nSelect a significance level for the test.\nConsider the distribution of the t-statistics when \\(H_0\\) is true\nCompute the observed value of the t-statistic in our sample\nObtain the critical values\n\nCompare the observed t-statistic with the critical values. If it lies beyond, reject the null hypothesis."
  },
  {
    "objectID": "rd2_03.html#worked-example",
    "href": "rd2_03.html#worked-example",
    "title": "Hypothesis testing: critical values",
    "section": "\n5 Worked example",
    "text": "5 Worked example\nIn this week’s exercises you will perform the same test of hypothesis as last week’s lab, but using the critical value method rather than the p-value method.\nA 2011 study by Courchesne et al.1 examined brain tissue of seven autistic male children between the ages of 2 and 16. The mean number of neurons in the prefrontal cortex in non-autistic male children of the same age is about 1.15 billion. The prefrontal cortex is the part of the brain most disrupted in autism, as it deals with language and social communication.\n\nResearch question\nDo autistic male children have more neurons (on average) in the prefrontal cortex than non-autistic children?\n\nThat is, we wish to test:\n\\[H_0 : \\mu = 1.15\\] \\[H_1 : \\mu &gt; 1.15\\]\nwhere \\(\\mu\\) is the mean number of neurons (in billions) in the prefrontal cortex for all autistic male children.\nIn the following you will use a significance level \\(\\alpha = 0.05\\).\n\n\n\n\n\n\nData Codebook\n\n\n\n\n\nDownload link\nThe data can be found at this address: https://uoepsy.github.io/data/NeuronCounts.csv\nPreview\n\n\n\n\n\n\n\nCase\n      Age\n      PFC_NC\n    \n\n\n1\n2\n2.42\n\n\n2\n3\n1.80\n\n\n3\n3\n2.21\n\n\n4\n4\n2.18\n\n\n5\n7\n1.28\n\n\n6\n8\n1.59\n\n\n7\n16\n2.09\n\n\n\n\n\n\nCodebook\n\nThe first column, Case, is an anonymised index used to identify each child.\nThe second column, Age, records the age of each child.\nThe last column, PFC_NC, contains the prefrontal cortex neuron counts (in billions).\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1. Read the data into R.\n\n\n\n\nlibrary(tidyverse)\nautism &lt;- read_csv('https://uoepsy.github.io/data/NeuronCounts.csv')\nautism\n\n# A tibble: 7 × 3\n   Case   Age PFC_NC\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1     2   2.42\n2     2     3   1.8 \n3     3     3   2.21\n4     4     4   2.18\n5     5     7   1.28\n6     6     8   1.59\n7     7    16   2.09\n\n\n\n\n\n\n\n\nQuestion 2. Compute the value of the t-statistic for the observed sample.\n\n\n\nRecall the formula for the t-statistic:\n\\[\nt = \\frac{\\bar x - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\]\nwhere\n\n\n\\(\\bar x\\) is the sample average number of neurons in the prefrontal cortex\n\n\\(\\mu_0\\) is the hypothesised value for the population parameter found in \\(H_0\\)\n\n\n\\(s\\) is the sample standard deviation\n\n\\(n\\) is the sample size\n\nHence, the value of the t-statistic for the observed sample is given by:\n\nxbar &lt;- mean(autism$PFC_NC)\nxbar\n\n[1] 1.938571\n\ns &lt;- sd(autism$PFC_NC)\nn &lt;- nrow(autism)\nSE &lt;- s / sqrt(n)\n\nmu0 &lt;- 1.15\n\ntobs &lt;- (xbar - mu0) / SE\ntobs\n\n[1] 5.212963\n\n\nThe sample mean neuron count, 1.94 billion, is 5.21 standard errors larger than the hypothesised value.\n\n\n\n\n\n\nQuestion 3. Identify the null distribution, i.e. the distribution of the t-statistic when \\(H_0\\) is true. Compute the critical value(s) for the null distribution using the appropriate significance level.\n\n\n\nWhen \\(H_0\\) is true, the t-statistic \\(t = (\\bar x - \\mu_0)/(s / \\sqrt{n})\\) follows a t(6) distribution, where the degrees of freedom are computed as df = n - 1 = 7 - 1 = 6.\nThe critical value is the t-value cutting an area of 0.05 to its right, i.e. the following quantile\n\ntstar &lt;- qt(0.95, df = n - 1)\ntstar\n\n[1] 1.94318\n\n\n\n\n\n\n\n\nQuestion 4. Make a decision.\n\n\n\nMaking a decision using the critical value method entails comparing the observed t-statistic with the critical value.\n\ntobs\n\n[1] 5.212963\n\ntstar\n\n[1] 1.94318\n\n\nThe observed t-statistic is larger than the critical value for a 5% significance level, hence we reject the null hypothesis.\n\ntobs &gt;= tstar\n\n[1] TRUE\n\n\nIf you were to compute the p-value, i.e. the probability of observing a t-statistic as large as, or larger, than the observed one, if the null hypothesis were true, you would obtain a value that is smaller than \\(\\alpha = 0.05\\).\n\n\n\n\n\n\nQuestion 5. Write up your results in the context of the research question.\n\n\n\n\n\n\n\n\n\nSide note\n\n\n\n\n\nWhen you use the critical value method, you don’t have a computed p-value, so the reporting will only say whether p &lt; .05 or p &gt; .05 depending on whether the observed t-statistic is beyond or not beyond the critical value(s).\n\nIf \\(t &gt; t^*\\), p &lt; .05 (if you use a different \\(\\alpha\\), change accordingly)\nIf \\(t &lt; t^*\\), p &gt; .05 (if you use a different \\(\\alpha\\), change accordingly)\n\n\n\n\nThe estimated mean number of neurons in the prefrontal cortex of male autistic children is 1.94 billion, with a standard error of 0.15 billion.\nAt the 5% significance level, we performed a one-sided test of significance against the null hypothesis that the mean number of neurons in the prefrontal cortex of all male autistic children was equal to that of all non-autistic male children. The sample results indicate that there is very strong evidence that the mean number of neurons in the prefrontal cortex may be larger for autistic compared to non-autistic male children: \\(t(6) = 5.21, p &lt; .05\\), one-sided."
  },
  {
    "objectID": "rd2_03.html#footnotes",
    "href": "rd2_03.html#footnotes",
    "title": "Hypothesis testing: critical values",
    "section": "Footnotes",
    "text": "Footnotes\n\nCourchesne, E., et al., “Neuron Number and Size in Prefrontal Cortex of Children with Autism,” Journal of the American Medical Association, November 2011;306(18): 2001–2010.↩︎"
  },
  {
    "objectID": "rd2_05.html",
    "href": "rd2_05.html",
    "title": "Errors, Power, Effect size, Assumptions",
    "section": "",
    "text": "You will be using the following dataset: https://uoepsy.github.io/data/students_reading_scores.csv\nThese contain measurements of age (in years) and reading scores (0 - 100) for 35 university students randomly sampled from a hypothetical university.\n\n\n\n\n\n\nQuestion 1. Read the data into R.\n\n\n\n\nlibrary(tidyverse)\nstudents &lt;- read_csv(\"https://uoepsy.github.io/data/students_reading_scores.csv\")\nhead(students)\n\n# A tibble: 6 × 2\n    age read_score\n  &lt;dbl&gt;      &lt;dbl&gt;\n1    28         84\n2   -99         75\n3    27         NA\n4    29         NA\n5    24         52\n6    25         23\n\n\n\n\n\n\n\n\nQuestion 2. Perform preliminary checks on the data by:\n\nplotting the variables in the dataset;\ncomputing a table of descriptive statistics.\n\n\n\n\nPlot the variables in the dataset:\n\nlibrary(patchwork)\n\nplt_age &lt;- ggplot(students, aes(x = age)) +\n    geom_histogram()\n\nplt_read &lt;- ggplot(students, aes(x = read_score)) +\n    geom_histogram()\n\nplt_age | plt_read\n\n\n\n\n\n\n\nTable of descriptive statistics:\n\nstudents %&gt;%\n    pivot_longer(cols = c(age, read_score), names_to = \"Variable\", values_to = \"Values\") %&gt;%\n    group_by(Variable) %&gt;%\n    summarise(M = mean(Values), \n              SD = sd(Values),\n              Min = min(Values),\n              Med = median(Values),\n              Max = max(Values),\n              NMiss = sum(is.na(Values)))\n\n# A tibble: 2 × 7\n  Variable       M    SD   Min   Med   Max NMiss\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 age        -9.49  57.5   -99    24    32     0\n2 read_score NA     NA      NA    NA    NA    10\n\n\n\n\n\n\n\n\nQuestion 3. What do you notice? Are there any issues with the data that need fixing? If yes, make appropriate changes to the data to fix those issues.\n\n\n\nIt looks like some students had their age stored as “-99”. This is an impossible value for age, and it was used by those entering the data as a missing value; i.e. when a student preferred to not disclose that information. The values “-99” should be replaced to be NAs.\nThe reading scores for some students were also missing but, if you look at the CSV file itself you will see that those were stored as spaces rather than -99s. This is in the CSV file, and not the object created into R when you read the CSV file data into R.\nHowever, R is smart enough that when you read CSV data into R and there are spaces, those are interpreted as missing values and converted to NAs automatically.\nAlternative 1:\n\nstudents$age[students$age == -99] &lt;- NA\n\nAlternative 2:\n\nstudents &lt;- students %&gt;%\n    mutate(\n        age = ifelse(age == -99, NA, age)\n    )\n\nhead(students)\n\n# A tibble: 6 × 2\n    age read_score\n  &lt;dbl&gt;      &lt;dbl&gt;\n1    28         84\n2    NA         75\n3    27         NA\n4    29         NA\n5    24         52\n6    25         23\n\n\nPlot the variables in the dataset:\n\nlibrary(patchwork)\n\nplt_age &lt;- ggplot(students, aes(x = age)) +\n    geom_histogram()\n\nplt_read &lt;- ggplot(students, aes(x = read_score)) +\n    geom_histogram()\n\nplt_age | plt_read\n\n\n\n\n\n\n\nTable of descriptive statistics:\n\nstudents %&gt;%\n    pivot_longer(cols = c(age, read_score), names_to = \"Variable\", values_to = \"Values\") %&gt;%\n    group_by(Variable) %&gt;%\n    summarise(M = mean(Values), \n              SD = sd(Values),\n              Min = min(Values),\n              Med = median(Values),\n              Max = max(Values),\n              NMiss = sum(is.na(Values)))\n\n# A tibble: 2 × 7\n  Variable       M    SD   Min   Med   Max NMiss\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 age           NA    NA    NA    NA    NA    10\n2 read_score    NA    NA    NA    NA    NA    10\n\n\nThe table still has NAs, because when you compute the mean for example, if you add a number to NA, you get NA. For example, 2 + NA = NA.\nTo tell R to ignore NAs when computing the mean, you say na.rm = TRUE.\n\nstudents %&gt;%\n    pivot_longer(cols = c(age, read_score), names_to = \"Variable\", values_to = \"Values\") %&gt;%\n    group_by(Variable) %&gt;%\n    summarise(M = mean(Values, na.rm = TRUE), \n              SD = sd(Values, na.rm = TRUE),\n              Min = min(Values, na.rm = TRUE),\n              Med = median(Values, na.rm = TRUE),\n              Max = max(Values, na.rm = TRUE),\n              NMiss = sum(is.na(Values)))\n\n# A tibble: 2 × 7\n  Variable       M    SD   Min   Med   Max NMiss\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 age         26.3  3.82    19    26    32    10\n2 read_score  59   17.6     23    59    92    10\n\n\n\n\n\n\n\n\nQuestion 4. We will only be using the reading scores. Remove from the data any rows for which the reading score is missing.\n\n\n\nAlternative 1:\n\nstudents_tidy &lt;- students %&gt;%\n    drop_na(read_score)\n\nhead(students_tidy)\n\n# A tibble: 6 × 2\n    age read_score\n  &lt;dbl&gt;      &lt;dbl&gt;\n1    28         84\n2    NA         75\n3    24         52\n4    25         23\n5    23         40\n6    24         58\n\n\nAlternative 2:\n\nstudents_tidy &lt;- students %&gt;%\n    filter(complete.cases(read_score))\n\nIf you used na.omit(students) or drop_na(), you would remove students having NAs also for age. However, students may not have provided their age, but we can have their reading score, so if we were to do this, we would throw away available data!\n\n\n\n\n\n\nQuestion 5. At the 5% significance level, test whether the sample data provide significance evidence that the mean reading score for all students in that university is not equal to 50.\n\n\n\n\nxbar &lt;- mean(students_tidy$read_score)\nxbar\n\n[1] 59\n\nn &lt;- nrow(students_tidy)\nn\n\n[1] 25\n\ns &lt;- sd(students_tidy$read_score)\ns\n\n[1] 17.57365\n\nse &lt;- s / sqrt(n)\nse\n\n[1] 3.514731\n\n\nObserved t-statistic\n\nmu0 &lt;- 50\ntobs &lt;- (xbar - mu0) / se\ntobs\n\n[1] 2.560651\n\n\nP-value = 2 * Probability to the right of tobs:\n\n2 * pt(abs(tobs), df = n - 1, lower.tail = FALSE)\n\n[1] 0.01716085\n\n\nThe sample results are significant, so it’s good practice to follow up with a confidence interval\n\ntstar &lt;- qt(c(0.025, 0.975), df = n - 1)\ntstar\n\n[1] -2.063899  2.063899\n\nci &lt;- xbar + tstar * se\nci\n\n[1] 51.74595 66.25405\n\n\n\n\n\n\n\n\nQuestion 6. The t-test for one population mean relies on some assumptions for the results to be valid. Check whether these are satisfied or not in this sample.\n\n\n\nThe data come from a random sample of students from that university, hence the independence assumption is satisfied.\nThe sample size - that is, the number of data points that are not missing - is \\(n = 25\\). This is not a large enough sample size for the sample mean to be normally distributed, so we will check whether the sample came from a population that follows a normal distribution. If this is the case, then the sample mean would be normally distributed irrespectively of the sample size.\n\nplt1 &lt;- ggplot(students_tidy, aes(x = read_score)) +\n    geom_density() +\n    labs(x = \"Reading scores (0-100)\", title = \"(a) Density plot\")\n\nplt2 &lt;- ggplot(students_tidy, aes(sample = read_score)) +\n    geom_qq() +\n    geom_qq_line() +\n    labs(x = \"Theoretical quantiles\", y = \"Sample quantiles\", title = \"(b) qqplot\")\n\nplt1 | plt2\n\n\n\nFigure 1: Distribution of reading scores\n\n\n\nThe density plot highlights a roughly bell-shaped distribution with a single peak near 60. The qqplot doesn’t highlight any severe departures from normality, in fact the sample quantiles follow the theoretical quantiles from a normal distribution pretty closely.\nAlternatively, you can also obtain the qqplot as follows:\n\nqqnorm(students_tidy$read_score)\nqqline(students_tidy$read_score)\n\n\n\n\n\n\n\nWe can also perform a Shapiro-Wilk test:\n\nshapiro.test(students_tidy$read_score)\n\n\n    Shapiro-Wilk normality test\n\ndata:  students_tidy$read_score\nW = 0.97792, p-value = 0.8411\n\n\n\n\n\n\n\n\nQuestion 7. Provide a write up pf your results in the context of the study.\n\n\n\nData were obtained from https://uoepsy.github.io/data/students_reading_scores.csv. The dataset contains measurements on age and reading scores for 35 randomly sampled students from a hypothetical university. Due to random sampling of the students, the independence assumption is satisfied. Some students had missing values for their reading scores, so those students were dropped from the analysis, leaving a sample of 25 students for the analysis. A Shapiro-Wilk normality test was performed at the 5% significance level (\\(W = 0.98, p = 0.84\\)). The sample data do not provide sufficient evidence to reject the null hypothesis of normality of the population data. Furthermore, the density plot shown in Figure 1(a) highlights a roughly bell-shaped distribution, and the qqplot in Figure 1(b) doesn’t show violations from normality.\nAt the 5% significance level, we performed a hypothesis test against the null hypothesis that the mean reading score of all students in that university was equal to 50. The sample data provide very strong evidence to reject the null hypothesis in favour of the alternative one that the population mean reading score is different from 50: \\(t(24) = 2.56, p = 0.02\\), two-sided.\nWe are 95% confident that the mean reading score for all students in that university is between 52 and 66.\n\n\n\n\n\n\nQuestion 8. Is there a significant effect? Is the effect important?\n\n\n\nYes, there is a significant effect, as we found a p-value smaller than the significance level of 5%. However, this is no guarantee that the effect is of practical importance.\nTo judge whether the effect is important, we need to compute the effect size, i.e. Cohen’s D:\n\nD &lt;- (xbar - mu0) / s\nD\n\n[1] 0.5121302\n\n\nCohen’D highlights a medium effect size, highlighting that the sample result is not only significant, but perhaps also of moderate importance and worth noting."
  },
  {
    "objectID": "rd2_05.html#worked-example-t-test-effect-size-assumptions",
    "href": "rd2_05.html#worked-example-t-test-effect-size-assumptions",
    "title": "Errors, Power, Effect size, Assumptions",
    "section": "",
    "text": "You will be using the following dataset: https://uoepsy.github.io/data/students_reading_scores.csv\nThese contain measurements of age (in years) and reading scores (0 - 100) for 35 university students randomly sampled from a hypothetical university.\n\n\n\n\n\n\nQuestion 1. Read the data into R.\n\n\n\n\nlibrary(tidyverse)\nstudents &lt;- read_csv(\"https://uoepsy.github.io/data/students_reading_scores.csv\")\nhead(students)\n\n# A tibble: 6 × 2\n    age read_score\n  &lt;dbl&gt;      &lt;dbl&gt;\n1    28         84\n2   -99         75\n3    27         NA\n4    29         NA\n5    24         52\n6    25         23\n\n\n\n\n\n\n\n\nQuestion 2. Perform preliminary checks on the data by:\n\nplotting the variables in the dataset;\ncomputing a table of descriptive statistics.\n\n\n\n\nPlot the variables in the dataset:\n\nlibrary(patchwork)\n\nplt_age &lt;- ggplot(students, aes(x = age)) +\n    geom_histogram()\n\nplt_read &lt;- ggplot(students, aes(x = read_score)) +\n    geom_histogram()\n\nplt_age | plt_read\n\n\n\n\n\n\n\nTable of descriptive statistics:\n\nstudents %&gt;%\n    pivot_longer(cols = c(age, read_score), names_to = \"Variable\", values_to = \"Values\") %&gt;%\n    group_by(Variable) %&gt;%\n    summarise(M = mean(Values), \n              SD = sd(Values),\n              Min = min(Values),\n              Med = median(Values),\n              Max = max(Values),\n              NMiss = sum(is.na(Values)))\n\n# A tibble: 2 × 7\n  Variable       M    SD   Min   Med   Max NMiss\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 age        -9.49  57.5   -99    24    32     0\n2 read_score NA     NA      NA    NA    NA    10\n\n\n\n\n\n\n\n\nQuestion 3. What do you notice? Are there any issues with the data that need fixing? If yes, make appropriate changes to the data to fix those issues.\n\n\n\nIt looks like some students had their age stored as “-99”. This is an impossible value for age, and it was used by those entering the data as a missing value; i.e. when a student preferred to not disclose that information. The values “-99” should be replaced to be NAs.\nThe reading scores for some students were also missing but, if you look at the CSV file itself you will see that those were stored as spaces rather than -99s. This is in the CSV file, and not the object created into R when you read the CSV file data into R.\nHowever, R is smart enough that when you read CSV data into R and there are spaces, those are interpreted as missing values and converted to NAs automatically.\nAlternative 1:\n\nstudents$age[students$age == -99] &lt;- NA\n\nAlternative 2:\n\nstudents &lt;- students %&gt;%\n    mutate(\n        age = ifelse(age == -99, NA, age)\n    )\n\nhead(students)\n\n# A tibble: 6 × 2\n    age read_score\n  &lt;dbl&gt;      &lt;dbl&gt;\n1    28         84\n2    NA         75\n3    27         NA\n4    29         NA\n5    24         52\n6    25         23\n\n\nPlot the variables in the dataset:\n\nlibrary(patchwork)\n\nplt_age &lt;- ggplot(students, aes(x = age)) +\n    geom_histogram()\n\nplt_read &lt;- ggplot(students, aes(x = read_score)) +\n    geom_histogram()\n\nplt_age | plt_read\n\n\n\n\n\n\n\nTable of descriptive statistics:\n\nstudents %&gt;%\n    pivot_longer(cols = c(age, read_score), names_to = \"Variable\", values_to = \"Values\") %&gt;%\n    group_by(Variable) %&gt;%\n    summarise(M = mean(Values), \n              SD = sd(Values),\n              Min = min(Values),\n              Med = median(Values),\n              Max = max(Values),\n              NMiss = sum(is.na(Values)))\n\n# A tibble: 2 × 7\n  Variable       M    SD   Min   Med   Max NMiss\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 age           NA    NA    NA    NA    NA    10\n2 read_score    NA    NA    NA    NA    NA    10\n\n\nThe table still has NAs, because when you compute the mean for example, if you add a number to NA, you get NA. For example, 2 + NA = NA.\nTo tell R to ignore NAs when computing the mean, you say na.rm = TRUE.\n\nstudents %&gt;%\n    pivot_longer(cols = c(age, read_score), names_to = \"Variable\", values_to = \"Values\") %&gt;%\n    group_by(Variable) %&gt;%\n    summarise(M = mean(Values, na.rm = TRUE), \n              SD = sd(Values, na.rm = TRUE),\n              Min = min(Values, na.rm = TRUE),\n              Med = median(Values, na.rm = TRUE),\n              Max = max(Values, na.rm = TRUE),\n              NMiss = sum(is.na(Values)))\n\n# A tibble: 2 × 7\n  Variable       M    SD   Min   Med   Max NMiss\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 age         26.3  3.82    19    26    32    10\n2 read_score  59   17.6     23    59    92    10\n\n\n\n\n\n\n\n\nQuestion 4. We will only be using the reading scores. Remove from the data any rows for which the reading score is missing.\n\n\n\nAlternative 1:\n\nstudents_tidy &lt;- students %&gt;%\n    drop_na(read_score)\n\nhead(students_tidy)\n\n# A tibble: 6 × 2\n    age read_score\n  &lt;dbl&gt;      &lt;dbl&gt;\n1    28         84\n2    NA         75\n3    24         52\n4    25         23\n5    23         40\n6    24         58\n\n\nAlternative 2:\n\nstudents_tidy &lt;- students %&gt;%\n    filter(complete.cases(read_score))\n\nIf you used na.omit(students) or drop_na(), you would remove students having NAs also for age. However, students may not have provided their age, but we can have their reading score, so if we were to do this, we would throw away available data!\n\n\n\n\n\n\nQuestion 5. At the 5% significance level, test whether the sample data provide significance evidence that the mean reading score for all students in that university is not equal to 50.\n\n\n\n\nxbar &lt;- mean(students_tidy$read_score)\nxbar\n\n[1] 59\n\nn &lt;- nrow(students_tidy)\nn\n\n[1] 25\n\ns &lt;- sd(students_tidy$read_score)\ns\n\n[1] 17.57365\n\nse &lt;- s / sqrt(n)\nse\n\n[1] 3.514731\n\n\nObserved t-statistic\n\nmu0 &lt;- 50\ntobs &lt;- (xbar - mu0) / se\ntobs\n\n[1] 2.560651\n\n\nP-value = 2 * Probability to the right of tobs:\n\n2 * pt(abs(tobs), df = n - 1, lower.tail = FALSE)\n\n[1] 0.01716085\n\n\nThe sample results are significant, so it’s good practice to follow up with a confidence interval\n\ntstar &lt;- qt(c(0.025, 0.975), df = n - 1)\ntstar\n\n[1] -2.063899  2.063899\n\nci &lt;- xbar + tstar * se\nci\n\n[1] 51.74595 66.25405\n\n\n\n\n\n\n\n\nQuestion 6. The t-test for one population mean relies on some assumptions for the results to be valid. Check whether these are satisfied or not in this sample.\n\n\n\nThe data come from a random sample of students from that university, hence the independence assumption is satisfied.\nThe sample size - that is, the number of data points that are not missing - is \\(n = 25\\). This is not a large enough sample size for the sample mean to be normally distributed, so we will check whether the sample came from a population that follows a normal distribution. If this is the case, then the sample mean would be normally distributed irrespectively of the sample size.\n\nplt1 &lt;- ggplot(students_tidy, aes(x = read_score)) +\n    geom_density() +\n    labs(x = \"Reading scores (0-100)\", title = \"(a) Density plot\")\n\nplt2 &lt;- ggplot(students_tidy, aes(sample = read_score)) +\n    geom_qq() +\n    geom_qq_line() +\n    labs(x = \"Theoretical quantiles\", y = \"Sample quantiles\", title = \"(b) qqplot\")\n\nplt1 | plt2\n\n\n\nFigure 1: Distribution of reading scores\n\n\n\nThe density plot highlights a roughly bell-shaped distribution with a single peak near 60. The qqplot doesn’t highlight any severe departures from normality, in fact the sample quantiles follow the theoretical quantiles from a normal distribution pretty closely.\nAlternatively, you can also obtain the qqplot as follows:\n\nqqnorm(students_tidy$read_score)\nqqline(students_tidy$read_score)\n\n\n\n\n\n\n\nWe can also perform a Shapiro-Wilk test:\n\nshapiro.test(students_tidy$read_score)\n\n\n    Shapiro-Wilk normality test\n\ndata:  students_tidy$read_score\nW = 0.97792, p-value = 0.8411\n\n\n\n\n\n\n\n\nQuestion 7. Provide a write up pf your results in the context of the study.\n\n\n\nData were obtained from https://uoepsy.github.io/data/students_reading_scores.csv. The dataset contains measurements on age and reading scores for 35 randomly sampled students from a hypothetical university. Due to random sampling of the students, the independence assumption is satisfied. Some students had missing values for their reading scores, so those students were dropped from the analysis, leaving a sample of 25 students for the analysis. A Shapiro-Wilk normality test was performed at the 5% significance level (\\(W = 0.98, p = 0.84\\)). The sample data do not provide sufficient evidence to reject the null hypothesis of normality of the population data. Furthermore, the density plot shown in Figure 1(a) highlights a roughly bell-shaped distribution, and the qqplot in Figure 1(b) doesn’t show violations from normality.\nAt the 5% significance level, we performed a hypothesis test against the null hypothesis that the mean reading score of all students in that university was equal to 50. The sample data provide very strong evidence to reject the null hypothesis in favour of the alternative one that the population mean reading score is different from 50: \\(t(24) = 2.56, p = 0.02\\), two-sided.\nWe are 95% confident that the mean reading score for all students in that university is between 52 and 66.\n\n\n\n\n\n\nQuestion 8. Is there a significant effect? Is the effect important?\n\n\n\nYes, there is a significant effect, as we found a p-value smaller than the significance level of 5%. However, this is no guarantee that the effect is of practical importance.\nTo judge whether the effect is important, we need to compute the effect size, i.e. Cohen’s D:\n\nD &lt;- (xbar - mu0) / s\nD\n\n[1] 0.5121302\n\n\nCohen’D highlights a medium effect size, highlighting that the sample result is not only significant, but perhaps also of moderate importance and worth noting."
  },
  {
    "objectID": "rd2_05.html#worked-example-power",
    "href": "rd2_05.html#worked-example-power",
    "title": "Errors, Power, Effect size, Assumptions",
    "section": "\n2 Worked example: power",
    "text": "2 Worked example: power\nTo compute power, you need to know the distribution of the sample mean\n\nwhen \\(H_0\\) is true\nwhen \\(H_1\\) is true\n\nThis seldom happens in practice, so these exercises will be more conceptual.\nBefore going ahead, though, remember that if the population has mean \\(\\mu\\) the sampling distribution of the mean is \\(N(\\mu, \\frac{\\sigma}{\\sqrt n})\\), where \\(\\sigma\\) is the population SD and \\(n\\) the size of a sample.\nSuppose you are testing\n\\[H_0 : \\mu = 0\\] \\[H_1 : \\mu &gt; 0\\]\nWhat is the power of the test if:\n\nthe population has a mean \\(\\mu = 5\\)\n\nthe population has a standard deviation \\(\\sigma = 3\\)\n\nyou will take a sample of size 30\n\nThe distribution of the sample mean in the population is\n\\[\\bar X \\sim N(5, \\frac{3}{\\sqrt{30}})\\]\nIf \\(H_0\\) is true, however, the distribution would be:\n\\[\\bar X \\sim N(0, \\frac{3}{\\sqrt{30}})\\]\n\n\n\n\n\n\n\n\nYou would reject the null hypothesis, at the 5% significance level, for values larger the 0.95 quantile of a \\(N(0, \\frac{3}{\\sqrt{30}})\\) distribution:\n\nsigma &lt;- 3\nn &lt;- 30\nSE &lt;- 3 / sqrt(30)\n\n\ntstar &lt;- qnorm(0.95, mean = 0, sd = SE)\ntstar\n\n[1] 0.9009234\n\n\n\n\n\n\n\n\n\n\nThe power is the probability of rejecting the null hypothesis when the null is false.\nSo, if the null is false, the mean follows the distribution \\(N(5, \\frac{3}{\\sqrt{30}})\\). The probability of correctly rejecting the null would be the probability under that distribution beyond -1.07\n\nPow &lt;- 1 - pnorm(0.90, mean = 5, sd = SE)\nPow\n\n[1] 1\n\n\nThe power is 1.\n\n\n\n\n\n\nQuestion 9. Assume now that the true population mean is \\(\\mu = 2\\). What is the power of the test?\n\n\n\n\n\n\n\n\n\n\n\nThe distribution of the mean when \\(H_0\\) is false is then\n\\[\\bar X \\sim N(2, \\frac{3}{\\sqrt{30}})\\]\nWe need to compute the probability in a \\(N(2, \\frac{3}{\\sqrt{30}})\\) distribution to the right of 0.90.\n\nPow &lt;- pnorm(0.90, mean = 2, sd = SE, lower.tail = FALSE)\nPow\n\n[1] 0.9776951\n\n\nor\n\nPow &lt;- 1 - pnorm(0.90, mean = 2, sd = SE)\nPow\n\n[1] 0.9776951\n\n\nThe power, i.e. the probability that the test rejects the null hypothesis when it is false, is 0.98.\n\n\n\n\n\n\nQuestion 10. Assume now that the true population mean is \\(\\mu = 2\\). What is the probability of a Type II error?\n\n\n\nThe easiest way to compute it is as \\(\\beta = 1 - \\text{Power}\\) because \\(\\text{Power} = 1 - \\beta\\).\n\nbeta &lt;- 1 - Pow\nbeta\n\n[1] 0.02230486\n\n\nThe probability of not rejecting the null when it is false is 0.02.\nIf you want to compute it from scratch you can compute it as the probability to the left of 0.90. This is because values below 0.90 lead to not rejecting the null hypothesis.\n\nbeta &lt;- pnorm(0.90, mean = 2, sd = SE)\nbeta\n\n[1] 0.02230486"
  }
]